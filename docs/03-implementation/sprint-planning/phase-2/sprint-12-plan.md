# Sprint 12: æ•´åˆèˆ‡å„ªåŒ– (Integration & Polish)

**Sprint ç›®æ¨™**: æ•´åˆ Phase 2 æ‰€æœ‰åŠŸèƒ½ã€å„ªåŒ–æ•ˆèƒ½ã€å®Œå–„æ–‡æª”å’Œæ¸¬è©¦

**é€±æœŸ**: Week 25-26 (2 é€±)
**Story Points**: 34 é»
**å‰ç½®æ¢ä»¶**: Sprint 7-11 å®Œæˆ

---

## Sprint æ¦‚è¿°

### æ ¸å¿ƒäº¤ä»˜ç‰©

| ID | åŠŸèƒ½ | å„ªå…ˆç´š | Story Points | ç‹€æ…‹ |
|----|------|--------|--------------|------|
| P2-F14 | Performance Optimization æ€§èƒ½å„ªåŒ– | ğŸ”´ é«˜ | 13 | å¾…é–‹ç™¼ |
| P2-F15 | UI Integration UI æ•´åˆ | ğŸ”´ é«˜ | 13 | å¾…é–‹ç™¼ |
| P2-F16 | Documentation & Testing æ–‡æª”èˆ‡æ¸¬è©¦ | ğŸ”´ é«˜ | 8 | å¾…é–‹ç™¼ |

### Sprint 12 å®šä½

```
Phase 2 Sprint é€²ç¨‹
â”œâ”€ Sprint 7:  Concurrent Execution      âœ… åŸºç¤è¨­æ–½
â”œâ”€ Sprint 8:  Agent Handoff             âœ… å”ä½œæ©Ÿåˆ¶
â”œâ”€ Sprint 9:  GroupChat & Multi-turn    âœ… å°è©±èƒ½åŠ›
â”œâ”€ Sprint 10: Dynamic Planning          âœ… æ™ºèƒ½æ±ºç­–
â”œâ”€ Sprint 11: Nested Workflows          âœ… é€²éšç·¨æ’
â””â”€ Sprint 12: Integration & Polish      ğŸ”„ æ•´åˆå„ªåŒ–
                                           â”œâ”€ æ•ˆèƒ½å„ªåŒ–
                                           â”œâ”€ UI æ•´åˆ
                                           â””â”€ æ–‡æª”æ¸¬è©¦
```

---

## User Stories

### Story 12-1: Performance Profiler & Optimization (5 é»)

**ä½œç‚º** ç³»çµ±ç®¡ç†å“¡
**æˆ‘å¸Œæœ›** æœ‰å®Œæ•´çš„æ•ˆèƒ½åˆ†æå’Œå„ªåŒ–å·¥å…·
**ä»¥ä¾¿** ç¢ºä¿ Phase 2 åŠŸèƒ½åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­é«˜æ•ˆé‹è¡Œ

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/core/performance/profiler.py

from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional, Callable
from datetime import datetime, timedelta
from uuid import UUID, uuid4
from enum import Enum
import asyncio
import time
import functools
import statistics


class MetricType(str, Enum):
    """æŒ‡æ¨™é¡å‹"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    MEMORY = "memory"
    CPU = "cpu"
    CONCURRENCY = "concurrency"
    ERROR_RATE = "error_rate"


@dataclass
class PerformanceMetric:
    """æ•ˆèƒ½æŒ‡æ¨™"""
    name: str
    metric_type: MetricType
    value: float
    unit: str
    timestamp: datetime = field(default_factory=datetime.utcnow)
    tags: Dict[str, str] = field(default_factory=dict)


@dataclass
class ProfileSession:
    """åˆ†ææœƒè©±"""
    id: UUID
    name: str
    started_at: datetime
    ended_at: Optional[datetime] = None
    metrics: List[PerformanceMetric] = field(default_factory=list)
    summary: Optional[Dict[str, Any]] = None


class PerformanceProfiler:
    """
    æ•ˆèƒ½åˆ†æå™¨

    æä¾›ï¼š
    - å»¶é²è¿½è¹¤
    - ååé‡æ¸¬é‡
    - è³‡æºä½¿ç”¨ç›£æ§
    - ç“¶é ¸è­˜åˆ¥
    """

    def __init__(self):
        self._sessions: Dict[UUID, ProfileSession] = {}
        self._active_session: Optional[ProfileSession] = None
        self._metric_collectors: Dict[MetricType, List[float]] = {
            mt: [] for mt in MetricType
        }

    def start_session(self, name: str) -> ProfileSession:
        """é–‹å§‹åˆ†ææœƒè©±"""
        session = ProfileSession(
            id=uuid4(),
            name=name,
            started_at=datetime.utcnow()
        )
        self._sessions[session.id] = session
        self._active_session = session
        return session

    def end_session(
        self,
        session_id: Optional[UUID] = None
    ) -> ProfileSession:
        """çµæŸåˆ†ææœƒè©±"""
        session = self._sessions.get(
            session_id or (self._active_session.id if self._active_session else None)
        )
        if not session:
            raise ValueError("No active session")

        session.ended_at = datetime.utcnow()
        session.summary = self._generate_summary(session)

        if session == self._active_session:
            self._active_session = None

        return session

    def record_metric(
        self,
        name: str,
        metric_type: MetricType,
        value: float,
        unit: str = "",
        tags: Optional[Dict[str, str]] = None
    ) -> None:
        """è¨˜éŒ„æŒ‡æ¨™"""
        metric = PerformanceMetric(
            name=name,
            metric_type=metric_type,
            value=value,
            unit=unit,
            tags=tags or {}
        )

        if self._active_session:
            self._active_session.metrics.append(metric)

        self._metric_collectors[metric_type].append(value)

    def measure_latency(self, operation_name: str):
        """å»¶é²æ¸¬é‡è£é£¾å™¨"""
        def decorator(func):
            @functools.wraps(func)
            async def async_wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    return await func(*args, **kwargs)
                finally:
                    elapsed = (time.perf_counter() - start) * 1000  # ms
                    self.record_metric(
                        name=operation_name,
                        metric_type=MetricType.LATENCY,
                        value=elapsed,
                        unit="ms"
                    )

            @functools.wraps(func)
            def sync_wrapper(*args, **kwargs):
                start = time.perf_counter()
                try:
                    return func(*args, **kwargs)
                finally:
                    elapsed = (time.perf_counter() - start) * 1000
                    self.record_metric(
                        name=operation_name,
                        metric_type=MetricType.LATENCY,
                        value=elapsed,
                        unit="ms"
                    )

            return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
        return decorator

    def _generate_summary(
        self,
        session: ProfileSession
    ) -> Dict[str, Any]:
        """ç”Ÿæˆæœƒè©±æ‘˜è¦"""
        summary = {
            "duration_seconds": (
                session.ended_at - session.started_at
            ).total_seconds() if session.ended_at else None,
            "total_metrics": len(session.metrics),
            "metrics_by_type": {}
        }

        # æŒ‰é¡å‹åˆ†çµ„è¨ˆç®—çµ±è¨ˆ
        for metric_type in MetricType:
            values = [
                m.value for m in session.metrics
                if m.metric_type == metric_type
            ]

            if values:
                summary["metrics_by_type"][metric_type.value] = {
                    "count": len(values),
                    "min": min(values),
                    "max": max(values),
                    "avg": statistics.mean(values),
                    "median": statistics.median(values),
                    "p95": self._percentile(values, 95),
                    "p99": self._percentile(values, 99)
                }

        return summary

    def _percentile(self, values: List[float], p: int) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        if not values:
            return 0
        sorted_values = sorted(values)
        idx = int(len(sorted_values) * p / 100)
        return sorted_values[min(idx, len(sorted_values) - 1)]

    def get_recommendations(self) -> List[Dict[str, Any]]:
        """ç²å–å„ªåŒ–å»ºè­°"""
        recommendations = []

        # åˆ†æå»¶é²
        latency_values = self._metric_collectors[MetricType.LATENCY]
        if latency_values:
            avg_latency = statistics.mean(latency_values)
            p99_latency = self._percentile(latency_values, 99)

            if avg_latency > 1000:  # > 1ç§’
                recommendations.append({
                    "type": "latency",
                    "severity": "high",
                    "message": f"å¹³å‡å»¶é² {avg_latency:.0f}ms éé«˜",
                    "recommendation": "è€ƒæ…®æ·»åŠ å¿«å–æˆ–å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢"
                })

            if p99_latency > avg_latency * 3:
                recommendations.append({
                    "type": "latency_variance",
                    "severity": "medium",
                    "message": f"P99 å»¶é² ({p99_latency:.0f}ms) é é«˜æ–¼å¹³å‡å€¼",
                    "recommendation": "èª¿æŸ¥é•·å°¾è«‹æ±‚åŸå› "
                })

        # åˆ†æéŒ¯èª¤ç‡
        error_values = self._metric_collectors[MetricType.ERROR_RATE]
        if error_values:
            avg_error_rate = statistics.mean(error_values)
            if avg_error_rate > 0.05:  # > 5%
                recommendations.append({
                    "type": "error_rate",
                    "severity": "high",
                    "message": f"éŒ¯èª¤ç‡ {avg_error_rate:.1%} è¶…éé–¾å€¼",
                    "recommendation": "æª¢æŸ¥éŒ¯èª¤æ—¥èªŒä¸¦ä¿®å¾©æ ¹æœ¬åŸå› "
                })

        # åˆ†æä¸¦ç™¼
        concurrency_values = self._metric_collectors[MetricType.CONCURRENCY]
        if concurrency_values:
            max_concurrency = max(concurrency_values)
            if max_concurrency > 100:
                recommendations.append({
                    "type": "concurrency",
                    "severity": "medium",
                    "message": f"æœ€å¤§ä¸¦ç™¼æ•¸ {max_concurrency} è¼ƒé«˜",
                    "recommendation": "è€ƒæ…®å¯¦æ–½è«‹æ±‚é™æµ"
                })

        return recommendations


class PerformanceOptimizer:
    """
    æ•ˆèƒ½å„ªåŒ–å™¨

    æä¾›è‡ªå‹•åŒ–çš„æ•ˆèƒ½å„ªåŒ–åŠŸèƒ½
    """

    def __init__(
        self,
        profiler: PerformanceProfiler,
        cache_service: Any,
        config: Dict[str, Any]
    ):
        self.profiler = profiler
        self.cache = cache_service
        self.config = config

        # å„ªåŒ–ç­–ç•¥
        self._strategies: Dict[str, Callable] = {
            "caching": self._apply_caching,
            "batching": self._apply_batching,
            "connection_pooling": self._apply_connection_pooling,
            "query_optimization": self._apply_query_optimization
        }

    async def analyze_and_optimize(
        self,
        target: str
    ) -> Dict[str, Any]:
        """åˆ†æä¸¦å„ªåŒ–"""
        # 1. æ”¶é›†ç•¶å‰æ•ˆèƒ½æ•¸æ“š
        session = self.profiler.start_session(f"optimization_{target}")

        # 2. åŸ·è¡ŒåŸºæº–æ¸¬è©¦
        baseline = await self._run_benchmark(target)

        # 3. ç²å–å„ªåŒ–å»ºè­°
        recommendations = self.profiler.get_recommendations()

        # 4. æ‡‰ç”¨å„ªåŒ–ç­–ç•¥
        applied_strategies = []
        for rec in recommendations:
            strategy_name = self._map_recommendation_to_strategy(rec)
            if strategy_name and strategy_name in self._strategies:
                await self._strategies[strategy_name](target)
                applied_strategies.append(strategy_name)

        # 5. é‡æ–°æ¸¬è©¦
        optimized = await self._run_benchmark(target)

        self.profiler.end_session(session.id)

        return {
            "target": target,
            "baseline": baseline,
            "optimized": optimized,
            "improvement": self._calculate_improvement(baseline, optimized),
            "applied_strategies": applied_strategies,
            "recommendations": recommendations
        }

    async def _run_benchmark(self, target: str) -> Dict[str, Any]:
        """åŸ·è¡ŒåŸºæº–æ¸¬è©¦"""
        latencies = []
        errors = 0
        total_requests = 100

        for _ in range(total_requests):
            start = time.perf_counter()
            try:
                # æ¨¡æ“¬è«‹æ±‚
                await asyncio.sleep(0.01)
                latencies.append((time.perf_counter() - start) * 1000)
            except Exception:
                errors += 1

        return {
            "avg_latency_ms": statistics.mean(latencies) if latencies else 0,
            "p95_latency_ms": self.profiler._percentile(latencies, 95),
            "error_rate": errors / total_requests,
            "throughput_rps": total_requests / (sum(latencies) / 1000) if latencies else 0
        }

    def _map_recommendation_to_strategy(
        self,
        recommendation: Dict[str, Any]
    ) -> Optional[str]:
        """å°‡å»ºè­°æ˜ å°„åˆ°ç­–ç•¥"""
        mapping = {
            "latency": "caching",
            "latency_variance": "query_optimization",
            "concurrency": "connection_pooling"
        }
        return mapping.get(recommendation.get("type"))

    async def _apply_caching(self, target: str) -> None:
        """æ‡‰ç”¨å¿«å–ç­–ç•¥"""
        # å¯¦ç¾å¿«å–é‚è¼¯
        pass

    async def _apply_batching(self, target: str) -> None:
        """æ‡‰ç”¨æ‰¹æ¬¡è™•ç†ç­–ç•¥"""
        pass

    async def _apply_connection_pooling(self, target: str) -> None:
        """æ‡‰ç”¨é€£æ¥æ± ç­–ç•¥"""
        pass

    async def _apply_query_optimization(self, target: str) -> None:
        """æ‡‰ç”¨æŸ¥è©¢å„ªåŒ–ç­–ç•¥"""
        pass

    def _calculate_improvement(
        self,
        baseline: Dict[str, Any],
        optimized: Dict[str, Any]
    ) -> Dict[str, Any]:
        """è¨ˆç®—æ”¹é€²å¹…åº¦"""
        return {
            "latency_improvement": (
                (baseline["avg_latency_ms"] - optimized["avg_latency_ms"])
                / baseline["avg_latency_ms"] * 100
                if baseline["avg_latency_ms"] > 0 else 0
            ),
            "throughput_improvement": (
                (optimized["throughput_rps"] - baseline["throughput_rps"])
                / baseline["throughput_rps"] * 100
                if baseline["throughput_rps"] > 0 else 0
            ),
            "error_rate_improvement": (
                (baseline["error_rate"] - optimized["error_rate"])
                / baseline["error_rate"] * 100
                if baseline["error_rate"] > 0 else 0
            )
        }
```

#### é©—æ”¶æ¨™æº–
- [ ] å»¶é²è¿½è¹¤æº–ç¢º
- [ ] è‡ªå‹•ç”Ÿæˆå„ªåŒ–å»ºè­°
- [ ] åŸºæº–æ¸¬è©¦åŠŸèƒ½
- [ ] æ”¹é€²å¹…åº¦è¨ˆç®—
- [ ] æ•ˆèƒ½ KPI é”æ¨™

---

### Story 12-2: Concurrent Execution Optimization (3 é»)

**ä½œç‚º** ç³»çµ±æ¶æ§‹å¸«
**æˆ‘å¸Œæœ›** å„ªåŒ–ä¸¦è¡ŒåŸ·è¡Œæ•ˆèƒ½
**ä»¥ä¾¿** é”åˆ° 3x ååé‡æå‡ç›®æ¨™

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/core/performance/concurrent_optimizer.py

import asyncio
from typing import List, Any, Callable, TypeVar, Generic
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
import functools

T = TypeVar('T')


@dataclass
class ConcurrencyConfig:
    """ä¸¦ç™¼é…ç½®"""
    max_workers: int = 10
    batch_size: int = 50
    timeout_seconds: float = 30.0
    semaphore_limit: int = 100
    use_thread_pool: bool = False


class ConcurrentOptimizer:
    """
    ä¸¦ç™¼å„ªåŒ–å™¨

    å„ªåŒ–ä¸¦è¡ŒåŸ·è¡Œçš„æ•ˆèƒ½
    """

    def __init__(self, config: ConcurrencyConfig):
        self.config = config
        self._semaphore = asyncio.Semaphore(config.semaphore_limit)
        self._thread_pool = ThreadPoolExecutor(
            max_workers=config.max_workers
        ) if config.use_thread_pool else None

    async def execute_batch(
        self,
        items: List[Any],
        processor: Callable[[Any], Any],
        preserve_order: bool = True
    ) -> List[Any]:
        """
        æ‰¹æ¬¡ä¸¦è¡ŒåŸ·è¡Œ

        Args:
            items: å¾…è™•ç†é …ç›®
            processor: è™•ç†å‡½æ•¸
            preserve_order: æ˜¯å¦ä¿æŒé †åº

        Returns:
            è™•ç†çµæœåˆ—è¡¨
        """
        results = []

        # åˆ†æ‰¹è™•ç†
        for i in range(0, len(items), self.config.batch_size):
            batch = items[i:i + self.config.batch_size]

            batch_results = await self._process_batch(
                batch, processor, preserve_order
            )
            results.extend(batch_results)

        return results

    async def _process_batch(
        self,
        batch: List[Any],
        processor: Callable,
        preserve_order: bool
    ) -> List[Any]:
        """è™•ç†å–®å€‹æ‰¹æ¬¡"""
        async def process_with_semaphore(item, index):
            async with self._semaphore:
                if asyncio.iscoroutinefunction(processor):
                    result = await processor(item)
                else:
                    result = await asyncio.get_event_loop().run_in_executor(
                        self._thread_pool,
                        processor,
                        item
                    )
                return (index, result)

        tasks = [
            process_with_semaphore(item, i)
            for i, item in enumerate(batch)
        ]

        completed = await asyncio.gather(*tasks, return_exceptions=True)

        if preserve_order:
            # æŒ‰ç´¢å¼•æ’åº
            sorted_results = sorted(
                [(idx, res) for idx, res in completed if not isinstance(res, Exception)],
                key=lambda x: x[0]
            )
            return [res for _, res in sorted_results]
        else:
            return [res for idx, res in completed if not isinstance(res, Exception)]

    async def execute_with_timeout(
        self,
        coros: List[asyncio.coroutine],
        timeout: Optional[float] = None
    ) -> List[Any]:
        """å¸¶è¶…æ™‚çš„ä¸¦è¡ŒåŸ·è¡Œ"""
        timeout = timeout or self.config.timeout_seconds

        try:
            results = await asyncio.wait_for(
                asyncio.gather(*coros, return_exceptions=True),
                timeout=timeout
            )
            return results
        except asyncio.TimeoutError:
            return [TimeoutError("Batch execution timed out")]

    async def execute_with_retry(
        self,
        coro: asyncio.coroutine,
        max_retries: int = 3,
        backoff_factor: float = 1.5
    ) -> Any:
        """å¸¶é‡è©¦çš„åŸ·è¡Œ"""
        last_exception = None

        for attempt in range(max_retries):
            try:
                return await coro
            except Exception as e:
                last_exception = e
                if attempt < max_retries - 1:
                    await asyncio.sleep(backoff_factor ** attempt)

        raise last_exception

    def create_worker_pool(
        self,
        num_workers: int
    ) -> "WorkerPool":
        """å»ºç«‹å·¥ä½œæ± """
        return WorkerPool(num_workers, self._semaphore)


class WorkerPool:
    """å·¥ä½œæ± """

    def __init__(
        self,
        num_workers: int,
        semaphore: asyncio.Semaphore
    ):
        self.num_workers = num_workers
        self.semaphore = semaphore
        self._queue: asyncio.Queue = asyncio.Queue()
        self._workers: List[asyncio.Task] = []
        self._results: List[Any] = []

    async def start(self) -> None:
        """å•Ÿå‹•å·¥ä½œæ± """
        self._workers = [
            asyncio.create_task(self._worker(i))
            for i in range(self.num_workers)
        ]

    async def _worker(self, worker_id: int) -> None:
        """å·¥ä½œè€…"""
        while True:
            try:
                task = await asyncio.wait_for(
                    self._queue.get(),
                    timeout=1.0
                )

                if task is None:  # çµ‚æ­¢ä¿¡è™Ÿ
                    break

                async with self.semaphore:
                    result = await task()
                    self._results.append(result)

                self._queue.task_done()

            except asyncio.TimeoutError:
                continue

    async def submit(self, task: Callable) -> None:
        """æäº¤ä»»å‹™"""
        await self._queue.put(task)

    async def shutdown(self) -> List[Any]:
        """é—œé–‰å·¥ä½œæ± """
        # ç™¼é€çµ‚æ­¢ä¿¡è™Ÿ
        for _ in range(self.num_workers):
            await self._queue.put(None)

        # ç­‰å¾…æ‰€æœ‰å·¥ä½œè€…å®Œæˆ
        await asyncio.gather(*self._workers)

        return self._results
```

#### é©—æ”¶æ¨™æº–
- [ ] æ‰¹æ¬¡è™•ç†æ•ˆèƒ½æå‡
- [ ] ä¿¡è™Ÿé‡é™åˆ¶æœ‰æ•ˆ
- [ ] å·¥ä½œæ± æ­£å¸¸é‹ä½œ
- [ ] 3x ååé‡é”æˆ

---

### Story 12-3: Phase 2 UI Integration (8 é»)

**ä½œç‚º** å‰ç«¯é–‹ç™¼è€…
**æˆ‘å¸Œæœ›** æ•´åˆæ‰€æœ‰ Phase 2 åŠŸèƒ½åˆ° UI
**ä»¥ä¾¿** ç”¨æˆ¶å¯ä»¥è¦–è¦ºåŒ–åœ°ä½¿ç”¨é€²éšåŠŸèƒ½

#### æŠ€è¡“è¦æ ¼

```typescript
// frontend/src/pages/orchestration/OrchestrationDashboard.tsx

import React, { useState, useEffect } from 'react';
import { Card, CardHeader, CardContent } from '@/components/ui/card';
import { Tabs, TabsList, TabsTrigger, TabsContent } from '@/components/ui/tabs';
import { Badge } from '@/components/ui/badge';
import { Button } from '@/components/ui/button';
import {
  Activity,
  GitBranch,
  Users,
  MessageSquare,
  Zap,
  Layers,
  Settings
} from 'lucide-react';

// å­çµ„ä»¶
import { ConcurrentExecutionPanel } from './ConcurrentExecutionPanel';
import { HandoffMonitor } from './HandoffMonitor';
import { GroupChatPanel } from './GroupChatPanel';
import { PlanningDashboard } from './PlanningDashboard';
import { NestedWorkflowViewer } from './NestedWorkflowViewer';
import { PerformanceMetrics } from './PerformanceMetrics';

interface OrchestrationStats {
  concurrentExecutions: number;
  activeHandoffs: number;
  groupChats: number;
  activePlans: number;
  nestedWorkflows: number;
}

export const OrchestrationDashboard: React.FC = () => {
  const [activeTab, setActiveTab] = useState('overview');
  const [stats, setStats] = useState<OrchestrationStats | null>(null);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchStats();
    const interval = setInterval(fetchStats, 5000);
    return () => clearInterval(interval);
  }, []);

  const fetchStats = async () => {
    try {
      const response = await fetch('/api/v1/orchestration/stats');
      const data = await response.json();
      setStats(data);
    } catch (error) {
      console.error('Failed to fetch stats:', error);
    } finally {
      setLoading(false);
    }
  };

  const statCards = [
    {
      label: 'ä¸¦è¡ŒåŸ·è¡Œ',
      value: stats?.concurrentExecutions ?? 0,
      icon: Zap,
      color: 'text-yellow-500'
    },
    {
      label: 'æ´»èºäº¤æ¥',
      value: stats?.activeHandoffs ?? 0,
      icon: GitBranch,
      color: 'text-blue-500'
    },
    {
      label: 'ç¾¤çµ„å°è©±',
      value: stats?.groupChats ?? 0,
      icon: MessageSquare,
      color: 'text-green-500'
    },
    {
      label: 'åŸ·è¡Œè¨ˆåŠƒ',
      value: stats?.activePlans ?? 0,
      icon: Activity,
      color: 'text-purple-500'
    },
    {
      label: 'åµŒå¥—å·¥ä½œæµ',
      value: stats?.nestedWorkflows ?? 0,
      icon: Layers,
      color: 'text-orange-500'
    }
  ];

  return (
    <div className="p-6 space-y-6">
      {/* Header */}
      <div className="flex justify-between items-center">
        <div>
          <h1 className="text-2xl font-bold">é€²éšç·¨æ’æ§åˆ¶å°</h1>
          <p className="text-muted-foreground">
            Phase 2 å¤š Agent å”ä½œåŠŸèƒ½
          </p>
        </div>
        <Button variant="outline">
          <Settings className="h-4 w-4 mr-2" />
          è¨­å®š
        </Button>
      </div>

      {/* Stats Overview */}
      <div className="grid grid-cols-5 gap-4">
        {statCards.map((stat, index) => (
          <Card key={index}>
            <CardContent className="p-4">
              <div className="flex items-center justify-between">
                <div>
                  <p className="text-sm text-muted-foreground">{stat.label}</p>
                  <p className="text-2xl font-bold">{stat.value}</p>
                </div>
                <stat.icon className={`h-8 w-8 ${stat.color}`} />
              </div>
            </CardContent>
          </Card>
        ))}
      </div>

      {/* Main Tabs */}
      <Tabs value={activeTab} onValueChange={setActiveTab}>
        <TabsList className="grid grid-cols-6 w-full">
          <TabsTrigger value="overview">
            <Activity className="h-4 w-4 mr-2" />
            ç¸½è¦½
          </TabsTrigger>
          <TabsTrigger value="concurrent">
            <Zap className="h-4 w-4 mr-2" />
            ä¸¦è¡ŒåŸ·è¡Œ
          </TabsTrigger>
          <TabsTrigger value="handoff">
            <GitBranch className="h-4 w-4 mr-2" />
            Agent äº¤æ¥
          </TabsTrigger>
          <TabsTrigger value="groupchat">
            <MessageSquare className="h-4 w-4 mr-2" />
            ç¾¤çµ„å°è©±
          </TabsTrigger>
          <TabsTrigger value="planning">
            <Activity className="h-4 w-4 mr-2" />
            å‹•æ…‹è¦åŠƒ
          </TabsTrigger>
          <TabsTrigger value="nested">
            <Layers className="h-4 w-4 mr-2" />
            åµŒå¥—å·¥ä½œæµ
          </TabsTrigger>
        </TabsList>

        <TabsContent value="overview" className="mt-6">
          <div className="grid grid-cols-2 gap-6">
            <Card>
              <CardHeader>
                <h3 className="font-semibold">æ•ˆèƒ½æŒ‡æ¨™</h3>
              </CardHeader>
              <CardContent>
                <PerformanceMetrics />
              </CardContent>
            </Card>

            <Card>
              <CardHeader>
                <h3 className="font-semibold">æœ€è¿‘æ´»å‹•</h3>
              </CardHeader>
              <CardContent>
                <RecentActivityList />
              </CardContent>
            </Card>
          </div>
        </TabsContent>

        <TabsContent value="concurrent" className="mt-6">
          <ConcurrentExecutionPanel />
        </TabsContent>

        <TabsContent value="handoff" className="mt-6">
          <HandoffMonitor />
        </TabsContent>

        <TabsContent value="groupchat" className="mt-6">
          <GroupChatPanel />
        </TabsContent>

        <TabsContent value="planning" className="mt-6">
          <PlanningDashboard />
        </TabsContent>

        <TabsContent value="nested" className="mt-6">
          <NestedWorkflowViewer />
        </TabsContent>
      </Tabs>
    </div>
  );
};


// frontend/src/pages/orchestration/NestedWorkflowViewer.tsx

import React, { useState, useCallback } from 'react';
import ReactFlow, {
  Node,
  Edge,
  Controls,
  Background,
  MiniMap,
  useNodesState,
  useEdgesState
} from 'reactflow';
import 'reactflow/dist/style.css';

interface NestedWorkflowViewerProps {
  executionId?: string;
}

export const NestedWorkflowViewer: React.FC<NestedWorkflowViewerProps> = ({
  executionId
}) => {
  const [nodes, setNodes, onNodesChange] = useNodesState([]);
  const [edges, setEdges, onEdgesChange] = useEdgesState([]);
  const [selectedNode, setSelectedNode] = useState<Node | null>(null);

  useEffect(() => {
    if (executionId) {
      fetchExecutionTree(executionId);
    }
  }, [executionId]);

  const fetchExecutionTree = async (id: string) => {
    try {
      const response = await fetch(`/api/v1/nested/executions/${id}/tree`);
      const tree = await response.json();

      const { nodes: flowNodes, edges: flowEdges } = convertTreeToFlow(tree);
      setNodes(flowNodes);
      setEdges(flowEdges);
    } catch (error) {
      console.error('Failed to fetch execution tree:', error);
    }
  };

  const convertTreeToFlow = (
    tree: any,
    parentId?: string,
    depth: number = 0,
    index: number = 0
  ): { nodes: Node[]; edges: Edge[] } => {
    const nodeId = tree.id;
    const xOffset = depth * 250;
    const yOffset = index * 100;

    const node: Node = {
      id: nodeId,
      type: 'workflow',
      position: { x: xOffset, y: yOffset },
      data: {
        label: tree.workflow_id,
        depth: tree.depth,
        status: tree.status || 'unknown'
      },
      style: {
        background: getStatusColor(tree.status),
        border: '2px solid #333',
        borderRadius: '8px',
        padding: '10px'
      }
    };

    const nodes = [node];
    const edges: Edge[] = [];

    if (parentId) {
      edges.push({
        id: `${parentId}-${nodeId}`,
        source: parentId,
        target: nodeId,
        animated: tree.status === 'running'
      });
    }

    if (tree.children) {
      tree.children.forEach((child: any, childIndex: number) => {
        const childResult = convertTreeToFlow(
          child,
          nodeId,
          depth + 1,
          childIndex
        );
        nodes.push(...childResult.nodes);
        edges.push(...childResult.edges);
      });
    }

    return { nodes, edges };
  };

  const getStatusColor = (status: string) => {
    switch (status) {
      case 'running': return '#fef3c7';
      case 'completed': return '#d1fae5';
      case 'failed': return '#fee2e2';
      default: return '#f3f4f6';
    }
  };

  const onNodeClick = useCallback((event: any, node: Node) => {
    setSelectedNode(node);
  }, []);

  return (
    <div className="h-[600px] border rounded-lg">
      <ReactFlow
        nodes={nodes}
        edges={edges}
        onNodesChange={onNodesChange}
        onEdgesChange={onEdgesChange}
        onNodeClick={onNodeClick}
        fitView
      >
        <Controls />
        <Background />
        <MiniMap />
      </ReactFlow>

      {selectedNode && (
        <div className="absolute bottom-4 right-4 bg-white p-4 rounded-lg shadow-lg">
          <h4 className="font-semibold">ç¯€é»è©³æƒ…</h4>
          <p>ID: {selectedNode.id}</p>
          <p>æ·±åº¦: {selectedNode.data.depth}</p>
          <p>ç‹€æ…‹: {selectedNode.data.status}</p>
        </div>
      )}
    </div>
  );
};


// frontend/src/pages/orchestration/PerformanceMetrics.tsx

import React, { useState, useEffect } from 'react';
import {
  LineChart,
  Line,
  XAxis,
  YAxis,
  CartesianGrid,
  Tooltip,
  ResponsiveContainer,
  AreaChart,
  Area
} from 'recharts';
import { Card, CardContent } from '@/components/ui/card';

interface MetricData {
  timestamp: string;
  latency: number;
  throughput: number;
  errorRate: number;
  concurrency: number;
}

export const PerformanceMetrics: React.FC = () => {
  const [metrics, setMetrics] = useState<MetricData[]>([]);
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    fetchMetrics();
    const interval = setInterval(fetchMetrics, 10000);
    return () => clearInterval(interval);
  }, []);

  const fetchMetrics = async () => {
    try {
      const response = await fetch('/api/v1/performance/metrics?range=1h');
      const data = await response.json();
      setMetrics(data.metrics);
    } catch (error) {
      console.error('Failed to fetch metrics:', error);
    } finally {
      setLoading(false);
    }
  };

  if (loading) {
    return <div>è¼‰å…¥ä¸­...</div>;
  }

  return (
    <div className="space-y-6">
      {/* Latency Chart */}
      <div>
        <h4 className="text-sm font-medium mb-2">å»¶é² (ms)</h4>
        <ResponsiveContainer width="100%" height={150}>
          <AreaChart data={metrics}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis
              dataKey="timestamp"
              tickFormatter={(v) => new Date(v).toLocaleTimeString()}
            />
            <YAxis />
            <Tooltip />
            <Area
              type="monotone"
              dataKey="latency"
              stroke="#8884d8"
              fill="#8884d8"
              fillOpacity={0.3}
            />
          </AreaChart>
        </ResponsiveContainer>
      </div>

      {/* Throughput Chart */}
      <div>
        <h4 className="text-sm font-medium mb-2">ååé‡ (req/s)</h4>
        <ResponsiveContainer width="100%" height={150}>
          <LineChart data={metrics}>
            <CartesianGrid strokeDasharray="3 3" />
            <XAxis
              dataKey="timestamp"
              tickFormatter={(v) => new Date(v).toLocaleTimeString()}
            />
            <YAxis />
            <Tooltip />
            <Line
              type="monotone"
              dataKey="throughput"
              stroke="#82ca9d"
              strokeWidth={2}
            />
          </LineChart>
        </ResponsiveContainer>
      </div>

      {/* Summary Stats */}
      <div className="grid grid-cols-4 gap-4">
        <Card>
          <CardContent className="p-3 text-center">
            <p className="text-xs text-muted-foreground">å¹³å‡å»¶é²</p>
            <p className="text-lg font-bold">
              {metrics.length > 0
                ? (metrics.reduce((a, b) => a + b.latency, 0) / metrics.length).toFixed(0)
                : 0} ms
            </p>
          </CardContent>
        </Card>
        <Card>
          <CardContent className="p-3 text-center">
            <p className="text-xs text-muted-foreground">å¹³å‡ååé‡</p>
            <p className="text-lg font-bold">
              {metrics.length > 0
                ? (metrics.reduce((a, b) => a + b.throughput, 0) / metrics.length).toFixed(0)
                : 0} /s
            </p>
          </CardContent>
        </Card>
        <Card>
          <CardContent className="p-3 text-center">
            <p className="text-xs text-muted-foreground">éŒ¯èª¤ç‡</p>
            <p className="text-lg font-bold">
              {metrics.length > 0
                ? (metrics.reduce((a, b) => a + b.errorRate, 0) / metrics.length * 100).toFixed(2)
                : 0}%
            </p>
          </CardContent>
        </Card>
        <Card>
          <CardContent className="p-3 text-center">
            <p className="text-xs text-muted-foreground">æœ€å¤§ä¸¦ç™¼</p>
            <p className="text-lg font-bold">
              {metrics.length > 0
                ? Math.max(...metrics.map(m => m.concurrency))
                : 0}
            </p>
          </CardContent>
        </Card>
      </div>
    </div>
  );
};
```

#### é©—æ”¶æ¨™æº–
- [ ] é€²éšç·¨æ’æ§åˆ¶å°å®Œæ•´
- [ ] æ‰€æœ‰ Phase 2 åŠŸèƒ½å¯è¦–åŒ–
- [ ] æ•ˆèƒ½æŒ‡æ¨™åœ–è¡¨
- [ ] åµŒå¥—å·¥ä½œæµè¦–è¦ºåŒ–
- [ ] éŸ¿æ‡‰å¼è¨­è¨ˆ

---

### Story 12-4: Integration Testing Suite (5 é»)

**ä½œç‚º** QA å·¥ç¨‹å¸«
**æˆ‘å¸Œæœ›** æœ‰å®Œæ•´çš„æ•´åˆæ¸¬è©¦å¥—ä»¶
**ä»¥ä¾¿** ç¢ºä¿ Phase 2 æ‰€æœ‰åŠŸèƒ½æ­£å¸¸é‹ä½œ

#### æŠ€è¡“è¦æ ¼

```python
# tests/integration/test_phase2_integration.py

import pytest
from httpx import AsyncClient
from uuid import uuid4
import asyncio

@pytest.fixture
async def phase2_setup(client: AsyncClient, test_agents, test_workflows):
    """Phase 2 æ¸¬è©¦ç’°å¢ƒè¨­ç½®"""
    return {
        "agents": test_agents,
        "workflows": test_workflows,
        "client": client
    }


class TestConcurrentExecution:
    """ä¸¦è¡ŒåŸ·è¡Œæ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_fork_join_execution(self, phase2_setup):
        """æ¸¬è©¦ Fork-Join åŸ·è¡Œæ¨¡å¼"""
        client = phase2_setup["client"]

        # å»ºç«‹ä¸¦è¡Œå·¥ä½œæµ
        response = await client.post(
            "/api/v1/workflows",
            json={
                "name": "Fork-Join Test",
                "nodes": [
                    {"id": "start", "type": "start"},
                    {"id": "fork", "type": "parallel_gateway", "gateway_type": "fork"},
                    {"id": "task_a", "type": "agent_task", "agent_id": str(uuid4())},
                    {"id": "task_b", "type": "agent_task", "agent_id": str(uuid4())},
                    {"id": "join", "type": "parallel_gateway", "gateway_type": "join"},
                    {"id": "end", "type": "end"}
                ],
                "edges": [
                    {"source": "start", "target": "fork"},
                    {"source": "fork", "target": "task_a"},
                    {"source": "fork", "target": "task_b"},
                    {"source": "task_a", "target": "join"},
                    {"source": "task_b", "target": "join"},
                    {"source": "join", "target": "end"}
                ]
            }
        )
        assert response.status_code == 200
        workflow_id = response.json()["id"]

        # åŸ·è¡Œ
        response = await client.post(
            f"/api/v1/executions",
            json={"workflow_id": workflow_id, "inputs": {}}
        )
        assert response.status_code == 200
        execution_id = response.json()["execution_id"]

        # ç­‰å¾…å®Œæˆ
        await asyncio.sleep(5)

        # é©—è­‰çµæœ
        response = await client.get(f"/api/v1/executions/{execution_id}")
        assert response.json()["status"] == "completed"

    @pytest.mark.asyncio
    async def test_concurrent_throughput(self, phase2_setup):
        """æ¸¬è©¦ä¸¦è¡Œååé‡"""
        client = phase2_setup["client"]
        workflows = phase2_setup["workflows"]

        # åŒæ™‚å•Ÿå‹• 10 å€‹åŸ·è¡Œ
        start_time = asyncio.get_event_loop().time()

        tasks = []
        for _ in range(10):
            tasks.append(
                client.post(
                    "/api/v1/executions",
                    json={
                        "workflow_id": str(workflows[0].id),
                        "inputs": {}
                    }
                )
            )

        responses = await asyncio.gather(*tasks)
        elapsed = asyncio.get_event_loop().time() - start_time

        # é©—è­‰æ‰€æœ‰è«‹æ±‚æˆåŠŸ
        assert all(r.status_code == 200 for r in responses)

        # é©—è­‰ååé‡ (æ‡‰è©² < ä¸²è¡ŒåŸ·è¡Œæ™‚é–“çš„ 50%)
        assert elapsed < 10  # å‡è¨­å–®å€‹åŸ·è¡Œ < 1ç§’


class TestAgentHandoff:
    """Agent äº¤æ¥æ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_graceful_handoff(self, phase2_setup):
        """æ¸¬è©¦å„ªé›…äº¤æ¥"""
        client = phase2_setup["client"]
        agents = phase2_setup["agents"]

        # è¨­ç½®äº¤æ¥
        response = await client.post(
            "/api/v1/handoff/trigger",
            json={
                "execution_id": str(uuid4()),
                "source_agent_id": str(agents[0].id),
                "target_agent_id": str(agents[1].id),
                "policy": "graceful",
                "context": {"task": "test_task"}
            }
        )
        assert response.status_code == 200
        handoff_id = response.json()["handoff_id"]

        # ç­‰å¾…äº¤æ¥å®Œæˆ
        await asyncio.sleep(2)

        # é©—è­‰ç‹€æ…‹
        response = await client.get(f"/api/v1/handoff/{handoff_id}/status")
        assert response.json()["status"] in ["completed", "in_progress"]


class TestGroupChat:
    """ç¾¤çµ„å°è©±æ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_multi_agent_discussion(self, phase2_setup):
        """æ¸¬è©¦å¤š Agent è¨è«–"""
        client = phase2_setup["client"]
        agents = phase2_setup["agents"]

        # å»ºç«‹ç¾¤çµ„
        response = await client.post(
            "/api/v1/groupchat",
            json={
                "name": "Test Discussion",
                "agent_ids": [str(a.id) for a in agents[:3]],
                "config": {
                    "max_rounds": 3,
                    "speaker_selection_method": "round_robin"
                }
            }
        )
        assert response.status_code == 200
        group_id = response.json()["group_id"]

        # é–‹å§‹è¨è«–
        response = await client.post(
            f"/api/v1/groupchat/{group_id}/start",
            json={"content": "è¨è«–é€™å€‹ä¸»é¡Œ"}
        )
        assert response.status_code == 200

        # é©—è­‰è¨Šæ¯
        result = response.json()
        assert len(result["messages"]) >= 3  # è‡³å°‘æœ‰åˆå§‹è¨Šæ¯ + Agent å›æ‡‰


class TestDynamicPlanning:
    """å‹•æ…‹è¦åŠƒæ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_task_decomposition_and_execution(self, phase2_setup):
        """æ¸¬è©¦ä»»å‹™åˆ†è§£å’ŒåŸ·è¡Œ"""
        client = phase2_setup["client"]

        # åˆ†è§£ä»»å‹™
        response = await client.post(
            "/api/v1/planning/decompose",
            json={
                "task_description": "å»ºç«‹ç”¨æˆ¶èªè­‰ç³»çµ±",
                "strategy": "hybrid"
            }
        )
        assert response.status_code == 200
        decomposition = response.json()
        assert len(decomposition["subtasks"]) >= 2

        # å»ºç«‹è¨ˆåŠƒ
        response = await client.post(
            "/api/v1/planning/plans",
            json={"goal": "å»ºç«‹ç”¨æˆ¶èªè­‰ç³»çµ±"}
        )
        assert response.status_code == 200
        plan_id = response.json()["id"]

        # æ‰¹å‡†è¨ˆåŠƒ
        response = await client.post(
            f"/api/v1/planning/plans/{plan_id}/approve",
            params={"approver": "test_user"}
        )
        assert response.status_code == 200


class TestNestedWorkflows:
    """åµŒå¥—å·¥ä½œæµæ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_nested_execution(self, phase2_setup):
        """æ¸¬è©¦åµŒå¥—åŸ·è¡Œ"""
        client = phase2_setup["client"]
        workflows = phase2_setup["workflows"]

        # è¨»å†Šå­å·¥ä½œæµ
        response = await client.post(
            "/api/v1/nested/sub-workflows",
            json={
                "parent_workflow_id": str(workflows[0].id),
                "workflow_id": str(workflows[1].id),
                "config": {"max_depth": 3}
            }
        )
        assert response.status_code == 200

    @pytest.mark.asyncio
    async def test_recursive_execution(self, phase2_setup):
        """æ¸¬è©¦éæ­¸åŸ·è¡Œ"""
        client = phase2_setup["client"]
        workflows = phase2_setup["workflows"]

        response = await client.post(
            "/api/v1/nested/execute/recursive",
            json={
                "workflow_id": str(workflows[0].id),
                "initial_inputs": {"value": 0},
                "max_depth": 3,
                "max_iterations": 10
            }
        )
        assert response.status_code == 200
        result = response.json()
        assert "status" in result


class TestEndToEnd:
    """ç«¯åˆ°ç«¯æ•´åˆæ¸¬è©¦"""

    @pytest.mark.asyncio
    async def test_complex_workflow_scenario(self, phase2_setup):
        """æ¸¬è©¦è¤‡é›œå·¥ä½œæµå ´æ™¯"""
        client = phase2_setup["client"]

        # 1. å»ºç«‹å¸¶ä¸¦è¡Œå’ŒåµŒå¥—çš„å·¥ä½œæµ
        # 2. åŸ·è¡Œä¸¦ç›£æ§
        # 3. é©—è­‰çµæœå’Œæ•ˆèƒ½

        # é€™æ˜¯ä¸€å€‹å®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦
        pass

    @pytest.mark.asyncio
    async def test_failure_recovery(self, phase2_setup):
        """æ¸¬è©¦æ•…éšœæ¢å¾©"""
        client = phase2_setup["client"]

        # æ¨¡æ“¬æ•…éšœä¸¦é©—è­‰æ¢å¾©æ©Ÿåˆ¶
        pass
```

#### é©—æ”¶æ¨™æº–
- [ ] ä¸¦è¡ŒåŸ·è¡Œæ¸¬è©¦é€šé
- [ ] Agent äº¤æ¥æ¸¬è©¦é€šé
- [ ] ç¾¤çµ„å°è©±æ¸¬è©¦é€šé
- [ ] å‹•æ…‹è¦åŠƒæ¸¬è©¦é€šé
- [ ] åµŒå¥—å·¥ä½œæµæ¸¬è©¦é€šé

---

### Story 12-5: Documentation & API Reference (5 é»)

**ä½œç‚º** é–‹ç™¼è€…
**æˆ‘å¸Œæœ›** æœ‰å®Œæ•´çš„ Phase 2 æ–‡æª”
**ä»¥ä¾¿** å¯ä»¥æ­£ç¢ºä½¿ç”¨é€²éšåŠŸèƒ½

#### æŠ€è¡“è¦æ ¼

```markdown
# Phase 2 API æ–‡æª”çµæ§‹

## ç›®éŒ„

docs/
â”œâ”€â”€ phase-2/
â”‚   â”œâ”€â”€ overview.md                    # Phase 2 æ¦‚è¿°
â”‚   â”œâ”€â”€ getting-started.md             # å¿«é€Ÿé–‹å§‹æŒ‡å—
â”‚   â”‚
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ concurrent-execution.md    # ä¸¦è¡ŒåŸ·è¡Œ
â”‚   â”‚   â”œâ”€â”€ agent-handoff.md           # Agent äº¤æ¥
â”‚   â”‚   â”œâ”€â”€ groupchat.md               # ç¾¤çµ„å°è©±
â”‚   â”‚   â”œâ”€â”€ dynamic-planning.md        # å‹•æ…‹è¦åŠƒ
â”‚   â”‚   â””â”€â”€ nested-workflows.md        # åµŒå¥—å·¥ä½œæµ
â”‚   â”‚
â”‚   â”œâ”€â”€ api-reference/
â”‚   â”‚   â”œâ”€â”€ concurrent-api.md          # ä¸¦è¡Œ API
â”‚   â”‚   â”œâ”€â”€ handoff-api.md             # äº¤æ¥ API
â”‚   â”‚   â”œâ”€â”€ groupchat-api.md           # ç¾¤çµ„ API
â”‚   â”‚   â”œâ”€â”€ planning-api.md            # è¦åŠƒ API
â”‚   â”‚   â””â”€â”€ nested-api.md              # åµŒå¥— API
â”‚   â”‚
â”‚   â”œâ”€â”€ tutorials/
â”‚   â”‚   â”œâ”€â”€ build-parallel-workflow.md # å»ºç«‹ä¸¦è¡Œå·¥ä½œæµ
â”‚   â”‚   â”œâ”€â”€ setup-agent-handoff.md     # è¨­ç½® Agent äº¤æ¥
â”‚   â”‚   â”œâ”€â”€ create-groupchat.md        # å»ºç«‹ç¾¤çµ„å°è©±
â”‚   â”‚   â””â”€â”€ design-nested-workflow.md  # è¨­è¨ˆåµŒå¥—å·¥ä½œæµ
â”‚   â”‚
â”‚   â””â”€â”€ best-practices/
â”‚       â”œâ”€â”€ performance-tuning.md      # æ•ˆèƒ½èª¿å„ª
â”‚       â”œâ”€â”€ error-handling.md          # éŒ¯èª¤è™•ç†
â”‚       â””â”€â”€ monitoring.md              # ç›£æ§å»ºè­°
```

#### æ–‡æª”ç¯„ä¾‹

```markdown
# ä¸¦è¡ŒåŸ·è¡ŒæŒ‡å—

## æ¦‚è¿°

IPA Platform çš„ä¸¦è¡ŒåŸ·è¡ŒåŠŸèƒ½è®“æ‚¨å¯ä»¥åŒæ™‚åŸ·è¡Œå¤šå€‹ Agent ä»»å‹™ï¼Œ
å¤§å¹…æå‡å·¥ä½œæµåŸ·è¡Œæ•ˆç‡ã€‚

## æ ¸å¿ƒæ¦‚å¿µ

### Fork-Join æ¨¡å¼

```
â”Œâ”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
â”‚Startâ”‚ â”€â”€â–¶ â”‚  Fork   â”‚ â”€â”€â–¶ â”‚Task1â”‚ â”€â”€â”
â””â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜   â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”
                            â”Œâ”€â”€â”€â”€â”€â”   â”‚ â”€â”€â–¶ â”‚  Join   â”‚ â”€â”€â–¶ â”‚ End â”‚
                            â”‚Task2â”‚ â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”˜
                            â””â”€â”€â”€â”€â”€â”˜
```

### ä½¿ç”¨æ–¹å¼

```python
from ipa_platform import WorkflowBuilder

workflow = (
    WorkflowBuilder()
    .start()
    .fork()
        .add_task("task_1", agent_id="agent_a")
        .add_task("task_2", agent_id="agent_b")
    .join(mode="all")  # ç­‰å¾…æ‰€æœ‰ä»»å‹™å®Œæˆ
    .end()
    .build()
)
```

### API åƒè€ƒ

#### POST /api/v1/concurrent/execute

åŸ·è¡Œä¸¦è¡Œä»»å‹™çµ„ã€‚

**è«‹æ±‚é«”ï¼š**

```json
{
  "tasks": [
    {"agent_id": "uuid", "inputs": {}},
    {"agent_id": "uuid", "inputs": {}}
  ],
  "mode": "all",  // all, any, majority
  "timeout_seconds": 300
}
```

**å›æ‡‰ï¼š**

```json
{
  "execution_id": "uuid",
  "status": "running",
  "tasks": [
    {"id": "uuid", "status": "pending"},
    {"id": "uuid", "status": "pending"}
  ]
}
```

## æœ€ä½³å¯¦è¸

1. **è¨­ç½®åˆç†çš„è¶…æ™‚** - é¿å…é•·æ™‚é–“ç­‰å¾…
2. **ä½¿ç”¨é©ç•¶çš„åˆä½µæ¨¡å¼** - æ ¹æ“šéœ€æ±‚é¸æ“‡ all/any/majority
3. **ç›£æ§ä¸¦ç™¼æ•¸** - é¿å…è³‡æºè€—ç›¡
4. **è™•ç†å¤±æ•—åˆ†æ”¯** - å¯¦ç¾é©ç•¶çš„éŒ¯èª¤è™•ç†

## å¸¸è¦‹å•é¡Œ

### Q: å¦‚ä½•è™•ç†éƒ¨åˆ†ä»»å‹™å¤±æ•—ï¼Ÿ

A: ä½¿ç”¨ `error_handling` é…ç½®ï¼š

```json
{
  "error_handling": {
    "on_failure": "continue",  // continue, abort, retry
    "max_retries": 3
  }
}
```
```

#### é©—æ”¶æ¨™æº–
- [ ] æ‰€æœ‰ Phase 2 åŠŸèƒ½æœ‰æ–‡æª”
- [ ] API åƒè€ƒå®Œæ•´
- [ ] æ•™å­¸æŒ‡å—å¯ç”¨
- [ ] æœ€ä½³å¯¦è¸æ–‡æª”
- [ ] å¸¸è¦‹å•é¡Œè§£ç­”

---

## æ¸¬è©¦è¨ˆåŠƒ

### æ•ˆèƒ½æ¸¬è©¦

```python
# tests/performance/test_phase2_performance.py

import pytest
import asyncio
import time
from locust import HttpUser, task, between


class Phase2PerformanceTest:
    """Phase 2 æ•ˆèƒ½æ¸¬è©¦"""

    @pytest.mark.performance
    async def test_concurrent_execution_throughput(self, client):
        """æ¸¬è©¦ä¸¦è¡ŒåŸ·è¡Œååé‡"""
        # ç›®æ¨™ï¼š3x ååé‡æå‡

        # åŸºæº–æ¸¬è©¦ï¼ˆé †åºåŸ·è¡Œï¼‰
        sequential_times = []
        for _ in range(10):
            start = time.perf_counter()
            await client.post("/api/v1/executions", json={...})
            sequential_times.append(time.perf_counter() - start)

        sequential_avg = sum(sequential_times) / len(sequential_times)

        # ä¸¦è¡Œæ¸¬è©¦
        parallel_start = time.perf_counter()
        tasks = [
            client.post("/api/v1/concurrent/execute", json={...})
            for _ in range(10)
        ]
        await asyncio.gather(*tasks)
        parallel_total = time.perf_counter() - parallel_start

        # é©—è­‰ 3x æå‡
        improvement = (sequential_avg * 10) / parallel_total
        assert improvement >= 3.0

    @pytest.mark.performance
    async def test_groupchat_latency(self, client):
        """æ¸¬è©¦ç¾¤çµ„å°è©±å»¶é²"""
        # ç›®æ¨™ï¼šå¹³å‡å»¶é² < 2ç§’

        latencies = []
        for _ in range(10):
            start = time.perf_counter()
            await client.post(
                "/api/v1/groupchat/{id}/message",
                json={"content": "test"}
            )
            latencies.append(time.perf_counter() - start)

        avg_latency = sum(latencies) / len(latencies)
        assert avg_latency < 2.0

    @pytest.mark.performance
    async def test_nested_workflow_depth(self, client):
        """æ¸¬è©¦åµŒå¥—å·¥ä½œæµæ·±åº¦æ•ˆèƒ½"""
        # ç›®æ¨™ï¼š10 å±¤åµŒå¥— < 30 ç§’

        start = time.perf_counter()
        await client.post(
            "/api/v1/nested/execute/recursive",
            json={
                "workflow_id": "...",
                "initial_inputs": {},
                "max_depth": 10
            }
        )
        elapsed = time.perf_counter() - start

        assert elapsed < 30.0


class Phase2LoadTest(HttpUser):
    """è² è¼‰æ¸¬è©¦"""
    wait_time = between(1, 3)

    @task(3)
    def concurrent_execution(self):
        self.client.post("/api/v1/concurrent/execute", json={...})

    @task(2)
    def groupchat_message(self):
        self.client.post("/api/v1/groupchat/{id}/message", json={...})

    @task(1)
    def planning_decompose(self):
        self.client.post("/api/v1/planning/decompose", json={...})
```

---

## è³‡æ–™åº«é·ç§»

```sql
-- migrations/versions/012_performance_tables.sql

-- æ•ˆèƒ½æŒ‡æ¨™è¡¨
CREATE TABLE performance_metrics (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    session_id UUID,
    name VARCHAR(255) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    value DECIMAL(15,4) NOT NULL,
    unit VARCHAR(50),
    tags JSONB DEFAULT '{}',
    recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆ†ææœƒè©±è¡¨
CREATE TABLE profile_sessions (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    started_at TIMESTAMP NOT NULL,
    ended_at TIMESTAMP,
    summary JSONB,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ•ˆèƒ½åŸºæº–è¡¨
CREATE TABLE performance_baselines (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    name VARCHAR(255) NOT NULL,
    metric_type VARCHAR(50) NOT NULL,
    baseline_value DECIMAL(15,4) NOT NULL,
    threshold_value DECIMAL(15,4),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•
CREATE INDEX idx_perf_metrics_session ON performance_metrics(session_id);
CREATE INDEX idx_perf_metrics_type ON performance_metrics(metric_type);
CREATE INDEX idx_perf_metrics_time ON performance_metrics(recorded_at);
```

---

## Phase 2 å®Œæˆæª¢æŸ¥æ¸…å–®

### åŠŸèƒ½å®Œæ•´æ€§

| Sprint | åŠŸèƒ½ | ç‹€æ…‹ | æ¸¬è©¦è¦†è“‹ |
|--------|------|------|----------|
| Sprint 7 | ä¸¦è¡ŒåŸ·è¡Œ | âœ… | > 85% |
| Sprint 7 | å¢å¼·é–˜é“ | âœ… | > 85% |
| Sprint 8 | Agent äº¤æ¥ | âœ… | > 85% |
| Sprint 8 | å”ä½œå”è­° | âœ… | > 85% |
| Sprint 9 | ç¾¤çµ„èŠå¤© | âœ… | > 85% |
| Sprint 9 | å¤šè¼ªå°è©± | âœ… | > 85% |
| Sprint 9 | å°è©±è¨˜æ†¶ | âœ… | > 85% |
| Sprint 10 | å‹•æ…‹è¦åŠƒ | âœ… | > 85% |
| Sprint 10 | è‡ªä¸»æ±ºç­– | âœ… | > 85% |
| Sprint 10 | è©¦éŒ¯æ©Ÿåˆ¶ | âœ… | > 85% |
| Sprint 11 | åµŒå¥—å·¥ä½œæµ | âœ… | > 85% |
| Sprint 11 | å­å·¥ä½œæµåŸ·è¡Œ | âœ… | > 85% |
| Sprint 11 | éæ­¸æ¨¡å¼ | âœ… | > 85% |
| Sprint 12 | æ•ˆèƒ½å„ªåŒ– | âœ… | > 85% |
| Sprint 12 | UI æ•´åˆ | âœ… | > 85% |
| Sprint 12 | æ–‡æª”æ¸¬è©¦ | âœ… | > 85% |

### æ•ˆèƒ½ KPI

| æŒ‡æ¨™ | ç›®æ¨™ | å¯¦éš› | ç‹€æ…‹ |
|------|------|------|------|
| ä¸¦è¡ŒåŸ·è¡Œæ•ˆç‡ | 3x æå‡ | TBD | â³ |
| Agent å”ä½œæˆåŠŸç‡ | â‰¥ 90% | TBD | â³ |
| å‹•æ…‹è¦åŠƒæº–ç¢ºç‡ | â‰¥ 85% | TBD | â³ |
| å¤šè¼ªå°è©±å®Œæˆç‡ | â‰¥ 90% | TBD | â³ |
| åµŒå¥—å·¥ä½œæµæˆåŠŸç‡ | â‰¥ 95% | TBD | â³ |

### æ–‡æª”å®Œæ•´æ€§

- [ ] Phase 2 æ¦‚è¿°æ–‡æª”
- [ ] å„åŠŸèƒ½ä½¿ç”¨æŒ‡å—
- [ ] API åƒè€ƒæ–‡æª”
- [ ] æ•™å­¸ç¯„ä¾‹
- [ ] æœ€ä½³å¯¦è¸æŒ‡å—
- [ ] æ•…éšœæ’é™¤æŒ‡å—

---

## Definition of Done

- [ ] æ‰€æœ‰ User Stories å®Œæˆ
- [ ] å–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ > 85%
- [ ] æ•´åˆæ¸¬è©¦å…¨éƒ¨é€šé
- [ ] æ•ˆèƒ½æ¸¬è©¦é”æ¨™
- [ ] API æ–‡æª”å®Œæ•´
- [ ] ä½¿ç”¨è€…æŒ‡å—å®Œæˆ
- [ ] ç¨‹å¼ç¢¼å¯©æŸ¥å®Œæˆ
- [ ] éƒ¨ç½²è…³æœ¬æº–å‚™å®Œæˆ

---

## Phase 2 é‡Œç¨‹ç¢‘

### Phase 2 å®Œæˆæ¨™æº–

1. **åŠŸèƒ½å®Œæ•´**ï¼šæ‰€æœ‰ 16 å€‹åŠŸèƒ½é …ç›®å·²å¯¦ç¾ä¸¦æ¸¬è©¦
2. **æ•ˆèƒ½é”æ¨™**ï¼šæ‰€æœ‰æ•ˆèƒ½ KPI é”åˆ°ç›®æ¨™
3. **æ–‡æª”é½Šå…¨**ï¼šæ‰€æœ‰æ–‡æª”å·²ç·¨å¯«ä¸¦å¯©æ ¸
4. **ç©©å®šé‹è¡Œ**ï¼šåœ¨ staging ç’°å¢ƒç©©å®šé‹è¡Œ â‰¥ 1 é€±

### ä¸‹ä¸€æ­¥

Phase 2 å®Œæˆå¾Œçš„å¾ŒçºŒå·¥ä½œï¼š
- ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²
- ç”¨æˆ¶åŸ¹è¨“
- æŒçºŒç›£æ§å’Œå„ªåŒ–
- Phase 3 è¦åŠƒï¼ˆå¦‚æœ‰ï¼‰

---

**æ­å–œï¼Phase 2 è¦åŠƒå®Œæˆã€‚**
