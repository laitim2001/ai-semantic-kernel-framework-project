# Sprint 11: åµŒå¥—å·¥ä½œæµ (Nested Workflows & Advanced Orchestration)

**Sprint ç›®æ¨™**: å¯¦ç¾å·¥ä½œæµåµŒå¥—å’Œéæ­¸åŸ·è¡Œèƒ½åŠ›ï¼Œæ”¯æ´è¤‡é›œçš„éšå±¤å¼æµç¨‹ç·¨æ’

**é€±æœŸ**: Week 23-24 (2 é€±)
**Story Points**: 39 é»
**å‰ç½®æ¢ä»¶**: Sprint 7-10 å®Œæˆ

---

## Sprint æ¦‚è¿°

### æ ¸å¿ƒäº¤ä»˜ç‰©

| ID | åŠŸèƒ½ | å„ªå…ˆç´š | Story Points | ç‹€æ…‹ |
|----|------|--------|--------------|------|
| P2-F11 | Nested Workflows åµŒå¥—å·¥ä½œæµ | ğŸŸ¢ ä½ | 18 | å¾…é–‹ç™¼ |
| P2-F12 | Sub-workflow Execution å­å·¥ä½œæµåŸ·è¡Œ | ğŸŸ¢ ä½ | 13 | å¾…é–‹ç™¼ |
| P2-F13 | Recursive Patterns éæ­¸æ¨¡å¼ | ğŸŸ¢ ä½ | 8 | å¾…é–‹ç™¼ |

### è¨­è¨ˆæ¦‚å¿µ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Parent Workflow                          â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Task A â”‚ â”€â”€â–¶ â”‚     Sub-Workflow 1          â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚                  â”‚  â”‚ 1.1 â”‚â”€â–¶â”‚ 1.2 â”‚â”€â–¶â”‚ 1.3 â”‚  â”‚           â”‚
â”‚                  â”‚  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚         Parallel Sub-Workflows               â”‚           â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚           â”‚
â”‚  â”‚  â”‚ Sub-WF 2.1   â”‚    â”‚ Sub-WF 2.2   â”‚       â”‚           â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”  â”‚    â”‚ â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”  â”‚       â”‚           â”‚
â”‚  â”‚  â”‚ â”‚ A â”‚â”€â”‚ B â”‚  â”‚    â”‚ â”‚ X â”‚â”€â”‚ Y â”‚  â”‚       â”‚           â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜  â”‚    â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜  â”‚       â”‚           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚                              â”‚
â”‚                              â–¼                              â”‚
â”‚                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
â”‚                        â”‚  Task Z â”‚                          â”‚
â”‚                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## User Stories

### Story 11-1: Nested Workflow Manager (8 é»)

**ä½œç‚º** ç³»çµ±æ¶æ§‹å¸«
**æˆ‘å¸Œæœ›** å¯¦ç¾åµŒå¥—å·¥ä½œæµç®¡ç†å™¨
**ä»¥ä¾¿** å·¥ä½œæµå¯ä»¥åŒ…å«å­å·¥ä½œæµï¼Œå½¢æˆéšå±¤çµæ§‹

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/domain/orchestration/nested/workflow_manager.py

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Set
from uuid import UUID, uuid4
from datetime import datetime
from enum import Enum
import asyncio


class NestedWorkflowType(str, Enum):
    """åµŒå¥—å·¥ä½œæµé¡å‹"""
    INLINE = "inline"           # å…§è¯å®šç¾©çš„å­å·¥ä½œæµ
    REFERENCE = "reference"     # å¼•ç”¨ç¾æœ‰å·¥ä½œæµ
    DYNAMIC = "dynamic"         # å‹•æ…‹ç”Ÿæˆçš„å­å·¥ä½œæµ
    RECURSIVE = "recursive"     # éæ­¸èª¿ç”¨è‡ªèº«


class WorkflowScope(str, Enum):
    """å·¥ä½œæµä½œç”¨åŸŸ"""
    ISOLATED = "isolated"       # å®Œå…¨éš”é›¢ï¼Œç¨ç«‹ä¸Šä¸‹æ–‡
    INHERITED = "inherited"     # ç¹¼æ‰¿çˆ¶å·¥ä½œæµä¸Šä¸‹æ–‡
    SHARED = "shared"           # å…±äº«ä¸Šä¸‹æ–‡ï¼Œé›™å‘åŒæ­¥


@dataclass
class NestedWorkflowConfig:
    """åµŒå¥—å·¥ä½œæµé…ç½®"""
    workflow_type: NestedWorkflowType
    scope: WorkflowScope = WorkflowScope.INHERITED
    max_depth: int = 5          # æœ€å¤§åµŒå¥—æ·±åº¦
    timeout_seconds: int = 600  # å­å·¥ä½œæµè¶…æ™‚
    retry_on_failure: bool = True
    max_retries: int = 2
    pass_context: bool = True   # æ˜¯å¦å‚³éä¸Šä¸‹æ–‡
    return_outputs: bool = True # æ˜¯å¦è¿”å›è¼¸å‡º


@dataclass
class SubWorkflowReference:
    """å­å·¥ä½œæµå¼•ç”¨"""
    id: UUID
    parent_workflow_id: UUID
    workflow_id: Optional[UUID]  # å¼•ç”¨çš„å·¥ä½œæµ IDï¼ˆREFERENCE é¡å‹ï¼‰
    definition: Optional[Dict[str, Any]]  # å…§è¯å®šç¾©ï¼ˆINLINE/DYNAMIC é¡å‹ï¼‰
    config: NestedWorkflowConfig
    input_mapping: Dict[str, str]   # çˆ¶ä¸Šä¸‹æ–‡ -> å­è¼¸å…¥çš„æ˜ å°„
    output_mapping: Dict[str, str]  # å­è¼¸å‡º -> çˆ¶ä¸Šä¸‹æ–‡çš„æ˜ å°„
    position: int  # åœ¨çˆ¶å·¥ä½œæµä¸­çš„ä½ç½®
    status: str = "pending"
    created_at: datetime = field(default_factory=datetime.utcnow)


@dataclass
class NestedExecutionContext:
    """åµŒå¥—åŸ·è¡Œä¸Šä¸‹æ–‡"""
    execution_id: UUID
    parent_execution_id: Optional[UUID]
    workflow_id: UUID
    depth: int
    path: List[UUID]  # å¾æ ¹åˆ°ç•¶å‰çš„åŸ·è¡Œè·¯å¾‘
    variables: Dict[str, Any]
    parent_variables: Optional[Dict[str, Any]]
    started_at: datetime = field(default_factory=datetime.utcnow)
    completed_at: Optional[datetime] = None


class NestedWorkflowManager:
    """
    åµŒå¥—å·¥ä½œæµç®¡ç†å™¨

    è² è²¬ï¼š
    - ç®¡ç†å·¥ä½œæµçš„éšå±¤çµæ§‹
    - è™•ç†å­å·¥ä½œæµçš„åŸ·è¡Œ
    - ç®¡ç†ä¸Šä¸‹æ–‡å‚³é
    - æ·±åº¦é™åˆ¶å’Œå¾ªç’°æª¢æ¸¬
    """

    def __init__(
        self,
        workflow_service: Any,
        execution_service: Any,
        max_global_depth: int = 10
    ):
        self.workflow_service = workflow_service
        self.execution_service = execution_service
        self.max_global_depth = max_global_depth

        # æ´»èºçš„åµŒå¥—åŸ·è¡Œ
        self._active_executions: Dict[UUID, NestedExecutionContext] = {}

        # å·¥ä½œæµä¾è³´åœ–ï¼ˆç”¨æ–¼å¾ªç’°æª¢æ¸¬ï¼‰
        self._dependency_graph: Dict[UUID, Set[UUID]] = {}

    async def register_sub_workflow(
        self,
        parent_workflow_id: UUID,
        sub_workflow: SubWorkflowReference
    ) -> None:
        """
        è¨»å†Šå­å·¥ä½œæµ

        Args:
            parent_workflow_id: çˆ¶å·¥ä½œæµ ID
            sub_workflow: å­å·¥ä½œæµå¼•ç”¨
        """
        # æ›´æ–°ä¾è³´åœ–
        if parent_workflow_id not in self._dependency_graph:
            self._dependency_graph[parent_workflow_id] = set()

        if sub_workflow.workflow_id:
            self._dependency_graph[parent_workflow_id].add(sub_workflow.workflow_id)

        # æª¢æŸ¥å¾ªç’°ä¾è³´
        if self._has_cycle(parent_workflow_id):
            raise ValueError(
                f"Circular dependency detected when adding sub-workflow "
                f"to {parent_workflow_id}"
            )

    def _has_cycle(self, start_id: UUID) -> bool:
        """æª¢æ¸¬å¾ªç’°ä¾è³´"""
        visited = set()
        rec_stack = set()

        def dfs(node_id: UUID) -> bool:
            visited.add(node_id)
            rec_stack.add(node_id)

            for neighbor in self._dependency_graph.get(node_id, set()):
                if neighbor not in visited:
                    if dfs(neighbor):
                        return True
                elif neighbor in rec_stack:
                    return True

            rec_stack.remove(node_id)
            return False

        return dfs(start_id)

    async def execute_sub_workflow(
        self,
        parent_context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå­å·¥ä½œæµ

        Args:
            parent_context: çˆ¶åŸ·è¡Œä¸Šä¸‹æ–‡
            sub_workflow: å­å·¥ä½œæµå¼•ç”¨

        Returns:
            åŸ·è¡Œçµæœ
        """
        # æª¢æŸ¥æ·±åº¦é™åˆ¶
        new_depth = parent_context.depth + 1
        if new_depth > sub_workflow.config.max_depth:
            raise ValueError(
                f"Maximum nesting depth ({sub_workflow.config.max_depth}) exceeded"
            )

        if new_depth > self.max_global_depth:
            raise ValueError(
                f"Global maximum nesting depth ({self.max_global_depth}) exceeded"
            )

        # å»ºç«‹å­åŸ·è¡Œä¸Šä¸‹æ–‡
        child_context = self._create_child_context(
            parent_context,
            sub_workflow
        )

        self._active_executions[child_context.execution_id] = child_context

        try:
            # æ ¹æ“šé¡å‹åŸ·è¡Œ
            if sub_workflow.config.workflow_type == NestedWorkflowType.REFERENCE:
                result = await self._execute_reference_workflow(
                    child_context, sub_workflow
                )
            elif sub_workflow.config.workflow_type == NestedWorkflowType.INLINE:
                result = await self._execute_inline_workflow(
                    child_context, sub_workflow
                )
            elif sub_workflow.config.workflow_type == NestedWorkflowType.DYNAMIC:
                result = await self._execute_dynamic_workflow(
                    child_context, sub_workflow
                )
            elif sub_workflow.config.workflow_type == NestedWorkflowType.RECURSIVE:
                result = await self._execute_recursive_workflow(
                    child_context, sub_workflow
                )
            else:
                raise ValueError(f"Unknown workflow type: {sub_workflow.config.workflow_type}")

            # æ˜ å°„è¼¸å‡ºåˆ°çˆ¶ä¸Šä¸‹æ–‡
            if sub_workflow.config.return_outputs:
                self._map_outputs(parent_context, sub_workflow, result)

            child_context.completed_at = datetime.utcnow()
            return result

        except asyncio.TimeoutError:
            raise TimeoutError(
                f"Sub-workflow {sub_workflow.id} timed out "
                f"after {sub_workflow.config.timeout_seconds} seconds"
            )
        finally:
            del self._active_executions[child_context.execution_id]

    def _create_child_context(
        self,
        parent_context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> NestedExecutionContext:
        """å»ºç«‹å­åŸ·è¡Œä¸Šä¸‹æ–‡"""
        child_variables = {}

        # æ ¹æ“šä½œç”¨åŸŸè™•ç†è®Šæ•¸
        if sub_workflow.config.scope == WorkflowScope.INHERITED:
            child_variables = parent_context.variables.copy()
        elif sub_workflow.config.scope == WorkflowScope.SHARED:
            child_variables = parent_context.variables  # å…±äº«å¼•ç”¨
        # ISOLATED: ä¿æŒç©ºçš„ child_variables

        # æ‡‰ç”¨è¼¸å…¥æ˜ å°„
        for parent_key, child_key in sub_workflow.input_mapping.items():
            if parent_key in parent_context.variables:
                child_variables[child_key] = parent_context.variables[parent_key]

        return NestedExecutionContext(
            execution_id=uuid4(),
            parent_execution_id=parent_context.execution_id,
            workflow_id=sub_workflow.workflow_id or sub_workflow.id,
            depth=parent_context.depth + 1,
            path=parent_context.path + [parent_context.execution_id],
            variables=child_variables,
            parent_variables=parent_context.variables if sub_workflow.config.scope == WorkflowScope.SHARED else None
        )

    async def _execute_reference_workflow(
        self,
        context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> Dict[str, Any]:
        """åŸ·è¡Œå¼•ç”¨çš„å·¥ä½œæµ"""
        return await asyncio.wait_for(
            self.execution_service.execute_workflow(
                workflow_id=sub_workflow.workflow_id,
                inputs=context.variables,
                parent_execution_id=context.parent_execution_id
            ),
            timeout=sub_workflow.config.timeout_seconds
        )

    async def _execute_inline_workflow(
        self,
        context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> Dict[str, Any]:
        """åŸ·è¡Œå…§è¯å®šç¾©çš„å·¥ä½œæµ"""
        # å¾å®šç¾©ä¸­å»ºç«‹è‡¨æ™‚å·¥ä½œæµ
        definition = sub_workflow.definition
        if not definition:
            raise ValueError("Inline workflow requires definition")

        # ä½¿ç”¨å·¥ä½œæµæœå‹™åŸ·è¡Œå®šç¾©
        return await asyncio.wait_for(
            self.execution_service.execute_workflow_definition(
                definition=definition,
                inputs=context.variables,
                parent_execution_id=context.parent_execution_id
            ),
            timeout=sub_workflow.config.timeout_seconds
        )

    async def _execute_dynamic_workflow(
        self,
        context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> Dict[str, Any]:
        """åŸ·è¡Œå‹•æ…‹ç”Ÿæˆçš„å·¥ä½œæµ"""
        # å‹•æ…‹å·¥ä½œæµçš„å®šç¾©å¯èƒ½æ˜¯ä¸€å€‹ç”Ÿæˆå™¨å‡½æ•¸
        generator = sub_workflow.definition.get("generator")
        if callable(generator):
            definition = await generator(context.variables)
        else:
            definition = sub_workflow.definition

        return await asyncio.wait_for(
            self.execution_service.execute_workflow_definition(
                definition=definition,
                inputs=context.variables,
                parent_execution_id=context.parent_execution_id
            ),
            timeout=sub_workflow.config.timeout_seconds
        )

    async def _execute_recursive_workflow(
        self,
        context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œéæ­¸å·¥ä½œæµ

        éæ­¸å·¥ä½œæµèª¿ç”¨è‡ªèº«ï¼Œç›´åˆ°æ»¿è¶³çµ‚æ­¢æ¢ä»¶
        """
        # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
        termination_condition = sub_workflow.definition.get("termination_condition")
        if termination_condition and termination_condition(context.variables):
            return {"status": "terminated", "depth": context.depth}

        # éæ­¸åŸ·è¡Œ
        return await self._execute_reference_workflow(context, sub_workflow)

    def _map_outputs(
        self,
        parent_context: NestedExecutionContext,
        sub_workflow: SubWorkflowReference,
        result: Dict[str, Any]
    ) -> None:
        """å°‡å­å·¥ä½œæµè¼¸å‡ºæ˜ å°„å›çˆ¶ä¸Šä¸‹æ–‡"""
        for child_key, parent_key in sub_workflow.output_mapping.items():
            if child_key in result:
                parent_context.variables[parent_key] = result[child_key]

    def get_execution_tree(
        self,
        root_execution_id: UUID
    ) -> Dict[str, Any]:
        """ç²å–åŸ·è¡Œæ¨¹çµæ§‹"""
        def build_tree(exec_id: UUID) -> Dict[str, Any]:
            context = self._active_executions.get(exec_id)
            if not context:
                return {"id": str(exec_id), "status": "not_found"}

            children = [
                build_tree(child_id)
                for child_id, child_ctx in self._active_executions.items()
                if child_ctx.parent_execution_id == exec_id
            ]

            return {
                "id": str(exec_id),
                "workflow_id": str(context.workflow_id),
                "depth": context.depth,
                "started_at": context.started_at.isoformat(),
                "children": children
            }

        return build_tree(root_execution_id)

    async def cancel_nested_execution(
        self,
        execution_id: UUID,
        cascade: bool = True
    ) -> None:
        """
        å–æ¶ˆåµŒå¥—åŸ·è¡Œ

        Args:
            execution_id: åŸ·è¡Œ ID
            cascade: æ˜¯å¦ç´šè¯å–æ¶ˆæ‰€æœ‰å­åŸ·è¡Œ
        """
        if cascade:
            # æ‰¾å‡ºæ‰€æœ‰å­åŸ·è¡Œ
            children_to_cancel = [
                child_id
                for child_id, ctx in self._active_executions.items()
                if execution_id in ctx.path
            ]

            for child_id in children_to_cancel:
                await self.execution_service.cancel_execution(child_id)

        await self.execution_service.cancel_execution(execution_id)
```

#### é©—æ”¶æ¨™æº–
- [ ] æ”¯æ´å·¥ä½œæµåµŒå¥—çµæ§‹
- [ ] æ­£ç¢ºçš„æ·±åº¦é™åˆ¶
- [ ] å¾ªç’°ä¾è³´æª¢æ¸¬
- [ ] ä¸Šä¸‹æ–‡å‚³éæ­£ç¢º
- [ ] å–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ > 85%

---

### Story 11-2: Sub-workflow Executor (5 é»)

**ä½œç‚º** ç³»çµ±æ¶æ§‹å¸«
**æˆ‘å¸Œæœ›** å¯¦ç¾å­å·¥ä½œæµåŸ·è¡Œå™¨
**ä»¥ä¾¿** å­å·¥ä½œæµå¯ä»¥ç¨ç«‹åŸ·è¡Œä¸¦è¿”å›çµæœ

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/domain/orchestration/nested/sub_executor.py

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Callable, Awaitable
from uuid import UUID, uuid4
from datetime import datetime
from enum import Enum
import asyncio


class SubWorkflowExecutionMode(str, Enum):
    """å­å·¥ä½œæµåŸ·è¡Œæ¨¡å¼"""
    SYNC = "sync"               # åŒæ­¥ç­‰å¾…å®Œæˆ
    ASYNC = "async"             # ç•°æ­¥åŸ·è¡Œï¼Œä¸ç­‰å¾…
    FIRE_AND_FORGET = "fire_and_forget"  # ç™¼å°„å³å¿˜
    CALLBACK = "callback"       # å®Œæˆå¾Œå›èª¿


@dataclass
class SubExecutionState:
    """å­åŸ·è¡Œç‹€æ…‹"""
    execution_id: UUID
    sub_workflow_id: UUID
    mode: SubWorkflowExecutionMode
    status: str = "pending"
    result: Optional[Dict[str, Any]] = None
    error: Optional[str] = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    callback: Optional[Callable] = None


class SubWorkflowExecutor:
    """
    å­å·¥ä½œæµåŸ·è¡Œå™¨

    è² è²¬åŸ·è¡ŒåµŒå¥—çš„å­å·¥ä½œæµ
    """

    def __init__(
        self,
        workflow_engine: Any,
        checkpoint_service: Any
    ):
        self.workflow_engine = workflow_engine
        self.checkpoint_service = checkpoint_service

        # åŸ·è¡Œç‹€æ…‹è¿½è¹¤
        self._executions: Dict[UUID, SubExecutionState] = {}

        # ç•°æ­¥åŸ·è¡Œçš„ä»»å‹™
        self._async_tasks: Dict[UUID, asyncio.Task] = {}

    async def execute(
        self,
        sub_workflow_id: UUID,
        inputs: Dict[str, Any],
        mode: SubWorkflowExecutionMode = SubWorkflowExecutionMode.SYNC,
        callback: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œå­å·¥ä½œæµ

        Args:
            sub_workflow_id: å­å·¥ä½œæµ ID
            inputs: è¼¸å…¥åƒæ•¸
            mode: åŸ·è¡Œæ¨¡å¼
            callback: å®Œæˆå›èª¿ï¼ˆCALLBACK æ¨¡å¼ä½¿ç”¨ï¼‰

        Returns:
            åŸ·è¡Œçµæœï¼ˆåŒæ­¥æ¨¡å¼ï¼‰æˆ–åŸ·è¡Œ IDï¼ˆç•°æ­¥æ¨¡å¼ï¼‰
        """
        execution_id = uuid4()

        state = SubExecutionState(
            execution_id=execution_id,
            sub_workflow_id=sub_workflow_id,
            mode=mode,
            callback=callback
        )
        self._executions[execution_id] = state

        if mode == SubWorkflowExecutionMode.SYNC:
            return await self._execute_sync(state, inputs)

        elif mode == SubWorkflowExecutionMode.ASYNC:
            task = asyncio.create_task(
                self._execute_async(state, inputs)
            )
            self._async_tasks[execution_id] = task
            return {"execution_id": str(execution_id), "status": "started"}

        elif mode == SubWorkflowExecutionMode.FIRE_AND_FORGET:
            asyncio.create_task(self._execute_fire_forget(state, inputs))
            return {"execution_id": str(execution_id), "status": "dispatched"}

        elif mode == SubWorkflowExecutionMode.CALLBACK:
            if not callback:
                raise ValueError("Callback mode requires a callback function")
            task = asyncio.create_task(
                self._execute_with_callback(state, inputs)
            )
            self._async_tasks[execution_id] = task
            return {"execution_id": str(execution_id), "status": "started"}

    async def _execute_sync(
        self,
        state: SubExecutionState,
        inputs: Dict[str, Any]
    ) -> Dict[str, Any]:
        """åŒæ­¥åŸ·è¡Œ"""
        state.status = "running"
        state.started_at = datetime.utcnow()

        try:
            result = await self.workflow_engine.execute(
                workflow_id=state.sub_workflow_id,
                inputs=inputs
            )

            state.status = "completed"
            state.result = result
            state.completed_at = datetime.utcnow()

            return result

        except Exception as e:
            state.status = "failed"
            state.error = str(e)
            state.completed_at = datetime.utcnow()
            raise

    async def _execute_async(
        self,
        state: SubExecutionState,
        inputs: Dict[str, Any]
    ) -> None:
        """ç•°æ­¥åŸ·è¡Œ"""
        await self._execute_sync(state, inputs)

    async def _execute_fire_forget(
        self,
        state: SubExecutionState,
        inputs: Dict[str, Any]
    ) -> None:
        """ç™¼å°„å³å¿˜åŸ·è¡Œ"""
        try:
            await self._execute_sync(state, inputs)
        except Exception:
            # è¨˜éŒ„éŒ¯èª¤ä½†ä¸æ‹‹å‡º
            pass

    async def _execute_with_callback(
        self,
        state: SubExecutionState,
        inputs: Dict[str, Any]
    ) -> None:
        """å¸¶å›èª¿çš„åŸ·è¡Œ"""
        try:
            result = await self._execute_sync(state, inputs)
            if state.callback:
                await state.callback(result)
        except Exception as e:
            if state.callback:
                await state.callback({"error": str(e)})

    async def get_execution_status(
        self,
        execution_id: UUID
    ) -> Dict[str, Any]:
        """ç²å–åŸ·è¡Œç‹€æ…‹"""
        state = self._executions.get(execution_id)
        if not state:
            return {"error": "Execution not found"}

        return {
            "execution_id": str(execution_id),
            "sub_workflow_id": str(state.sub_workflow_id),
            "status": state.status,
            "result": state.result,
            "error": state.error,
            "started_at": state.started_at.isoformat() if state.started_at else None,
            "completed_at": state.completed_at.isoformat() if state.completed_at else None
        }

    async def wait_for_completion(
        self,
        execution_id: UUID,
        timeout: Optional[float] = None
    ) -> Dict[str, Any]:
        """ç­‰å¾…åŸ·è¡Œå®Œæˆ"""
        task = self._async_tasks.get(execution_id)
        if not task:
            state = self._executions.get(execution_id)
            if state and state.status in ["completed", "failed"]:
                return await self.get_execution_status(execution_id)
            return {"error": "Execution not found or not async"}

        try:
            await asyncio.wait_for(task, timeout=timeout)
        except asyncio.TimeoutError:
            return {"error": "Wait timeout", "status": "running"}

        return await self.get_execution_status(execution_id)

    async def cancel_execution(
        self,
        execution_id: UUID
    ) -> bool:
        """å–æ¶ˆåŸ·è¡Œ"""
        task = self._async_tasks.get(execution_id)
        if task:
            task.cancel()
            try:
                await task
            except asyncio.CancelledError:
                pass

            state = self._executions.get(execution_id)
            if state:
                state.status = "cancelled"

            return True

        return False

    async def execute_parallel(
        self,
        sub_workflows: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹å­å·¥ä½œæµ

        Args:
            sub_workflows: å­å·¥ä½œæµé…ç½®åˆ—è¡¨
                [{"id": UUID, "inputs": {...}}, ...]

        Returns:
            åŸ·è¡Œçµæœåˆ—è¡¨
        """
        tasks = []
        for sw in sub_workflows:
            task = self._execute_sync(
                SubExecutionState(
                    execution_id=uuid4(),
                    sub_workflow_id=sw["id"],
                    mode=SubWorkflowExecutionMode.SYNC
                ),
                sw.get("inputs", {})
            )
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        return [
            result if not isinstance(result, Exception)
            else {"error": str(result)}
            for result in results
        ]

    async def execute_sequential(
        self,
        sub_workflows: List[Dict[str, Any]],
        pass_outputs: bool = True
    ) -> List[Dict[str, Any]]:
        """
        é †åºåŸ·è¡Œå¤šå€‹å­å·¥ä½œæµ

        Args:
            sub_workflows: å­å·¥ä½œæµé…ç½®åˆ—è¡¨
            pass_outputs: æ˜¯å¦å°‡å‰ä¸€å€‹è¼¸å‡ºå‚³çµ¦ä¸‹ä¸€å€‹

        Returns:
            åŸ·è¡Œçµæœåˆ—è¡¨
        """
        results = []
        previous_output = {}

        for sw in sub_workflows:
            inputs = sw.get("inputs", {})
            if pass_outputs:
                inputs = {**previous_output, **inputs}

            state = SubExecutionState(
                execution_id=uuid4(),
                sub_workflow_id=sw["id"],
                mode=SubWorkflowExecutionMode.SYNC
            )

            try:
                result = await self._execute_sync(state, inputs)
                results.append(result)
                previous_output = result
            except Exception as e:
                results.append({"error": str(e)})
                if sw.get("stop_on_error", True):
                    break

        return results
```

#### é©—æ”¶æ¨™æº–
- [ ] æ”¯æ´åŒæ­¥/ç•°æ­¥åŸ·è¡Œæ¨¡å¼
- [ ] æ”¯æ´å›èª¿æ©Ÿåˆ¶
- [ ] æ”¯æ´ä¸¦è¡Œ/é †åºåŸ·è¡Œ
- [ ] åŸ·è¡Œç‹€æ…‹è¿½è¹¤
- [ ] å–æ¶ˆåŸ·è¡ŒåŠŸèƒ½

---

### Story 11-3: Recursive Pattern Handler (5 é»)

**ä½œç‚º** ç³»çµ±æ¶æ§‹å¸«
**æˆ‘å¸Œæœ›** å¯¦ç¾éæ­¸æ¨¡å¼è™•ç†å™¨
**ä»¥ä¾¿** å·¥ä½œæµå¯ä»¥å®‰å…¨åœ°éæ­¸åŸ·è¡Œ

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/domain/orchestration/nested/recursive_handler.py

from dataclasses import dataclass, field
from typing import Dict, Any, Optional, Callable, List
from uuid import UUID, uuid4
from datetime import datetime
from enum import Enum


class RecursionStrategy(str, Enum):
    """éæ­¸ç­–ç•¥"""
    DEPTH_FIRST = "depth_first"     # æ·±åº¦å„ªå…ˆ
    BREADTH_FIRST = "breadth_first" # å»£åº¦å„ªå…ˆ
    PARALLEL = "parallel"           # ä¸¦è¡Œéæ­¸


class TerminationType(str, Enum):
    """çµ‚æ­¢é¡å‹"""
    CONDITION = "condition"         # æ¢ä»¶æ»¿è¶³
    MAX_DEPTH = "max_depth"         # é”åˆ°æœ€å¤§æ·±åº¦
    MAX_ITERATIONS = "max_iterations"  # é”åˆ°æœ€å¤§è¿­ä»£
    TIMEOUT = "timeout"             # è¶…æ™‚
    CONVERGENCE = "convergence"     # çµæœæ”¶æ–‚


@dataclass
class RecursionConfig:
    """éæ­¸é…ç½®"""
    max_depth: int = 10
    max_iterations: int = 100
    timeout_seconds: int = 300
    strategy: RecursionStrategy = RecursionStrategy.DEPTH_FIRST
    termination_condition: Optional[Callable[[Dict[str, Any], int], bool]] = None
    convergence_threshold: Optional[float] = None
    memoization: bool = True  # æ˜¯å¦å•Ÿç”¨è¨˜æ†¶åŒ–


@dataclass
class RecursionState:
    """éæ­¸ç‹€æ…‹"""
    id: UUID
    workflow_id: UUID
    current_depth: int
    iteration_count: int
    history: List[Dict[str, Any]] = field(default_factory=list)
    memo: Dict[str, Any] = field(default_factory=dict)
    started_at: datetime = field(default_factory=datetime.utcnow)
    terminated: bool = False
    termination_type: Optional[TerminationType] = None


class RecursivePatternHandler:
    """
    éæ­¸æ¨¡å¼è™•ç†å™¨

    å®‰å…¨åœ°è™•ç†å·¥ä½œæµçš„éæ­¸åŸ·è¡Œ
    """

    def __init__(
        self,
        sub_executor: "SubWorkflowExecutor",
        config: RecursionConfig
    ):
        self.sub_executor = sub_executor
        self.config = config

        # æ´»èºçš„éæ­¸ç‹€æ…‹
        self._states: Dict[UUID, RecursionState] = {}

    async def execute_recursive(
        self,
        workflow_id: UUID,
        initial_inputs: Dict[str, Any],
        recursive_inputs_fn: Callable[[Dict[str, Any]], Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œéæ­¸å·¥ä½œæµ

        Args:
            workflow_id: å·¥ä½œæµ ID
            initial_inputs: åˆå§‹è¼¸å…¥
            recursive_inputs_fn: ç”Ÿæˆä¸‹ä¸€æ¬¡éæ­¸è¼¸å…¥çš„å‡½æ•¸

        Returns:
            æœ€çµ‚çµæœ
        """
        state = RecursionState(
            id=uuid4(),
            workflow_id=workflow_id,
            current_depth=0,
            iteration_count=0
        )
        self._states[state.id] = state

        try:
            return await self._recursive_execute(
                state=state,
                inputs=initial_inputs,
                recursive_inputs_fn=recursive_inputs_fn
            )
        finally:
            del self._states[state.id]

    async def _recursive_execute(
        self,
        state: RecursionState,
        inputs: Dict[str, Any],
        recursive_inputs_fn: Callable
    ) -> Dict[str, Any]:
        """å…§éƒ¨éæ­¸åŸ·è¡Œ"""
        # æª¢æŸ¥è¨˜æ†¶åŒ–
        if self.config.memoization:
            memo_key = self._generate_memo_key(inputs)
            if memo_key in state.memo:
                return state.memo[memo_key]

        # æª¢æŸ¥çµ‚æ­¢æ¢ä»¶
        termination = self._check_termination(state, inputs)
        if termination:
            state.terminated = True
            state.termination_type = termination
            return self._build_termination_result(state, inputs, termination)

        # åŸ·è¡Œç•¶å‰å±¤
        state.current_depth += 1
        state.iteration_count += 1

        result = await self.sub_executor.execute(
            sub_workflow_id=state.workflow_id,
            inputs=inputs,
            mode=SubWorkflowExecutionMode.SYNC
        )

        state.history.append({
            "depth": state.current_depth,
            "inputs": inputs,
            "result": result,
            "timestamp": datetime.utcnow().isoformat()
        })

        # æª¢æŸ¥æ˜¯å¦éœ€è¦ç¹¼çºŒéæ­¸
        if self._should_continue(state, result):
            # ç”Ÿæˆä¸‹ä¸€æ¬¡è¼¸å…¥
            next_inputs = recursive_inputs_fn(result)

            # æ ¹æ“šç­–ç•¥åŸ·è¡Œ
            if self.config.strategy == RecursionStrategy.DEPTH_FIRST:
                result = await self._recursive_execute(
                    state, next_inputs, recursive_inputs_fn
                )
            elif self.config.strategy == RecursionStrategy.PARALLEL:
                # ä¸¦è¡Œéæ­¸ï¼ˆå¦‚æœæœ‰å¤šå€‹åˆ†æ”¯ï¼‰
                if isinstance(next_inputs, list):
                    import asyncio
                    tasks = [
                        self._recursive_execute(state, inp, recursive_inputs_fn)
                        for inp in next_inputs
                    ]
                    results = await asyncio.gather(*tasks)
                    result = self._merge_results(results)
                else:
                    result = await self._recursive_execute(
                        state, next_inputs, recursive_inputs_fn
                    )

        # è¨˜æ†¶åŒ–çµæœ
        if self.config.memoization:
            memo_key = self._generate_memo_key(inputs)
            state.memo[memo_key] = result

        state.current_depth -= 1
        return result

    def _check_termination(
        self,
        state: RecursionState,
        inputs: Dict[str, Any]
    ) -> Optional[TerminationType]:
        """æª¢æŸ¥çµ‚æ­¢æ¢ä»¶"""
        # æª¢æŸ¥æœ€å¤§æ·±åº¦
        if state.current_depth >= self.config.max_depth:
            return TerminationType.MAX_DEPTH

        # æª¢æŸ¥æœ€å¤§è¿­ä»£
        if state.iteration_count >= self.config.max_iterations:
            return TerminationType.MAX_ITERATIONS

        # æª¢æŸ¥è¶…æ™‚
        elapsed = (datetime.utcnow() - state.started_at).total_seconds()
        if elapsed >= self.config.timeout_seconds:
            return TerminationType.TIMEOUT

        # æª¢æŸ¥è‡ªå®šç¾©æ¢ä»¶
        if self.config.termination_condition:
            if self.config.termination_condition(inputs, state.current_depth):
                return TerminationType.CONDITION

        # æª¢æŸ¥æ”¶æ–‚
        if self.config.convergence_threshold and len(state.history) >= 2:
            if self._check_convergence(state):
                return TerminationType.CONVERGENCE

        return None

    def _check_convergence(self, state: RecursionState) -> bool:
        """æª¢æŸ¥çµæœæ˜¯å¦æ”¶æ–‚"""
        if len(state.history) < 2:
            return False

        last_result = state.history[-1].get("result", {})
        prev_result = state.history[-2].get("result", {})

        # ç°¡å–®çš„æ”¶æ–‚æª¢æŸ¥ï¼šæ¯”è¼ƒçµæœçš„è®ŠåŒ–
        try:
            diff = self._calculate_diff(last_result, prev_result)
            return diff < self.config.convergence_threshold
        except:
            return False

    def _calculate_diff(
        self,
        result1: Dict[str, Any],
        result2: Dict[str, Any]
    ) -> float:
        """è¨ˆç®—çµæœå·®ç•°"""
        # ç°¡å–®å¯¦ç¾ï¼šè¨ˆç®—æ•¸å€¼æ¬„ä½çš„å¹³å‡è®ŠåŒ–
        diffs = []
        for key in result1:
            if key in result2:
                val1 = result1[key]
                val2 = result2[key]
                if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
                    diffs.append(abs(val1 - val2))

        return sum(diffs) / len(diffs) if diffs else float('inf')

    def _should_continue(
        self,
        state: RecursionState,
        result: Dict[str, Any]
    ) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç¹¼çºŒéæ­¸"""
        if state.terminated:
            return False

        # æª¢æŸ¥çµæœä¸­æ˜¯å¦æœ‰ç¹¼çºŒä¿¡è™Ÿ
        if result.get("continue_recursion") is False:
            return False

        return True

    def _generate_memo_key(self, inputs: Dict[str, Any]) -> str:
        """ç”Ÿæˆè¨˜æ†¶åŒ–éµ"""
        import hashlib
        import json

        # å°‡è¼¸å…¥è½‰ç‚ºå¯å“ˆå¸Œçš„å­—ä¸²
        serialized = json.dumps(inputs, sort_keys=True, default=str)
        return hashlib.md5(serialized.encode()).hexdigest()

    def _build_termination_result(
        self,
        state: RecursionState,
        last_inputs: Dict[str, Any],
        termination_type: TerminationType
    ) -> Dict[str, Any]:
        """æ§‹å»ºçµ‚æ­¢çµæœ"""
        return {
            "status": "terminated",
            "termination_type": termination_type.value,
            "depth_reached": state.current_depth,
            "total_iterations": state.iteration_count,
            "last_inputs": last_inputs,
            "history_length": len(state.history)
        }

    def _merge_results(
        self,
        results: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """åˆä½µä¸¦è¡Œéæ­¸çµæœ"""
        merged = {
            "branches": len(results),
            "results": results
        }

        # å˜—è©¦åˆä½µæ•¸å€¼æ¬„ä½
        numeric_keys = set()
        for r in results:
            for k, v in r.items():
                if isinstance(v, (int, float)):
                    numeric_keys.add(k)

        for key in numeric_keys:
            values = [r.get(key) for r in results if key in r]
            if values:
                merged[f"{key}_sum"] = sum(values)
                merged[f"{key}_avg"] = sum(values) / len(values)

        return merged

    def get_recursion_stats(
        self,
        state_id: UUID
    ) -> Dict[str, Any]:
        """ç²å–éæ­¸çµ±è¨ˆ"""
        state = self._states.get(state_id)
        if not state:
            return {"error": "State not found"}

        return {
            "id": str(state_id),
            "workflow_id": str(state.workflow_id),
            "current_depth": state.current_depth,
            "iteration_count": state.iteration_count,
            "history_length": len(state.history),
            "memo_size": len(state.memo),
            "terminated": state.terminated,
            "termination_type": state.termination_type.value if state.termination_type else None,
            "elapsed_seconds": (datetime.utcnow() - state.started_at).total_seconds()
        }
```

#### é©—æ”¶æ¨™æº–
- [ ] æ”¯æ´å¤šç¨®éæ­¸ç­–ç•¥
- [ ] æ­£ç¢ºçš„çµ‚æ­¢æ¢ä»¶æª¢æ¸¬
- [ ] è¨˜æ†¶åŒ–åŠŸèƒ½
- [ ] æ”¶æ–‚æª¢æ¸¬
- [ ] éæ­¸çµ±è¨ˆ

---

### Story 11-4: Workflow Composition Builder (5 é»)

**ä½œç‚º** ç³»çµ±æ¶æ§‹å¸«
**æˆ‘å¸Œæœ›** å¯¦ç¾å·¥ä½œæµçµ„åˆå»ºæ§‹å™¨
**ä»¥ä¾¿** å¯ä»¥éˆæ´»çµ„åˆå­å·¥ä½œæµ

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/domain/orchestration/nested/composition_builder.py

from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional, Union
from uuid import UUID, uuid4
from enum import Enum


class CompositionType(str, Enum):
    """çµ„åˆé¡å‹"""
    SEQUENCE = "sequence"       # é †åºçµ„åˆ
    PARALLEL = "parallel"       # ä¸¦è¡Œçµ„åˆ
    CONDITIONAL = "conditional" # æ¢ä»¶çµ„åˆ
    LOOP = "loop"               # è¿´åœˆçµ„åˆ
    SWITCH = "switch"           # åˆ†æ”¯çµ„åˆ


@dataclass
class WorkflowNode:
    """å·¥ä½œæµç¯€é»"""
    id: UUID
    workflow_id: Optional[UUID]  # å¼•ç”¨ç¾æœ‰å·¥ä½œæµ
    inline_definition: Optional[Dict[str, Any]]  # å…§è¯å®šç¾©
    name: str
    inputs_mapping: Dict[str, str] = field(default_factory=dict)
    outputs_mapping: Dict[str, str] = field(default_factory=dict)


@dataclass
class CompositionBlock:
    """çµ„åˆå¡Š"""
    id: UUID
    composition_type: CompositionType
    nodes: List[Union[WorkflowNode, "CompositionBlock"]]
    condition: Optional[str] = None  # æ¢ä»¶è¡¨é”å¼
    loop_config: Optional[Dict[str, Any]] = None
    switch_cases: Optional[Dict[str, Any]] = None


class WorkflowCompositionBuilder:
    """
    å·¥ä½œæµçµ„åˆå»ºæ§‹å™¨

    æä¾›æµæš¢çš„ API ä¾†çµ„åˆå­å·¥ä½œæµ
    """

    def __init__(self):
        self._root: Optional[CompositionBlock] = None
        self._current_block: Optional[CompositionBlock] = None
        self._block_stack: List[CompositionBlock] = []

    def sequence(self) -> "WorkflowCompositionBuilder":
        """é–‹å§‹é †åºçµ„åˆ"""
        block = CompositionBlock(
            id=uuid4(),
            composition_type=CompositionType.SEQUENCE,
            nodes=[]
        )
        self._push_block(block)
        return self

    def parallel(self) -> "WorkflowCompositionBuilder":
        """é–‹å§‹ä¸¦è¡Œçµ„åˆ"""
        block = CompositionBlock(
            id=uuid4(),
            composition_type=CompositionType.PARALLEL,
            nodes=[]
        )
        self._push_block(block)
        return self

    def conditional(
        self,
        condition: str
    ) -> "WorkflowCompositionBuilder":
        """é–‹å§‹æ¢ä»¶çµ„åˆ"""
        block = CompositionBlock(
            id=uuid4(),
            composition_type=CompositionType.CONDITIONAL,
            nodes=[],
            condition=condition
        )
        self._push_block(block)
        return self

    def loop(
        self,
        max_iterations: int = 10,
        condition: Optional[str] = None
    ) -> "WorkflowCompositionBuilder":
        """é–‹å§‹è¿´åœˆçµ„åˆ"""
        block = CompositionBlock(
            id=uuid4(),
            composition_type=CompositionType.LOOP,
            nodes=[],
            loop_config={
                "max_iterations": max_iterations,
                "condition": condition
            }
        )
        self._push_block(block)
        return self

    def switch(
        self,
        expression: str
    ) -> "WorkflowCompositionBuilder":
        """é–‹å§‹åˆ†æ”¯çµ„åˆ"""
        block = CompositionBlock(
            id=uuid4(),
            composition_type=CompositionType.SWITCH,
            nodes=[],
            switch_cases={"expression": expression, "cases": {}}
        )
        self._push_block(block)
        return self

    def case(
        self,
        value: Any
    ) -> "WorkflowCompositionBuilder":
        """æ·»åŠ  switch case"""
        if not self._current_block or self._current_block.composition_type != CompositionType.SWITCH:
            raise ValueError("case() must be called within switch()")

        self._current_block.switch_cases["cases"][value] = []
        return self

    def add_workflow(
        self,
        workflow_id: UUID,
        name: Optional[str] = None,
        inputs_mapping: Optional[Dict[str, str]] = None,
        outputs_mapping: Optional[Dict[str, str]] = None
    ) -> "WorkflowCompositionBuilder":
        """æ·»åŠ å·¥ä½œæµå¼•ç”¨"""
        node = WorkflowNode(
            id=uuid4(),
            workflow_id=workflow_id,
            inline_definition=None,
            name=name or f"workflow_{workflow_id}",
            inputs_mapping=inputs_mapping or {},
            outputs_mapping=outputs_mapping or {}
        )
        self._add_node(node)
        return self

    def add_inline(
        self,
        definition: Dict[str, Any],
        name: Optional[str] = None,
        inputs_mapping: Optional[Dict[str, str]] = None,
        outputs_mapping: Optional[Dict[str, str]] = None
    ) -> "WorkflowCompositionBuilder":
        """æ·»åŠ å…§è¯å·¥ä½œæµå®šç¾©"""
        node = WorkflowNode(
            id=uuid4(),
            workflow_id=None,
            inline_definition=definition,
            name=name or f"inline_{uuid4().hex[:8]}",
            inputs_mapping=inputs_mapping or {},
            outputs_mapping=outputs_mapping or {}
        )
        self._add_node(node)
        return self

    def end(self) -> "WorkflowCompositionBuilder":
        """çµæŸç•¶å‰çµ„åˆå¡Š"""
        self._pop_block()
        return self

    def build(self) -> Dict[str, Any]:
        """æ§‹å»ºæœ€çµ‚çš„çµ„åˆå®šç¾©"""
        if not self._root:
            raise ValueError("No composition defined")

        return self._serialize_block(self._root)

    def _push_block(self, block: CompositionBlock) -> None:
        """å£“å…¥æ–°çš„çµ„åˆå¡Š"""
        if self._current_block:
            self._current_block.nodes.append(block)
            self._block_stack.append(self._current_block)
        else:
            self._root = block

        self._current_block = block

    def _pop_block(self) -> None:
        """å½ˆå‡ºç•¶å‰çµ„åˆå¡Š"""
        if self._block_stack:
            self._current_block = self._block_stack.pop()
        else:
            self._current_block = None

    def _add_node(self, node: WorkflowNode) -> None:
        """æ·»åŠ ç¯€é»åˆ°ç•¶å‰å¡Š"""
        if not self._current_block:
            raise ValueError("No active composition block")

        if self._current_block.composition_type == CompositionType.SWITCH:
            # æ·»åŠ åˆ°æœ€å¾Œä¸€å€‹ case
            cases = self._current_block.switch_cases["cases"]
            if cases:
                last_case = list(cases.keys())[-1]
                cases[last_case].append(node)
        else:
            self._current_block.nodes.append(node)

    def _serialize_block(
        self,
        block: CompositionBlock
    ) -> Dict[str, Any]:
        """åºåˆ—åŒ–çµ„åˆå¡Š"""
        serialized = {
            "id": str(block.id),
            "type": block.composition_type.value,
            "nodes": []
        }

        for node in block.nodes:
            if isinstance(node, CompositionBlock):
                serialized["nodes"].append(self._serialize_block(node))
            else:
                serialized["nodes"].append(self._serialize_node(node))

        if block.condition:
            serialized["condition"] = block.condition

        if block.loop_config:
            serialized["loop_config"] = block.loop_config

        if block.switch_cases:
            serialized["switch"] = block.switch_cases

        return serialized

    def _serialize_node(self, node: WorkflowNode) -> Dict[str, Any]:
        """åºåˆ—åŒ–ç¯€é»"""
        return {
            "id": str(node.id),
            "name": node.name,
            "workflow_id": str(node.workflow_id) if node.workflow_id else None,
            "inline_definition": node.inline_definition,
            "inputs_mapping": node.inputs_mapping,
            "outputs_mapping": node.outputs_mapping
        }


# ä½¿ç”¨ç¯„ä¾‹
def create_complex_workflow():
    """å»ºç«‹è¤‡é›œçš„çµ„åˆå·¥ä½œæµç¯„ä¾‹"""
    builder = WorkflowCompositionBuilder()

    composition = (
        builder
        .sequence()
            .add_workflow(
                workflow_id=uuid4(),
                name="data_preparation",
                inputs_mapping={"raw_data": "input_data"}
            )
            .parallel()
                .add_workflow(
                    workflow_id=uuid4(),
                    name="process_branch_a"
                )
                .add_workflow(
                    workflow_id=uuid4(),
                    name="process_branch_b"
                )
            .end()  # end parallel
            .conditional("result.status == 'success'")
                .add_workflow(
                    workflow_id=uuid4(),
                    name="success_handler"
                )
            .end()  # end conditional
            .loop(max_iterations=5, condition="not converged")
                .add_workflow(
                    workflow_id=uuid4(),
                    name="refinement_step"
                )
            .end()  # end loop
        .end()  # end sequence
        .build()
    )

    return composition
```

#### é©—æ”¶æ¨™æº–
- [ ] æµæš¢çš„ API è¨­è¨ˆ
- [ ] æ”¯æ´æ‰€æœ‰çµ„åˆé¡å‹
- [ ] æ­£ç¢ºçš„åµŒå¥—çµæ§‹
- [ ] åºåˆ—åŒ–åŠŸèƒ½
- [ ] ä½¿ç”¨ç¯„ä¾‹å®Œæ•´

---

### Story 11-5: Nested Workflow API (8 é»)

**ä½œç‚º** å‰ç«¯é–‹ç™¼è€…
**æˆ‘å¸Œæœ›** æœ‰å®Œæ•´çš„åµŒå¥—å·¥ä½œæµ API
**ä»¥ä¾¿** åœ¨ UI ä¸­ç®¡ç†å’Œç›£æ§åµŒå¥—åŸ·è¡Œ

#### æŠ€è¡“è¦æ ¼

```python
# backend/src/api/v1/nested/routes.py

from fastapi import APIRouter, Depends, HTTPException, WebSocket
from typing import List, Optional
from uuid import UUID
from pydantic import BaseModel, Field
from datetime import datetime

router = APIRouter(prefix="/nested", tags=["Nested Workflows"])


# ============ Schemas ============

class SubWorkflowRequest(BaseModel):
    """å­å·¥ä½œæµè«‹æ±‚"""
    parent_workflow_id: UUID
    workflow_id: Optional[UUID] = None
    inline_definition: Optional[dict] = None
    config: dict = Field(default_factory=dict)
    input_mapping: dict = Field(default_factory=dict)
    output_mapping: dict = Field(default_factory=dict)
    position: int = 0


class ExecuteNestedRequest(BaseModel):
    """åŸ·è¡ŒåµŒå¥—å·¥ä½œæµè«‹æ±‚"""
    sub_workflow_id: UUID
    inputs: dict = Field(default_factory=dict)
    mode: str = "sync"  # sync, async, fire_and_forget, callback


class CompositionRequest(BaseModel):
    """çµ„åˆè«‹æ±‚"""
    name: str
    definition: dict


class RecursiveExecuteRequest(BaseModel):
    """éæ­¸åŸ·è¡Œè«‹æ±‚"""
    workflow_id: UUID
    initial_inputs: dict
    max_depth: int = 10
    max_iterations: int = 100
    termination_condition: Optional[str] = None


class NestedExecutionResponse(BaseModel):
    """åµŒå¥—åŸ·è¡Œå›æ‡‰"""
    execution_id: str
    parent_execution_id: Optional[str]
    workflow_id: str
    depth: int
    status: str
    started_at: datetime


class ExecutionTreeResponse(BaseModel):
    """åŸ·è¡Œæ¨¹å›æ‡‰"""
    id: str
    workflow_id: str
    depth: int
    status: str
    children: List["ExecutionTreeResponse"] = []

ExecutionTreeResponse.model_rebuild()


# ============ Routes ============

@router.post("/sub-workflows")
async def register_sub_workflow(
    request: SubWorkflowRequest,
    manager: NestedWorkflowManager = Depends(get_nested_manager)
):
    """
    è¨»å†Šå­å·¥ä½œæµ

    å°‡å­å·¥ä½œæµèˆ‡çˆ¶å·¥ä½œæµé—œè¯
    """
    sub_workflow = SubWorkflowReference(
        id=uuid4(),
        parent_workflow_id=request.parent_workflow_id,
        workflow_id=request.workflow_id,
        definition=request.inline_definition,
        config=NestedWorkflowConfig(**request.config),
        input_mapping=request.input_mapping,
        output_mapping=request.output_mapping,
        position=request.position
    )

    try:
        await manager.register_sub_workflow(
            parent_workflow_id=request.parent_workflow_id,
            sub_workflow=sub_workflow
        )

        return {
            "sub_workflow_id": str(sub_workflow.id),
            "parent_workflow_id": str(request.parent_workflow_id),
            "status": "registered"
        }
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))


@router.post("/execute", response_model=dict)
async def execute_nested_workflow(
    request: ExecuteNestedRequest,
    executor: SubWorkflowExecutor = Depends(get_sub_executor)
):
    """
    åŸ·è¡ŒåµŒå¥—å·¥ä½œæµ
    """
    from src.domain.orchestration.nested.sub_executor import SubWorkflowExecutionMode

    mode = SubWorkflowExecutionMode(request.mode)

    try:
        result = await executor.execute(
            sub_workflow_id=request.sub_workflow_id,
            inputs=request.inputs,
            mode=mode
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/executions/{execution_id}/status")
async def get_execution_status(
    execution_id: UUID,
    executor: SubWorkflowExecutor = Depends(get_sub_executor)
):
    """ç²å–åŸ·è¡Œç‹€æ…‹"""
    return await executor.get_execution_status(execution_id)


@router.post("/executions/{execution_id}/wait")
async def wait_for_execution(
    execution_id: UUID,
    timeout: Optional[float] = None,
    executor: SubWorkflowExecutor = Depends(get_sub_executor)
):
    """ç­‰å¾…åŸ·è¡Œå®Œæˆ"""
    return await executor.wait_for_completion(execution_id, timeout)


@router.post("/executions/{execution_id}/cancel")
async def cancel_execution(
    execution_id: UUID,
    cascade: bool = True,
    manager: NestedWorkflowManager = Depends(get_nested_manager)
):
    """å–æ¶ˆåŸ·è¡Œ"""
    await manager.cancel_nested_execution(execution_id, cascade)
    return {"status": "cancelled", "execution_id": str(execution_id)}


@router.get("/executions/{execution_id}/tree", response_model=ExecutionTreeResponse)
async def get_execution_tree(
    execution_id: UUID,
    manager: NestedWorkflowManager = Depends(get_nested_manager)
):
    """ç²å–åŸ·è¡Œæ¨¹"""
    tree = manager.get_execution_tree(execution_id)
    return ExecutionTreeResponse(**tree)


@router.post("/execute/parallel")
async def execute_parallel(
    sub_workflows: List[dict],
    executor: SubWorkflowExecutor = Depends(get_sub_executor)
):
    """ä¸¦è¡ŒåŸ·è¡Œå¤šå€‹å­å·¥ä½œæµ"""
    results = await executor.execute_parallel(sub_workflows)
    return {"results": results}


@router.post("/execute/sequential")
async def execute_sequential(
    sub_workflows: List[dict],
    pass_outputs: bool = True,
    executor: SubWorkflowExecutor = Depends(get_sub_executor)
):
    """é †åºåŸ·è¡Œå¤šå€‹å­å·¥ä½œæµ"""
    results = await executor.execute_sequential(sub_workflows, pass_outputs)
    return {"results": results}


@router.post("/execute/recursive")
async def execute_recursive(
    request: RecursiveExecuteRequest,
    handler: RecursivePatternHandler = Depends(get_recursive_handler)
):
    """
    åŸ·è¡Œéæ­¸å·¥ä½œæµ
    """
    # ç°¡å–®çš„éæ­¸è¼¸å…¥ç”Ÿæˆå‡½æ•¸
    def recursive_inputs_fn(result: Dict[str, Any]) -> Dict[str, Any]:
        # å¯ä»¥æ ¹æ“šå¯¦éš›éœ€æ±‚è‡ªå®šç¾©
        return {**result, "iteration": result.get("iteration", 0) + 1}

    try:
        result = await handler.execute_recursive(
            workflow_id=request.workflow_id,
            initial_inputs=request.initial_inputs,
            recursive_inputs_fn=recursive_inputs_fn
        )
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/recursive/{state_id}/stats")
async def get_recursion_stats(
    state_id: UUID,
    handler: RecursivePatternHandler = Depends(get_recursive_handler)
):
    """ç²å–éæ­¸çµ±è¨ˆ"""
    return handler.get_recursion_stats(state_id)


@router.post("/compositions")
async def create_composition(
    request: CompositionRequest,
    composition_service: Any = Depends(get_composition_service)
):
    """
    å»ºç«‹å·¥ä½œæµçµ„åˆ
    """
    composition_id = await composition_service.create(
        name=request.name,
        definition=request.definition
    )

    return {
        "composition_id": str(composition_id),
        "name": request.name,
        "status": "created"
    }


@router.get("/compositions/{composition_id}")
async def get_composition(
    composition_id: UUID,
    composition_service: Any = Depends(get_composition_service)
):
    """ç²å–çµ„åˆè©³æƒ…"""
    composition = await composition_service.get(composition_id)
    if not composition:
        raise HTTPException(status_code=404, detail="Composition not found")
    return composition


@router.post("/compositions/{composition_id}/execute")
async def execute_composition(
    composition_id: UUID,
    inputs: dict,
    composition_service: Any = Depends(get_composition_service)
):
    """åŸ·è¡Œçµ„åˆå·¥ä½œæµ"""
    result = await composition_service.execute(
        composition_id=composition_id,
        inputs=inputs
    )
    return result


@router.websocket("/executions/{execution_id}/ws")
async def execution_websocket(
    websocket: WebSocket,
    execution_id: UUID,
    manager: NestedWorkflowManager = Depends(get_nested_manager)
):
    """
    WebSocket ç›£æ§åµŒå¥—åŸ·è¡Œ

    å¯¦æ™‚ç²å–åŸ·è¡Œç‹€æ…‹æ›´æ–°
    """
    await websocket.accept()

    try:
        while True:
            # ç™¼é€ç•¶å‰åŸ·è¡Œæ¨¹
            tree = manager.get_execution_tree(execution_id)
            await websocket.send_json({
                "type": "tree_update",
                "data": tree
            })

            # ç­‰å¾…ä¸€æ®µæ™‚é–“å†æ›´æ–°
            import asyncio
            await asyncio.sleep(1)

            # æª¢æŸ¥æ˜¯å¦å®Œæˆ
            if tree.get("status") in ["completed", "failed", "cancelled"]:
                await websocket.send_json({
                    "type": "execution_complete",
                    "data": tree
                })
                break

    except Exception as e:
        await websocket.send_json({
            "type": "error",
            "message": str(e)
        })
    finally:
        await websocket.close()
```

#### é©—æ”¶æ¨™æº–
- [ ] å®Œæ•´çš„ CRUD API
- [ ] æ”¯æ´ä¸¦è¡Œ/é †åº/éæ­¸åŸ·è¡Œ
- [ ] åŸ·è¡Œæ¨¹è¦–è¦ºåŒ–
- [ ] WebSocket å¯¦æ™‚ç›£æ§
- [ ] API æ–‡æª”å®Œæ•´

---

## æ¸¬è©¦è¨ˆåŠƒ

### å–®å…ƒæ¸¬è©¦

```python
# tests/unit/test_nested_workflow.py

import pytest
from unittest.mock import AsyncMock, MagicMock
from uuid import uuid4

from src.domain.orchestration.nested.workflow_manager import (
    NestedWorkflowManager,
    NestedWorkflowConfig,
    NestedWorkflowType,
    WorkflowScope,
    SubWorkflowReference
)


@pytest.fixture
def nested_manager():
    return NestedWorkflowManager(
        workflow_service=MagicMock(),
        execution_service=MagicMock(),
        max_global_depth=10
    )


def test_cycle_detection(nested_manager):
    """æ¸¬è©¦å¾ªç’°ä¾è³´æª¢æ¸¬"""
    wf_a = uuid4()
    wf_b = uuid4()
    wf_c = uuid4()

    # A -> B -> C -> A (å¾ªç’°)
    nested_manager._dependency_graph[wf_a] = {wf_b}
    nested_manager._dependency_graph[wf_b] = {wf_c}
    nested_manager._dependency_graph[wf_c] = {wf_a}

    assert nested_manager._has_cycle(wf_a) is True


def test_no_cycle(nested_manager):
    """æ¸¬è©¦ç„¡å¾ªç’°æƒ…æ³"""
    wf_a = uuid4()
    wf_b = uuid4()
    wf_c = uuid4()

    # A -> B -> C (ç„¡å¾ªç’°)
    nested_manager._dependency_graph[wf_a] = {wf_b}
    nested_manager._dependency_graph[wf_b] = {wf_c}
    nested_manager._dependency_graph[wf_c] = set()

    assert nested_manager._has_cycle(wf_a) is False


@pytest.mark.asyncio
async def test_depth_limit(nested_manager):
    """æ¸¬è©¦æ·±åº¦é™åˆ¶"""
    parent_context = MagicMock()
    parent_context.depth = 5
    parent_context.path = [uuid4() for _ in range(5)]
    parent_context.variables = {}

    sub_workflow = SubWorkflowReference(
        id=uuid4(),
        parent_workflow_id=uuid4(),
        workflow_id=uuid4(),
        definition=None,
        config=NestedWorkflowConfig(max_depth=5),
        input_mapping={},
        output_mapping={},
        position=0
    )

    with pytest.raises(ValueError, match="depth"):
        await nested_manager.execute_sub_workflow(
            parent_context,
            sub_workflow
        )


# tests/unit/test_recursive_handler.py

@pytest.mark.asyncio
async def test_recursive_termination():
    """æ¸¬è©¦éæ­¸çµ‚æ­¢"""
    executor = MagicMock()
    executor.execute = AsyncMock(return_value={"value": 100})

    handler = RecursivePatternHandler(
        sub_executor=executor,
        config=RecursionConfig(
            max_depth=3,
            termination_condition=lambda inputs, depth: depth >= 3
        )
    )

    result = await handler.execute_recursive(
        workflow_id=uuid4(),
        initial_inputs={"value": 0},
        recursive_inputs_fn=lambda r: {"value": r["value"] + 1}
    )

    assert result["status"] == "terminated"
    assert result["termination_type"] == "max_depth"


@pytest.mark.asyncio
async def test_memoization():
    """æ¸¬è©¦è¨˜æ†¶åŒ–"""
    executor = MagicMock()
    call_count = 0

    async def execute_fn(*args, **kwargs):
        nonlocal call_count
        call_count += 1
        return {"result": "computed"}

    executor.execute = execute_fn

    handler = RecursivePatternHandler(
        sub_executor=executor,
        config=RecursionConfig(memoization=True, max_iterations=10)
    )

    # åŸ·è¡Œå…©æ¬¡ç›¸åŒè¼¸å…¥
    await handler._recursive_execute(
        state=RecursionState(
            id=uuid4(),
            workflow_id=uuid4(),
            current_depth=0,
            iteration_count=0
        ),
        inputs={"same": "input"},
        recursive_inputs_fn=lambda r: r
    )

    # ç”±æ–¼çµ‚æ­¢æ¢ä»¶ï¼Œæ‡‰è©²åªåŸ·è¡Œä¸€æ¬¡
    assert call_count >= 1
```

### æ•´åˆæ¸¬è©¦

```python
# tests/integration/test_nested_api.py

import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_register_and_execute_sub_workflow(client: AsyncClient, test_workflows):
    """æ¸¬è©¦è¨»å†Šå’ŒåŸ·è¡Œå­å·¥ä½œæµ"""
    parent_wf = test_workflows[0]
    child_wf = test_workflows[1]

    # è¨»å†Šå­å·¥ä½œæµ
    response = await client.post(
        "/api/v1/nested/sub-workflows",
        json={
            "parent_workflow_id": str(parent_wf.id),
            "workflow_id": str(child_wf.id),
            "config": {"max_depth": 5},
            "input_mapping": {"parent_data": "child_input"},
            "output_mapping": {"child_result": "parent_result"}
        }
    )
    assert response.status_code == 200
    sub_wf_id = response.json()["sub_workflow_id"]

    # åŸ·è¡Œ
    response = await client.post(
        "/api/v1/nested/execute",
        json={
            "sub_workflow_id": sub_wf_id,
            "inputs": {"parent_data": "test_value"},
            "mode": "sync"
        }
    )
    assert response.status_code == 200


@pytest.mark.asyncio
async def test_parallel_execution(client: AsyncClient, test_workflows):
    """æ¸¬è©¦ä¸¦è¡ŒåŸ·è¡Œ"""
    response = await client.post(
        "/api/v1/nested/execute/parallel",
        json=[
            {"id": str(test_workflows[0].id), "inputs": {"x": 1}},
            {"id": str(test_workflows[1].id), "inputs": {"x": 2}}
        ]
    )
    assert response.status_code == 200
    results = response.json()["results"]
    assert len(results) == 2
```

---

## è³‡æ–™åº«é·ç§»

```sql
-- migrations/versions/011_nested_workflow_tables.sql

-- å­å·¥ä½œæµå¼•ç”¨è¡¨
CREATE TABLE sub_workflow_references (
    id UUID PRIMARY KEY,
    parent_workflow_id UUID REFERENCES workflows(id) ON DELETE CASCADE,
    workflow_id UUID REFERENCES workflows(id),
    definition JSONB,
    config JSONB NOT NULL DEFAULT '{}',
    input_mapping JSONB DEFAULT '{}',
    output_mapping JSONB DEFAULT '{}',
    position INTEGER DEFAULT 0,
    status VARCHAR(50) DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åµŒå¥—åŸ·è¡Œè¨˜éŒ„è¡¨
CREATE TABLE nested_executions (
    id UUID PRIMARY KEY,
    parent_execution_id UUID REFERENCES nested_executions(id),
    workflow_id UUID NOT NULL,
    depth INTEGER NOT NULL DEFAULT 0,
    path UUID[] DEFAULT '{}',
    variables JSONB DEFAULT '{}',
    status VARCHAR(50) DEFAULT 'pending',
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT
);

-- éæ­¸ç‹€æ…‹è¡¨
CREATE TABLE recursion_states (
    id UUID PRIMARY KEY,
    workflow_id UUID NOT NULL,
    current_depth INTEGER DEFAULT 0,
    iteration_count INTEGER DEFAULT 0,
    history JSONB DEFAULT '[]',
    memo JSONB DEFAULT '{}',
    terminated BOOLEAN DEFAULT false,
    termination_type VARCHAR(50),
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP
);

-- å·¥ä½œæµçµ„åˆè¡¨
CREATE TABLE workflow_compositions (
    id UUID PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    definition JSONB NOT NULL,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- ç´¢å¼•
CREATE INDEX idx_sub_wf_parent ON sub_workflow_references(parent_workflow_id);
CREATE INDEX idx_nested_exec_parent ON nested_executions(parent_execution_id);
CREATE INDEX idx_nested_exec_workflow ON nested_executions(workflow_id);
CREATE INDEX idx_recursion_workflow ON recursion_states(workflow_id);
```

---

## é¢¨éšªèˆ‡ç·©è§£

| é¢¨éšª | å½±éŸ¿ | ç·©è§£æªæ–½ |
|------|------|----------|
| ç„¡é™éæ­¸ | é«˜ | å¼·åˆ¶æ·±åº¦é™åˆ¶ã€è¿­ä»£ä¸Šé™ |
| è¨˜æ†¶é«”çˆ†ç™¼ | é«˜ | é™åˆ¶æ­·å²è¨˜éŒ„å¤§å°ã€å®šæœŸæ¸…ç† |
| å¾ªç’°ä¾è³´ | ä¸­ | ä¾è³´åœ–æª¢æ¸¬ |
| ä¸Šä¸‹æ–‡æ±¡æŸ“ | ä¸­ | ä½œç”¨åŸŸéš”é›¢æ©Ÿåˆ¶ |
| åŸ·è¡Œè¿½è¹¤å›°é›£ | ä½ | å®Œæ•´çš„åŸ·è¡Œæ¨¹å’Œæ—¥èªŒ |

---

## Definition of Done

- [ ] æ‰€æœ‰ User Stories å®Œæˆ
- [ ] å–®å…ƒæ¸¬è©¦è¦†è“‹ç‡ > 85%
- [ ] æ•´åˆæ¸¬è©¦é€šé
- [ ] API æ–‡æª”æ›´æ–°
- [ ] è³‡æ–™åº«é·ç§»è…³æœ¬æº–å‚™å®Œæˆ
- [ ] ç¨‹å¼ç¢¼å¯©æŸ¥å®Œæˆ
- [ ] æ•ˆèƒ½æ¸¬è©¦é€šéï¼ˆ10 å±¤åµŒå¥— < 30ç§’ï¼‰

---

**ä¸‹ä¸€æ­¥**: [Sprint 12 - æ•´åˆèˆ‡å„ªåŒ–](./sprint-12-plan.md)
