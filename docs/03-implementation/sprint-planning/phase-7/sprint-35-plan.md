# Sprint 35: Phase 2 æ“´å±• LLM æ•´åˆ

**Sprint ç›®æ¨™**: å°‡ LLM æœå‹™æ³¨å…¥æ‰€æœ‰ Phase 2 æ“´å±•çµ„ä»¶ï¼Œå¯¦ç¾çœŸæ­£çš„ AI è‡ªä¸»æ±ºç­–
**ç¸½é»æ•¸**: 23 Story Points
**å„ªå…ˆç´š**: ğŸ”´ CRITICAL
**å‰ç½®æ¢ä»¶**: Sprint 34 å®Œæˆ

---

## èƒŒæ™¯

Sprint 34 å‰µå»ºäº† LLM æœå‹™åŸºç¤è¨­æ–½å¾Œï¼Œæœ¬ Sprint å°‡æŠŠ LLM æœå‹™æ³¨å…¥åˆ°æ‰€æœ‰ Phase 2 æ“´å±•çµ„ä»¶ä¸­ï¼Œä½¿å…¶å¾ã€Œè¦å‰‡å¼è‡ªå‹•åŒ–ã€å‡ç´šç‚ºã€ŒAI è‡ªä¸»æ±ºç­–ã€ã€‚

### ä¿®æ”¹ç›®æ¨™

```python
# ä¿®æ”¹å‰ (è¦å‰‡å¼)
self._task_decomposer = TaskDecomposer(
    max_subtasks=max_subtasks,
    max_depth=max_depth,
)  # âŒ ç„¡ LLM

# ä¿®æ”¹å¾Œ (AI è‡ªä¸»)
self._task_decomposer = TaskDecomposer(
    llm_service=self._llm_service,  # âœ… æ³¨å…¥ LLM
    max_subtasks=max_subtasks,
    max_depth=max_depth,
)
```

---

## Story æ¸…å–®

### S35-1: PlanningAdapter LLM æ•´åˆ (8 pts)

**å„ªå…ˆç´š**: ğŸ”´ P0 - CRITICAL
**é¡å‹**: é‡æ§‹
**å½±éŸ¿ç¯„åœ**: `backend/src/integrations/agent_framework/builders/planning.py`

#### è¨­è¨ˆ

```python
# ä¿®æ”¹ PlanningAdapter æ§‹é€ å‡½æ•¸

class PlanningAdapter:
    """å‹•æ…‹è¦åŠƒé©é…å™¨ - ç¾åœ¨æ”¯æ´ LLM!"""

    def __init__(
        self,
        id: str,
        config: Optional[PlanningConfig] = None,
        llm_service: Optional[LLMServiceProtocol] = None,  # æ–°å¢åƒæ•¸
    ):
        self._id = id
        self._config = config or PlanningConfig()

        # å®˜æ–¹ Builder å¯¦ä¾‹
        self._magentic_builder = MagenticBuilder()

        # LLM æœå‹™ (Phase 7 æ–°å¢)
        self._llm_service = llm_service
        self._ensure_llm_service()

        # Phase 2 æ“´å±•åŠŸèƒ½
        self._task_decomposer: Optional[TaskDecomposer] = None
        self._decision_engine: Optional[AutonomousDecisionEngine] = None
        self._trial_error_engine: Optional[TrialAndErrorEngine] = None

    def _ensure_llm_service(self) -> None:
        """ç¢ºä¿ LLM æœå‹™å¯ç”¨ã€‚"""
        if self._llm_service is None:
            from src.integrations.llm import LLMServiceFactory
            self._llm_service = LLMServiceFactory.get_singleton()
            logger.info(f"PlanningAdapter '{self._id}': Using singleton LLM service")
```

#### ä»»å‹™æ¸…å–®

1. **æ›´æ–° PlanningAdapter æ§‹é€ å‡½æ•¸**
   - æ·»åŠ  `llm_service` åƒæ•¸
   - å¯¦ç¾ `_ensure_llm_service()` æ–¹æ³•
   - æ›´æ–°æ—¥èªŒè¨˜éŒ„

2. **æ›´æ–° with_task_decomposition()**
   ```python
   def with_task_decomposition(
       self,
       strategy: DecompositionStrategy = DecompositionStrategy.HYBRID,
       max_subtasks: Optional[int] = None,
       max_depth: Optional[int] = None,
   ) -> "PlanningAdapter":
       self._decomposition_strategy = strategy

       self._task_decomposer = TaskDecomposer(
           llm_service=self._llm_service,  # âœ… æ³¨å…¥ LLM
           max_subtasks=max_subtasks or self._config.max_subtasks,
           max_depth=max_depth or self._config.max_depth,
       )

       self._mode = PlanningMode.DECOMPOSED
       logger.info(f"Enabled task decomposition with LLM: {strategy.value}")
       return self
   ```

3. **æ›´æ–° with_decision_engine()**
   ```python
   def with_decision_engine(
       self,
       risk_threshold: float = 0.7,
       auto_decision_confidence: float = 0.8,
       rules: Optional[List[DecisionRule]] = None,
   ) -> "PlanningAdapter":
       self._decision_engine = AutonomousDecisionEngine(
           llm_service=self._llm_service,  # âœ… æ³¨å…¥ LLM
           risk_threshold=risk_threshold,
           auto_decision_confidence=auto_decision_confidence,
       )

       if rules:
           for rule in rules:
               self._decision_engine.add_rule(...)

       self._mode = PlanningMode.DECISION_DRIVEN
       logger.info("Enabled decision engine with LLM")
       return self
   ```

4. **æ›´æ–° with_trial_error()**
   ```python
   def with_trial_error(
       self,
       max_retries: int = 3,
       learning_threshold: float = 0.6,
   ) -> "PlanningAdapter":
       self._trial_error_engine = TrialAndErrorEngine(
           llm_service=self._llm_service,  # âœ… æ³¨å…¥ LLM
           max_retries=max_retries,
           learning_threshold=learning_threshold,
           timeout_seconds=int(self._config.timeout_seconds),
       )

       self._mode = PlanningMode.ADAPTIVE
       logger.info("Enabled trial-error engine with LLM")
       return self
   ```

5. **æ·»åŠ å°å…¥èªå¥**
   ```python
   from src.integrations.llm import LLMServiceProtocol, LLMServiceFactory
   ```

#### é©—æ”¶æ¨™æº–
- [ ] PlanningAdapter æ¥å— `llm_service` åƒæ•¸
- [ ] æ‰€æœ‰ `with_*` æ–¹æ³•æ³¨å…¥ LLM æœå‹™
- [ ] è‡ªå‹•ç²å–å–®ä¾‹ LLM æœå‹™ï¼ˆå¦‚æœªæä¾›ï¼‰
- [ ] èªæ³•æª¢æŸ¥é€šé
- [ ] ç¾æœ‰æ¸¬è©¦ä¸å—å½±éŸ¿ï¼ˆå‘å¾Œå…¼å®¹ï¼‰

---

### S35-2: TaskDecomposer LLM å•Ÿç”¨ (5 pts)

**å„ªå…ˆç´š**: ğŸ”´ P0 - CRITICAL
**é¡å‹**: é©—è­‰/ä¿®å¾©
**å½±éŸ¿ç¯„åœ**: `backend/src/domain/orchestration/planning/task_decomposer.py`

#### èƒŒæ™¯

TaskDecomposer å·²ç¶“è¨­è¨ˆæ”¯æ´ LLMï¼ˆæœ‰ `llm_service` åƒæ•¸ï¼‰ï¼Œä½†éœ€è¦é©—è­‰ LLM è·¯å¾‘æ˜¯å¦æ­£ç¢ºå·¥ä½œã€‚

#### ä»»å‹™æ¸…å–®

1. **é©—è­‰ç¾æœ‰ LLM é‚è¼¯**
   ```python
   # é©—è­‰é€™æ®µé‚è¼¯æ­£ç¢ºå·¥ä½œ
   if self.llm_service:
       response = await self.llm_service.generate(
           prompt=prompt,
           max_tokens=2000
       )
       return self._parse_decomposition_response(task_id, response)

   # Fallback: simple rule-based decomposition
   return self._rule_based_decomposition(task_id, task_description, "hierarchical")
   ```

2. **å„ªåŒ– LLM Prompt æ¨¡æ¿**
   ```python
   DECOMPOSITION_PROMPT_TEMPLATE = """
   You are a task decomposition expert. Break down the following task into subtasks.

   Task: {task_description}
   Strategy: {strategy}
   Max Subtasks: {max_subtasks}
   Max Depth: {max_depth}

   Please provide a JSON response with the following structure:
   {
       "subtasks": [
           {
               "name": "subtask name",
               "description": "what this subtask does",
               "priority": "high|medium|low",
               "dependencies": ["dependency_id"],
               "estimated_duration_minutes": 30
           }
       ],
       "execution_order": ["subtask_id_1", "subtask_id_2"],
       "confidence": 0.85
   }
   """
   ```

3. **æ·»åŠ çµæ§‹åŒ–è¼¸å‡ºæ”¯æ´**
   - ä½¿ç”¨ `generate_structured()` æ›¿ä»£ `generate()` + æ‰‹å‹•è§£æ
   - å®šç¾©è¼¸å‡º JSON Schema

4. **å¢å¼·éŒ¯èª¤è™•ç†**
   - LLM å¤±æ•—æ™‚å„ªé›…é™ç´šåˆ°è¦å‰‡å¼
   - æ·»åŠ é‡è©¦é‚è¼¯

#### é©—æ”¶æ¨™æº–
- [ ] LLM è·¯å¾‘æ­£ç¢ºè§¸ç™¼ï¼ˆéè¦å‰‡å¼ï¼‰
- [ ] Prompt æ¨¡æ¿å„ªåŒ–å®Œæˆ
- [ ] çµæ§‹åŒ–è¼¸å‡ºæ­£å¸¸å·¥ä½œ
- [ ] éŒ¯èª¤è™•ç†å’Œé™ç´šæ­£ç¢º

---

### S35-3: DecisionEngine LLM å•Ÿç”¨ (5 pts)

**å„ªå…ˆç´š**: ğŸ”´ P0 - CRITICAL
**é¡å‹**: é©—è­‰/ä¿®å¾©
**å½±éŸ¿ç¯„åœ**: `backend/src/domain/orchestration/planning/decision_engine.py`

#### ä»»å‹™æ¸…å–®

1. **é©—è­‰ç¾æœ‰ LLM é‚è¼¯**
   ```python
   # decision_engine.py è¡Œ 329-360
   if self.llm_service:
       # æ‡‰è©²åŸ·è¡Œ LLM æ±ºç­–åˆ†æ
       ...
   ```

2. **å„ªåŒ–æ±ºç­– Prompt**
   ```python
   DECISION_PROMPT_TEMPLATE = """
   You are a decision-making expert. Analyze the following situation and options.

   Situation: {situation}
   Decision Type: {decision_type}

   Options:
   {options_list}

   Context:
   {context}

   Please analyze each option and provide a JSON response:
   {
       "selected_option": "option_id",
       "confidence": 0.85,
       "reasoning": "explanation of why this option was chosen",
       "pros": ["advantage 1", "advantage 2"],
       "cons": ["disadvantage 1"],
       "risk_assessment": {
           "level": "low|medium|high",
           "factors": ["risk factor 1"],
           "mitigations": ["mitigation 1"]
       }
   }
   """
   ```

3. **æ·»åŠ çµæ§‹åŒ–è¼¸å‡ºæ”¯æ´**

4. **é©—è­‰é¢¨éšªè©•ä¼°é‚è¼¯**
   - ç¢ºä¿ LLM è¿”å›çš„é¢¨éšªè©•ä¼°æ­£ç¢ºè§£æ
   - é©—è­‰ç½®ä¿¡åº¦è¨ˆç®—

#### é©—æ”¶æ¨™æº–
- [ ] LLM æ±ºç­–åˆ†ææ­£ç¢ºè§¸ç™¼
- [ ] Prompt æ¨¡æ¿å„ªåŒ–å®Œæˆ
- [ ] é¢¨éšªè©•ä¼°æ­£ç¢ºè§£æ
- [ ] æ±ºç­–è¨˜éŒ„åŒ…å« LLM æ¨ç†

---

### S35-4: TrialAndErrorEngine LLM å•Ÿç”¨ (5 pts)

**å„ªå…ˆç´š**: ğŸŸ¡ P1
**é¡å‹**: é©—è­‰/ä¿®å¾©
**å½±éŸ¿ç¯„åœ**: `backend/src/domain/orchestration/planning/trial_error.py`

#### ä»»å‹™æ¸…å–®

1. **é©—è­‰ç¾æœ‰ LLM é‚è¼¯**
   ```python
   # trial_error.py è¡Œ 355-390
   if self.llm_service:
       # æ‡‰è©²åŸ·è¡Œ LLM éŒ¯èª¤åˆ†æ
       response = await self.llm_service.generate(
           prompt=prompt,
           max_tokens=1000
       )
   ```

2. **å„ªåŒ–éŒ¯èª¤åˆ†æ Prompt**
   ```python
   ERROR_ANALYSIS_PROMPT_TEMPLATE = """
   You are an error analysis expert. Analyze the following execution failure.

   Task: {task_description}
   Error: {error_message}
   Attempt: {attempt_number} of {max_attempts}

   Previous attempts:
   {previous_attempts}

   Please analyze and provide a JSON response:
   {
       "error_category": "category name",
       "root_cause": "analysis of root cause",
       "is_recoverable": true,
       "suggested_fix": "how to fix this",
       "parameter_adjustments": {
           "param1": "new_value"
       },
       "confidence": 0.75
   }
   """
   ```

3. **æ·»åŠ å­¸ç¿’æ¨¡å¼æ”¯æ´**
   - æˆåŠŸ/å¤±æ•—æ¨¡å¼å­¸ç¿’
   - åƒæ•¸èª¿æ•´å»ºè­°

4. **é©—è­‰é‡è©¦é‚è¼¯**
   - ç¢ºä¿ LLM å»ºè­°çš„åƒæ•¸èª¿æ•´è¢«æ‡‰ç”¨

#### é©—æ”¶æ¨™æº–
- [ ] LLM éŒ¯èª¤åˆ†ææ­£ç¢ºè§¸ç™¼
- [ ] Prompt æ¨¡æ¿å„ªåŒ–å®Œæˆ
- [ ] åƒæ•¸èª¿æ•´å»ºè­°æ­£ç¢ºæ‡‰ç”¨
- [ ] å­¸ç¿’æ¨¡å¼æ­£å¸¸å·¥ä½œ

---

## æ•´åˆæ¸¬è©¦è¨­è¨ˆ

### ç«¯åˆ°ç«¯æ¸¬è©¦ç”¨ä¾‹

```python
# tests/integration/test_planning_with_llm.py

import pytest
from src.integrations.agent_framework.builders import PlanningAdapter
from src.integrations.llm import MockLLMService


class TestPlanningAdapterWithLLM:
    """PlanningAdapter LLM æ•´åˆæ¸¬è©¦ã€‚"""

    @pytest.fixture
    def mock_llm(self):
        """å‰µå»º Mock LLM æœå‹™ã€‚"""
        return MockLLMService(responses={
            "decompose": '{"subtasks": [...], "confidence": 0.9}',
            "decide": '{"selected_option": "option_1", "confidence": 0.85}',
        })

    @pytest.mark.asyncio
    async def test_decomposition_uses_llm(self, mock_llm):
        """é©—è­‰ä»»å‹™åˆ†è§£ä½¿ç”¨ LLMã€‚"""
        adapter = PlanningAdapter("test", llm_service=mock_llm)
        adapter.with_task_decomposition()

        result = await adapter.decompose("Build a REST API")

        assert mock_llm.call_count > 0
        assert result.confidence > 0.8

    @pytest.mark.asyncio
    async def test_decision_uses_llm(self, mock_llm):
        """é©—è­‰æ±ºç­–ä½¿ç”¨ LLMã€‚"""
        adapter = PlanningAdapter("test", llm_service=mock_llm)
        adapter.with_decision_engine()

        result = await adapter.decide(
            situation="Choose deployment strategy",
            options=["Blue-Green", "Canary", "Rolling"]
        )

        assert mock_llm.call_count > 0
        assert result["reasoning"] is not None
```

---

## é©—è­‰å‘½ä»¤

```bash
# 1. èªæ³•æª¢æŸ¥
cd backend
python -m py_compile src/integrations/agent_framework/builders/planning.py

# 2. é©—è­‰ LLM æ³¨å…¥
python -c "
from src.integrations.agent_framework.builders import PlanningAdapter

adapter = PlanningAdapter('test')
print(f'LLM Service: {adapter._llm_service}')
adapter.with_task_decomposition()
print(f'TaskDecomposer LLM: {adapter._task_decomposer.llm_service}')
"
# é æœŸ: LLM Service å’Œ TaskDecomposer LLM éƒ½ä¸ç‚º None

# 3. é‹è¡Œæ•´åˆæ¸¬è©¦
pytest tests/integration/test_planning_with_llm.py -v

# 4. é©—è­‰ç¾æœ‰æ¸¬è©¦ä¸å—å½±éŸ¿
pytest tests/unit/test_planning*.py -v
```

---

## å®Œæˆå®šç¾©

- [ ] æ‰€æœ‰ S35 Story å®Œæˆ
- [ ] PlanningAdapter æ³¨å…¥ LLM æœå‹™
- [ ] TaskDecomposer ä½¿ç”¨ LLM
- [ ] DecisionEngine ä½¿ç”¨ LLM
- [ ] TrialAndErrorEngine ä½¿ç”¨ LLM
- [ ] æ•´åˆæ¸¬è©¦é€šé
- [ ] ç¾æœ‰æ¸¬è©¦ä¸å—å½±éŸ¿
- [ ] ä»£ç¢¼å¯©æŸ¥å®Œæˆ

---

**å‰µå»ºæ—¥æœŸ**: 2025-12-21
