# Sprint 36: é©—è­‰èˆ‡å„ªåŒ–

**Sprint ç›®æ¨™**: å®Œæ•´æ¸¬è©¦ AI è‡ªä¸»æ±ºç­–èƒ½åŠ›ï¼Œå„ªåŒ–æ€§èƒ½ï¼Œæº–å‚™ UAT
**ç¸½é»æ•¸**: 15 Story Points
**å„ªå…ˆç´š**: ğŸŸ¡ é‡è¦
**å‰ç½®æ¢ä»¶**: Sprint 34, 35 å®Œæˆ

---

## èƒŒæ™¯

Sprint 34-35 å®Œæˆäº† LLM æœå‹™åŸºç¤è¨­æ–½å’Œ Phase 2 æ“´å±•æ•´åˆå¾Œï¼Œæœ¬ Sprint é€²è¡Œå…¨é¢é©—è­‰ã€æ€§èƒ½å„ªåŒ–ï¼Œä¸¦æº–å‚™ UAT æ¸¬è©¦ã€‚

### é©—è­‰ç›®æ¨™

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Phase 7 é©—è­‰çŸ©é™£                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                         â”‚
â”‚  åŠŸèƒ½é©—è­‰                                               â”‚
â”‚  â”œâ”€â”€ TaskDecomposer + LLM â†’ æ™ºèƒ½ä»»å‹™åˆ†è§£               â”‚
â”‚  â”œâ”€â”€ DecisionEngine + LLM â†’ æ™ºèƒ½æ±ºç­–åˆ†æ               â”‚
â”‚  â”œâ”€â”€ TrialAndErrorEngine + LLM â†’ æ™ºèƒ½éŒ¯èª¤å­¸ç¿’          â”‚
â”‚  â””â”€â”€ PlanningAdapter å®Œæ•´æµç¨‹ â†’ ç«¯åˆ°ç«¯ AI è‡ªä¸»æ±ºç­–     â”‚
â”‚                                                         â”‚
â”‚  æ€§èƒ½é©—è­‰                                               â”‚
â”‚  â”œâ”€â”€ LLM èª¿ç”¨å»¶é² < 5 ç§’ (P95)                         â”‚
â”‚  â”œâ”€â”€ ç·©å­˜å‘½ä¸­ç‡ > 30%                                  â”‚
â”‚  â””â”€â”€ ä¸¦ç™¼è™•ç†èƒ½åŠ›é©—è­‰                                  â”‚
â”‚                                                         â”‚
â”‚  é™ç´šé©—è­‰                                               â”‚
â”‚  â”œâ”€â”€ LLM ä¸å¯ç”¨ â†’ è¦å‰‡å¼é™ç´š                           â”‚
â”‚  â””â”€â”€ è¶…æ™‚è™•ç† â†’ å„ªé›…å¤±æ•—                               â”‚
â”‚                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Story æ¸…å–®

### S36-1: ç«¯åˆ°ç«¯ AI æ±ºç­–æ¸¬è©¦ (5 pts)

**å„ªå…ˆç´š**: ğŸ”´ P0 - CRITICAL
**é¡å‹**: æ¸¬è©¦
**å½±éŸ¿ç¯„åœ**: `backend/tests/e2e/`

#### è¨­è¨ˆ

å‰µå»ºå®Œæ•´çš„ç«¯åˆ°ç«¯æ¸¬è©¦å¥—ä»¶ï¼Œé©—è­‰ AI è‡ªä¸»æ±ºç­–æµç¨‹ã€‚

```python
# tests/e2e/test_ai_autonomous_decision.py

import pytest
from src.integrations.agent_framework.builders import PlanningAdapter
from src.integrations.llm import AzureOpenAILLMService


class TestAIAutonomousDecision:
    """AI è‡ªä¸»æ±ºç­–ç«¯åˆ°ç«¯æ¸¬è©¦ã€‚

    ä½¿ç”¨çœŸå¯¦ Azure OpenAI API é€²è¡Œå®Œæ•´æµç¨‹é©—è­‰ã€‚
    """

    @pytest.fixture
    def real_llm_service(self):
        """å‰µå»ºçœŸå¯¦ LLM æœå‹™ï¼ˆéœ€è¦æœ‰æ•ˆé…ç½®ï¼‰ã€‚"""
        return AzureOpenAILLMService()

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_intelligent_task_decomposition(self, real_llm_service):
        """æ¸¬è©¦æ™ºèƒ½ä»»å‹™åˆ†è§£ã€‚

        é©—è­‰ TaskDecomposer ä½¿ç”¨ LLM é€²è¡Œèªç¾©ç†è§£å’Œåˆ†è§£ã€‚
        """
        adapter = PlanningAdapter("e2e-test", llm_service=real_llm_service)
        adapter.with_task_decomposition()

        # è¤‡é›œä»»å‹™æè¿°
        task = """
        Build a user authentication system with:
        - Email/password login
        - OAuth2 (Google, GitHub)
        - Two-factor authentication
        - Password reset flow
        - Session management
        """

        result = await adapter.decompose(task)

        # é©—è­‰ LLM æ™ºèƒ½åˆ†è§£
        assert len(result.subtasks) >= 5
        assert result.confidence >= 0.7
        assert any("oauth" in st.name.lower() for st in result.subtasks)
        assert any("2fa" in st.name.lower() or "two-factor" in st.name.lower()
                   for st in result.subtasks)

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_intelligent_decision_making(self, real_llm_service):
        """æ¸¬è©¦æ™ºèƒ½æ±ºç­–ã€‚

        é©—è­‰ DecisionEngine ä½¿ç”¨ LLM é€²è¡Œé¢¨éšªè©•ä¼°å’Œæ±ºç­–åˆ†æã€‚
        """
        adapter = PlanningAdapter("e2e-test", llm_service=real_llm_service)
        adapter.with_decision_engine()

        result = await adapter.decide(
            situation="Choose a database for high-traffic e-commerce platform",
            options=[
                "PostgreSQL - Relational, ACID compliant",
                "MongoDB - Document store, flexible schema",
                "Redis - In-memory, high performance",
                "CockroachDB - Distributed SQL"
            ],
            context={
                "traffic": "10M daily users",
                "data_type": "transactions",
                "consistency_requirement": "high"
            }
        )

        # é©—è­‰ LLM æ™ºèƒ½æ±ºç­–
        assert result["selected_option"] is not None
        assert len(result["reasoning"]) > 50  # æœ‰å¯¦è³ªæ€§æ¨ç†
        assert result["confidence"] >= 0.6
        assert "risk_assessment" in result

    @pytest.mark.e2e
    @pytest.mark.asyncio
    async def test_full_planning_workflow(self, real_llm_service):
        """æ¸¬è©¦å®Œæ•´è¦åŠƒå·¥ä½œæµç¨‹ã€‚

        é©—è­‰å¾ä»»å‹™åˆ†è§£åˆ°æ±ºç­–çš„å®Œæ•´ AI è‡ªä¸»æ±ºç­–æµç¨‹ã€‚
        """
        adapter = PlanningAdapter("e2e-test", llm_service=real_llm_service)
        adapter.with_task_decomposition()
        adapter.with_decision_engine()
        adapter.with_trial_error(max_retries=2)

        result = await adapter.run(
            goal="Implement a REST API for user management with CRUD operations"
        )

        # é©—è­‰å®Œæ•´æµç¨‹
        assert result.status.value in ["completed", "ready"]
        assert len(result.subtasks) > 0
        assert result.duration_ms > 0
```

#### ä»»å‹™æ¸…å–®

1. **å‰µå»ºæ¸¬è©¦ç›®éŒ„çµæ§‹**
   ```
   backend/tests/e2e/
   â”œâ”€â”€ __init__.py
   â”œâ”€â”€ conftest.py                    # E2E fixtures
   â”œâ”€â”€ test_ai_autonomous_decision.py # ä¸»è¦æ¸¬è©¦
   â””â”€â”€ test_llm_integration.py        # LLM ç›´æ¥æ•´åˆæ¸¬è©¦
   ```

2. **å¯¦ç¾æ¸¬è©¦ç”¨ä¾‹**
   - æ™ºèƒ½ä»»å‹™åˆ†è§£æ¸¬è©¦
   - æ™ºèƒ½æ±ºç­–æ¸¬è©¦
   - æ™ºèƒ½éŒ¯èª¤å­¸ç¿’æ¸¬è©¦
   - å®Œæ•´è¦åŠƒå·¥ä½œæµç¨‹æ¸¬è©¦

3. **æ·»åŠ æ¸¬è©¦æ¨™è¨˜**
   ```python
   # conftest.py
   def pytest_configure(config):
       config.addinivalue_line("markers", "e2e: End-to-end tests (require real API)")
   ```

4. **é…ç½® CI/CD é›†æˆ**
   - E2E æ¸¬è©¦éœ€è¦çœŸå¯¦ APIï¼Œå¯èƒ½éœ€è¦ç‰¹æ®Šç’°å¢ƒ

#### é©—æ”¶æ¨™æº–
- [ ] E2E æ¸¬è©¦å¥—ä»¶å‰µå»ºå®Œæˆ
- [ ] æ™ºèƒ½ä»»å‹™åˆ†è§£æ¸¬è©¦é€šé
- [ ] æ™ºèƒ½æ±ºç­–æ¸¬è©¦é€šé
- [ ] å®Œæ•´å·¥ä½œæµç¨‹æ¸¬è©¦é€šé
- [ ] æ¸¬è©¦å¯åœ¨ CI/CD ä¸­é‹è¡Œ

---

### S36-2: æ€§èƒ½åŸºæº–æ¸¬è©¦èˆ‡å„ªåŒ– (5 pts)

**å„ªå…ˆç´š**: ğŸŸ¡ P1
**é¡å‹**: æ¸¬è©¦/å„ªåŒ–
**å½±éŸ¿ç¯„åœ**: `backend/tests/performance/`

#### è¨­è¨ˆ

```python
# tests/performance/test_llm_performance.py

import pytest
import asyncio
import statistics
from src.integrations.llm import LLMServiceFactory


class TestLLMPerformance:
    """LLM æœå‹™æ€§èƒ½æ¸¬è©¦ã€‚"""

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_single_request_latency(self):
        """æ¸¬è©¦å–®å€‹è«‹æ±‚å»¶é²ã€‚

        ç›®æ¨™: P95 < 5 ç§’
        """
        service = LLMServiceFactory.create()
        latencies = []

        for i in range(10):
            start = asyncio.get_event_loop().time()
            await service.generate("Hello, this is a test prompt.")
            latency = asyncio.get_event_loop().time() - start
            latencies.append(latency)

        p95 = sorted(latencies)[int(len(latencies) * 0.95)]
        avg = statistics.mean(latencies)

        print(f"Average latency: {avg:.2f}s")
        print(f"P95 latency: {p95:.2f}s")

        assert p95 < 5.0, f"P95 latency {p95:.2f}s exceeds 5s target"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_concurrent_requests(self):
        """æ¸¬è©¦ä¸¦ç™¼è«‹æ±‚è™•ç†ã€‚

        ç›®æ¨™: 10 å€‹ä¸¦ç™¼è«‹æ±‚å…¨éƒ¨æˆåŠŸ
        """
        service = LLMServiceFactory.create()

        async def make_request(i):
            return await service.generate(f"Test prompt {i}")

        tasks = [make_request(i) for i in range(10)]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        successes = [r for r in results if not isinstance(r, Exception)]
        failures = [r for r in results if isinstance(r, Exception)]

        print(f"Successes: {len(successes)}, Failures: {len(failures)}")

        assert len(successes) >= 8, f"Too many failures: {len(failures)}"

    @pytest.mark.performance
    @pytest.mark.asyncio
    async def test_cache_effectiveness(self):
        """æ¸¬è©¦ç·©å­˜æ•ˆæœã€‚

        ç›®æ¨™: ç›¸åŒè«‹æ±‚ç¬¬äºŒæ¬¡å»¶é² < 100ms
        """
        service = LLMServiceFactory.create(use_cache=True)
        prompt = "What is the capital of France?"

        # ç¬¬ä¸€æ¬¡è«‹æ±‚ (cold)
        start = asyncio.get_event_loop().time()
        await service.generate(prompt)
        cold_latency = asyncio.get_event_loop().time() - start

        # ç¬¬äºŒæ¬¡è«‹æ±‚ (cached)
        start = asyncio.get_event_loop().time()
        await service.generate(prompt)
        cached_latency = asyncio.get_event_loop().time() - start

        print(f"Cold latency: {cold_latency:.3f}s")
        print(f"Cached latency: {cached_latency:.3f}s")
        print(f"Speedup: {cold_latency / cached_latency:.1f}x")

        assert cached_latency < 0.1, "Cache miss or slow cache"
```

#### ä»»å‹™æ¸…å–®

1. **å‰µå»ºæ€§èƒ½æ¸¬è©¦å¥—ä»¶**
   - å–®è«‹æ±‚å»¶é²æ¸¬è©¦
   - ä¸¦ç™¼è«‹æ±‚æ¸¬è©¦
   - ç·©å­˜æ•ˆæœæ¸¬è©¦
   - è¶…æ™‚è™•ç†æ¸¬è©¦

2. **å»ºç«‹æ€§èƒ½åŸºæº–**
   | æŒ‡æ¨™ | ç›®æ¨™ | å¯¦éš› |
   |------|------|------|
   | å–®è«‹æ±‚ P95 å»¶é² | < 5s | TBD |
   | ä¸¦ç™¼æˆåŠŸç‡ (10 req) | > 80% | TBD |
   | ç·©å­˜å‘½ä¸­å»¶é² | < 100ms | TBD |

3. **å„ªåŒ–æªæ–½**
   - ç·©å­˜é ç†±ç­–ç•¥
   - é€£æ¥æ± å„ªåŒ–
   - è¶…æ™‚é…ç½®èª¿æ•´

#### é©—æ”¶æ¨™æº–
- [ ] æ€§èƒ½æ¸¬è©¦å¥—ä»¶å‰µå»ºå®Œæˆ
- [ ] P95 å»¶é² < 5 ç§’
- [ ] ä¸¦ç™¼æˆåŠŸç‡ > 80%
- [ ] ç·©å­˜æœ‰æ•ˆå·¥ä½œ

---

### S36-3: æ–‡æª”æ›´æ–°å’Œ UAT æº–å‚™ (3 pts)

**å„ªå…ˆç´š**: ğŸŸ¡ P1
**é¡å‹**: æ–‡æª”
**å½±éŸ¿ç¯„åœ**: `docs/`, `claudedocs/`

#### ä»»å‹™æ¸…å–®

1. **æ›´æ–°æŠ€è¡“æ–‡æª”**
   - `docs/02-architecture/technical-architecture.md` - æ·»åŠ  LLM æœå‹™å±¤
   - `backend/src/integrations/llm/README.md` - LLM æœå‹™ä½¿ç”¨æŒ‡å—

2. **æ›´æ–° UAT æ¸¬è©¦è¨ˆåŠƒ**
   - æ·»åŠ  AI è‡ªä¸»æ±ºç­–æ¸¬è©¦å ´æ™¯
   - æ›´æ–° FEATURE-INDEX.md ä¸­ç›¸é—œåŠŸèƒ½ç‹€æ…‹

3. **æ›´æ–° CLAUDE.md**
   - æ·»åŠ  LLM é…ç½®èªªæ˜
   - æ›´æ–°é–‹ç™¼å‘½ä»¤

4. **å‰µå»º Phase 7 å®Œæˆå ±å‘Š**
   ```markdown
   # Phase 7 å®Œæˆå ±å‘Š

   ## æˆæœæ‘˜è¦
   - LLM æœå‹™åŸºç¤è¨­æ–½å®Œæˆ
   - Phase 2 æ“´å±•å…¨éƒ¨å•Ÿç”¨ LLM
   - ç«¯åˆ°ç«¯ AI è‡ªä¸»æ±ºç­–é©—è­‰é€šé

   ## æ€§èƒ½æŒ‡æ¨™
   - å–®è«‹æ±‚å»¶é²: X.Xs (P95)
   - ç·©å­˜å‘½ä¸­ç‡: X%
   - ä¸¦ç™¼æˆåŠŸç‡: X%

   ## æ–°å¢åŠŸèƒ½
   - æ™ºèƒ½ä»»å‹™åˆ†è§£ (LLM é©…å‹•)
   - æ™ºèƒ½æ±ºç­–åˆ†æ (LLM é©…å‹•)
   - æ™ºèƒ½éŒ¯èª¤å­¸ç¿’ (LLM é©…å‹•)
   ```

#### é©—æ”¶æ¨™æº–
- [ ] æŠ€è¡“æ–‡æª”æ›´æ–°å®Œæˆ
- [ ] UAT æ¸¬è©¦è¨ˆåŠƒæ›´æ–°å®Œæˆ
- [ ] CLAUDE.md æ›´æ–°å®Œæˆ
- [ ] Phase 7 å®Œæˆå ±å‘Šå‰µå»º

---

### S36-4: LLM å›é€€ç­–ç•¥é©—è­‰ (2 pts)

**å„ªå…ˆç´š**: ğŸŸ¢ P2
**é¡å‹**: æ¸¬è©¦
**å½±éŸ¿ç¯„åœ**: `backend/tests/`

#### è¨­è¨ˆ

é©—è­‰ LLM ä¸å¯ç”¨æ™‚çš„å„ªé›…é™ç´šã€‚

```python
# tests/unit/test_llm_fallback.py

import pytest
from unittest.mock import AsyncMock, patch
from src.integrations.agent_framework.builders import PlanningAdapter


class TestLLMFallback:
    """LLM å›é€€ç­–ç•¥æ¸¬è©¦ã€‚"""

    @pytest.mark.asyncio
    async def test_decomposer_fallback_on_llm_error(self):
        """æ¸¬è©¦ LLM éŒ¯èª¤æ™‚é™ç´šåˆ°è¦å‰‡å¼åˆ†è§£ã€‚"""
        # å‰µå»ºæœƒå¤±æ•—çš„ LLM æœå‹™
        mock_llm = AsyncMock()
        mock_llm.generate.side_effect = Exception("API Error")

        adapter = PlanningAdapter("test", llm_service=mock_llm)
        adapter.with_task_decomposition()

        # æ‡‰è©²é™ç´šåˆ°è¦å‰‡å¼ï¼Œä¸æ‡‰è©²æ‹‹å‡ºç•°å¸¸
        result = await adapter.decompose("Build an API")

        assert result is not None
        assert len(result.subtasks) > 0  # è¦å‰‡å¼åˆ†è§£ä»ç„¶ç”¢ç”Ÿçµæœ

    @pytest.mark.asyncio
    async def test_decomposer_fallback_on_timeout(self):
        """æ¸¬è©¦ LLM è¶…æ™‚æ™‚é™ç´šåˆ°è¦å‰‡å¼åˆ†è§£ã€‚"""
        mock_llm = AsyncMock()
        mock_llm.generate = AsyncMock(side_effect=asyncio.TimeoutError())

        adapter = PlanningAdapter("test", llm_service=mock_llm)
        adapter.with_task_decomposition()

        result = await adapter.decompose("Build an API")

        assert result is not None

    @pytest.mark.asyncio
    async def test_no_llm_uses_rule_based(self):
        """æ¸¬è©¦ç„¡ LLM æœå‹™æ™‚ä½¿ç”¨è¦å‰‡å¼ã€‚"""
        adapter = PlanningAdapter("test", llm_service=None)
        adapter.with_task_decomposition()

        result = await adapter.decompose("Build an API")

        assert result is not None
        # å¯ä»¥æª¢æŸ¥æ˜¯å¦ä½¿ç”¨äº†è¦å‰‡å¼ï¼ˆé€šéæ—¥èªŒæˆ–çµæœç‰¹å¾µï¼‰
```

#### é©—æ”¶æ¨™æº–
- [ ] LLM éŒ¯èª¤é™ç´šæ¸¬è©¦é€šé
- [ ] LLM è¶…æ™‚é™ç´šæ¸¬è©¦é€šé
- [ ] ç„¡ LLM è¦å‰‡å¼æ¸¬è©¦é€šé
- [ ] é™ç´šä¸å½±éŸ¿åŠŸèƒ½å®Œæ•´æ€§

---

## é©—è­‰å‘½ä»¤

```bash
# 1. é‹è¡Œ E2E æ¸¬è©¦ (éœ€è¦çœŸå¯¦ API)
cd backend
pytest tests/e2e/ -v -m e2e

# 2. é‹è¡Œæ€§èƒ½æ¸¬è©¦
pytest tests/performance/ -v -m performance

# 3. é‹è¡Œå›é€€ç­–ç•¥æ¸¬è©¦
pytest tests/unit/test_llm_fallback.py -v

# 4. ç”Ÿæˆæ¸¬è©¦è¦†è“‹ç‡å ±å‘Š
pytest tests/ -v --cov=src --cov-report=html

# 5. å®Œæ•´é©—è­‰
pytest tests/ -v --ignore=tests/e2e  # æ’é™¤éœ€è¦çœŸå¯¦ API çš„æ¸¬è©¦
```

---

## å®Œæˆå®šç¾©

- [ ] æ‰€æœ‰ S36 Story å®Œæˆ
- [ ] E2E æ¸¬è©¦å¥—ä»¶å‰µå»ºä¸¦é€šé
- [ ] æ€§èƒ½åŸºæº–æ¸¬è©¦é€šé
- [ ] æ–‡æª”æ›´æ–°å®Œæˆ
- [ ] å›é€€ç­–ç•¥é©—è­‰é€šé
- [ ] Phase 7 å®Œæˆå ±å‘Šå‰µå»º
- [ ] UAT æ¸¬è©¦è¨ˆåŠƒæ›´æ–°

---

## Phase 7 å®Œæˆæ¨™æº–

| æ¨™æº– | ç›®æ¨™ | ç‹€æ…‹ |
|------|------|------|
| LLM æœå‹™åŸºç¤è¨­æ–½ | å®Œæˆ | â³ |
| Phase 2 æ“´å±• LLM æ•´åˆ | 100% | â³ |
| ç«¯åˆ°ç«¯æ¸¬è©¦ | é€šé | â³ |
| æ€§èƒ½é”æ¨™ | P95 < 5s | â³ |
| é™ç´šç­–ç•¥ | é©—è­‰ | â³ |
| æ–‡æª”æ›´æ–° | å®Œæˆ | â³ |

---

**å‰µå»ºæ—¥æœŸ**: 2025-12-21
