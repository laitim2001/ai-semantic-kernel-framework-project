# Sprint 0: Infrastructure & Foundation - MVP èª¿æ•´ç‰ˆ

> âš ï¸ **é‡è¦æç¤º**: æœ¬æ–‡æª”ç‚º **Phase 2 (é›²ç«¯éƒ¨ç½²)** åƒè€ƒæ–‡æª”  
> ğŸ“„ **é–‹ç™¼éšæ®µè«‹ä½¿ç”¨**: [Sprint 0 æœ¬åœ°é–‹ç™¼ç‰ˆ](./sprint-0-local-development.md)  
> ğŸ”„ **é©ç”¨æ™‚æ©Ÿ**: Sprint 4+ é›†æˆæ¸¬è©¦/ç”Ÿç”¢ç’°å¢ƒéƒ¨ç½²

**ç‰ˆæœ¬**: 1.1 (MVP èª¿æ•´ç‰ˆ - Phase 2 Reference)  
**å‰µå»ºæ—¥æœŸ**: 2025-11-20  
**Sprint æœŸé–“**: 2025-11-25 è‡³ 2025-12-06 (2é€±)  
**åœ˜éšŠè¦æ¨¡**: 8äºº (3å¾Œç«¯, 2å‰ç«¯, 1 DevOps, 1 QA, 1 PO)

---

## ğŸ¯ èª¿æ•´èªªæ˜

åŸºæ–¼ MVP å¿«é€Ÿä¸Šç·šçš„ç­–ç•¥ï¼ŒSprint 0 é€²è¡Œä»¥ä¸‹èª¿æ•´ï¼š

### ä¸»è¦è®Šæ›´

#### Phase 1: æœ¬åœ°é–‹ç™¼ï¼ˆSprint 0-3ï¼‰âœ… ç•¶å‰éšæ®µ
1. **é–‹ç™¼ç’°å¢ƒ**: Docker Composeï¼ˆå®Œå…¨æœ¬åœ°ï¼‰
2. **æ¶ˆæ¯éšŠåˆ—**: **RabbitMQ** (æœ¬åœ° Docker å®¹å™¨)
3. **èªè­‰æ–¹å¼**: **Mock Authentication** (ç„¡éœ€ Azure AD)
4. **æ—¥èªŒæ–¹æ¡ˆ**: **Console Logging** (æ¨™æº–è¼¸å‡º)
5. **æˆæœ¬**: **$0 Azure è²»ç”¨** (åƒ… OpenAI API ~$20/æœˆ)

#### Phase 2: é›²ç«¯éƒ¨ç½²ï¼ˆSprint 4+ é›†æˆæ¸¬è©¦/ç”Ÿç”¢ï¼‰
1. **éƒ¨ç½²å¹³å°**: Kubernetes (AKS) â†’ **Azure App Service**
2. **æ¶ˆæ¯éšŠåˆ—**: RabbitMQ â†’ **Azure Service Bus**
3. **èªè­‰æ–¹å¼**: Mock â†’ **Azure AD OAuth 2.0**
4. **ç›£æ§æ–¹æ¡ˆ**: Console â†’ **Application Insights + Azure Monitor**
5. **æˆæœ¬**: ~$123-143/æœˆ

### èª¿æ•´ç†ç”±
- âœ… **é›¶ Azure æˆæœ¬**: é–‹ç™¼éšæ®µå®Œå…¨æœ¬åœ°ï¼Œçœä¸‹ 3 å€‹æœˆ $114 è¨‚é–±è²»
- âœ… **å¿«é€Ÿè¿­ä»£**: ç„¡ç¶²çµ¡å»¶é²ï¼Œæœ¬åœ°èª¿è©¦æ–¹ä¾¿
- âœ… **é™ä½è¤‡é›œåº¦**: ç„¡éœ€å­¸ç¿’ Kubernetesï¼Œå°ˆæ³¨æ¥­å‹™é‚è¼¯
- âœ… **é›¢ç·šé–‹ç™¼**: ä¸ä¾è³´ç¶²çµ¡é€£æ¥ï¼Œé©åˆä»»ä½•ç’°å¢ƒ
- âœ… **å¹³æ»‘é·ç§»**: ä»£ç¢¼ç„¡éœ€ä¿®æ”¹ï¼Œåƒ…åˆ‡æ›ç’°å¢ƒè®Šé‡

### å¾ŒæœŸæ“´å±•è·¯å¾‘
ç•¶ MVP é©—è­‰æˆåŠŸï¼Œæ¥­å‹™éœ€è¦æ›´é«˜å½ˆæ€§æ™‚ï¼Œå¯é·ç§»åˆ° Kubernetesï¼š
- æ›´ç´°ç²’åº¦çš„æœå‹™æ‹†åˆ†
- ç¨ç«‹æ“´å±•æ¯å€‹å¾®æœå‹™
- è—ç¶ éƒ¨ç½²/é‡‘çµ²é›€ç™¼å¸ƒ
- è·¨å€åŸŸé«˜å¯ç”¨

---

## ğŸ“Š èª¿æ•´å¾Œçš„ Story Points

**ç¸½è¨ˆåŠƒé»æ•¸**: 33 (èª¿æ•´å‰: 42 â†’ 38 â†’ 33)  
**æ¸›å°‘åŸå› **: 
- å®Œå…¨æœ¬åœ°é–‹ç™¼ï¼Œç„¡éœ€ Azure è³‡æºé…ç½®
- ä½¿ç”¨ RabbitMQ æ›¿ä»£ Service Busï¼ˆæ›´ç°¡å–®ï¼‰
- Mock èªè­‰æ›¿ä»£ Azure ADï¼ˆé–‹ç™¼éšæ®µï¼‰
- Console æ—¥èªŒæ›¿ä»£ Application Insights

**æŒ‰å„ªå…ˆç´šåˆ†é…**:
- P0 (Critical): 28 é» (85%)
- P1 (High): 5 é» (15%)

**æŒ‰åœ˜éšŠåˆ†é…**:
- DevOps: 13 é» (39%)
- Backend: 20 é» (61%)

---

## ğŸ¯ èª¿æ•´å¾Œçš„ Sprint Backlog

### S0-1: Development Environment Setup âœ… (ç„¡è®Šæ›´)
**Story Points**: 5  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: DevOps  
**ä¾è³´**: ç„¡

#### æè¿°
é…ç½®**å®Œå…¨æœ¬åœ°**é–‹ç™¼ç’°å¢ƒï¼Œä½¿ç”¨ Docker Compose ç·¨æ’æ‰€æœ‰æœå‹™ï¼Œç„¡éœ€ä»»ä½• Azure è³‡æºã€‚

#### é©—æ”¶æ¨™æº–
- [x] Docker Compose é…ç½®å®Œæˆï¼ŒåŒ…å«: âœ… **å·²å®Œæˆ**
  - PostgreSQL 16 (æœ¬åœ°å®¹å™¨)
  - Redis 7 (æœ¬åœ°å®¹å™¨)
  - **RabbitMQ 3.12** (æœ¬åœ°å®¹å™¨ï¼Œæ›¿ä»£ Azure Service Bus)
  - Backend API (Python FastAPI)
- [x] README åŒ…å«æœ¬åœ°ç’°å¢ƒè¨­ç½®æŒ‡å— (< 15 åˆ†é˜å®Œæˆ) âœ… **å·²å®Œæˆ**
- [x] ç’°å¢ƒè®Šé‡æ¨¡æ¿ (.env.example) âœ… **å·²å®Œæˆ**
- [x] æœ¬åœ°é–‹ç™¼æŒ‡å— (local-development-guide.md) âœ… **å·²å®Œæˆ**
- [ ] RabbitMQ Management UI å¯è¨ªå• (http://localhost:15672)

#### æŠ€è¡“å¯¦ç¾
```yaml
# docker-compose.yml
version: '3.8'
services:
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_DB: ipa_platform
      POSTGRES_USER: ${DB_USER}
      POSTGRES_PASSWORD: ${DB_PASSWORD}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${DB_USER}"]
      interval: 10s
      timeout: 5s
      retries: 5
  
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3
  
  backend:
    build: ./backend
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://${DB_USER}:${DB_PASSWORD}@postgres:5432/ipa_platform
      REDIS_URL: redis://redis:6379/0
      AZURE_SERVICE_BUS_CONNECTION_STRING: ${SERVICE_BUS_CONN_STR}
    depends_on:
      - postgres
      - redis
    volumes:
      - ./backend:/app
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload

volumes:
  postgres_data:
  redis_data:
```

---

### S0-2: Azure App Service Setup ğŸ†• (æ›¿ä»£ Kubernetes)
**Story Points**: 5 (åŸ 8 é»ï¼Œé™ä½è¤‡é›œåº¦)  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: DevOps  
**ä¾è³´**: ç„¡

#### æè¿°
åœ¨ Azure å‰µå»º App Service Plan å’Œ App Serviceï¼Œé…ç½® Staging å’Œ Production ç’°å¢ƒã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Azure App Service Plan å‰µå»ºå®Œæˆ
  - Plan: Standard S1 (1 vCore, 1.75GB RAM, $75/æœˆ)
- [ ] 2 å€‹ App Service å¯¦ä¾‹
  - **ipa-backend-staging**: æ¸¬è©¦ç’°å¢ƒ
  - **ipa-backend-prod**: ç”Ÿç”¢ç’°å¢ƒ
- [ ] é…ç½® Deployment Slots (staging â†’ production swap)
- [ ] å•Ÿç”¨ Application Insights
- [ ] é…ç½®è‡ªå‹•æ“´å±•è¦å‰‡ (CPU > 70% æ™‚æ“´å±•)
- [ ] è¨­ç½®å¥åº·æª¢æŸ¥ç«¯é» (/health)

#### æŠ€è¡“å¯¦ç¾
```bash
# Azure CLI å‘½ä»¤
az group create --name rg-ipa-platform --location eastus

# å‰µå»º App Service Plan
az appservice plan create \
  --name plan-ipa-platform \
  --resource-group rg-ipa-platform \
  --sku S1 \
  --is-linux

# å‰µå»º Backend App Service (Staging)
az webapp create \
  --name ipa-backend-staging \
  --resource-group rg-ipa-platform \
  --plan plan-ipa-platform \
  --runtime "PYTHON:3.11"

# å‰µå»º Backend App Service (Production)
az webapp create \
  --name ipa-backend-prod \
  --resource-group rg-ipa-platform \
  --plan plan-ipa-platform \
  --runtime "PYTHON:3.11"

# é…ç½® Application Insights
az monitor app-insights component create \
  --app ipa-platform-insights \
  --location eastus \
  --resource-group rg-ipa-platform \
  --application-type web

# å•Ÿç”¨è‡ªå‹•æ“´å±•
az monitor autoscale create \
  --resource ipa-backend-prod \
  --resource-group rg-ipa-platform \
  --resource-type Microsoft.Web/serverfarms \
  --name autoscale-ipa \
  --min-count 1 \
  --max-count 5 \
  --count 1

az monitor autoscale rule create \
  --resource ipa-backend-prod \
  --resource-group rg-ipa-platform \
  --autoscale-name autoscale-ipa \
  --condition "Percentage CPU > 70 avg 5m" \
  --scale out 1
```

#### å­ä»»å‹™
1. [ ] å‰µå»º Azure Resource Group
2. [ ] å‰µå»º App Service Plan
3. [ ] å‰µå»º Staging å’Œ Production App Service
4. [ ] é…ç½® Deployment Slots
5. [ ] å•Ÿç”¨ Application Insights
6. [ ] é…ç½®è‡ªå‹•æ“´å±•è¦å‰‡
7. [ ] è¨­ç½®ç’°å¢ƒè®Šé‡ (App Settings)
8. [ ] æ¸¬è©¦éƒ¨ç½²æµç¨‹

---

### S0-3: CI/CD Pipeline for App Service ğŸ”„ (èª¿æ•´)
**Story Points**: 5 (åŸ 8 é»ï¼ŒApp Service éƒ¨ç½²æ›´ç°¡å–®)  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: DevOps  
**ä¾è³´**: S0-2

#### æè¿°
å‰µå»º GitHub Actions æµæ°´ç·šï¼Œå¯¦ç¾è‡ªå‹•æ§‹å»ºã€æ¸¬è©¦ã€éƒ¨ç½²åˆ° Azure App Serviceã€‚

#### é©—æ”¶æ¨™æº–
- [ ] GitHub Actions workflow é…ç½®å®Œæˆ
  - `.github/workflows/ci-cd.yml`
- [ ] Pipeline éšæ®µ:
  1. Build: Docker image æ§‹å»º
  2. Test: pytest å–®å…ƒæ¸¬è©¦ (coverage > 80%)
  3. Security Scan: Trivy, Bandit, Safety
  4. Deploy to Staging: è‡ªå‹•éƒ¨ç½²
  5. Smoke Test: åŸºæœ¬å¥åº·æª¢æŸ¥
  6. Deploy to Production: æ‰‹å‹•æ‰¹å‡†
- [ ] Azure Service Principal é…ç½®å®Œæˆ (ç”¨æ–¼ CI/CD èªè­‰)
- [ ] Secrets é…ç½®åœ¨ GitHub Secrets

#### æŠ€è¡“å¯¦ç¾
```yaml
# .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

env:
  AZURE_WEBAPP_NAME: ipa-backend-staging
  PYTHON_VERSION: '3.11'

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: ${{ env.PYTHON_VERSION }}
      
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install pytest pytest-cov bandit safety
      
      - name: Run tests
        run: |
          pytest --cov=. --cov-report=xml --cov-report=term
      
      - name: Security scan - Bandit
        run: bandit -r . -f json -o bandit-report.json
      
      - name: Security scan - Safety
        run: safety check --json
      
      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v3
  
  deploy-staging:
    needs: build-and-test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    steps:
      - uses: actions/checkout@v4
      
      - name: Login to Azure
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Deploy to App Service (Staging)
        uses: azure/webapps-deploy@v2
        with:
          app-name: ${{ env.AZURE_WEBAPP_NAME }}
          package: .
      
      - name: Run smoke tests
        run: |
          curl -f https://ipa-backend-staging.azurewebsites.net/health || exit 1
  
  deploy-production:
    needs: deploy-staging
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: https://ipa-backend-prod.azurewebsites.net
    steps:
      - uses: actions/checkout@v4
      
      - name: Login to Azure
        uses: azure/login@v1
        with:
          creds: ${{ secrets.AZURE_CREDENTIALS }}
      
      - name: Deploy to App Service (Production)
        uses: azure/webapps-deploy@v2
        with:
          app-name: ipa-backend-prod
          package: .
      
      - name: Run smoke tests
        run: |
          curl -f https://ipa-backend-prod.azurewebsites.net/health || exit 1
```

#### å­ä»»å‹™
1. [ ] å‰µå»º Azure Service Principal
2. [ ] é…ç½® GitHub Secrets (AZURE_CREDENTIALS)
3. [ ] ç·¨å¯« CI/CD workflow æ–‡ä»¶
4. [ ] é…ç½®æ¸¬è©¦éšæ®µ (pytest, coverage)
5. [ ] é…ç½®å®‰å…¨æƒæ (Trivy, Bandit, Safety)
6. [ ] é…ç½® Staging éƒ¨ç½²
7. [ ] é…ç½® Production éƒ¨ç½² (éœ€æ‰‹å‹•æ‰¹å‡†)
8. [ ] æ¸¬è©¦å®Œæ•´ CI/CD æµç¨‹

---

### S0-4: Database Infrastructure âœ… (ç„¡è®Šæ›´)
**Story Points**: 5  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: Backend  
**ä¾è³´**: ç„¡

#### æè¿°
è¨­ç½® Azure Database for PostgreSQL Flexible Serverï¼Œåˆå§‹åŒ– schemaï¼Œé…ç½® Alembic é·ç§»æ¡†æ¶ã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Azure PostgreSQL Flexible Server å‰µå»ºå®Œæˆ
  - SKU: Burstable B1ms (1 vCore, 2GB RAM, $12/æœˆ)
  - PostgreSQL ç‰ˆæœ¬: 16
  - å‚™ä»½ä¿ç•™: 7 å¤©
- [ ] Database åˆå§‹åŒ–
  - Database name: `ipa_platform`
  - User: `ipa_admin`
- [ ] Alembic é·ç§»æ¡†æ¶é…ç½®
  - `alembic.ini` é…ç½®
  - `migrations/` ç›®éŒ„çµæ§‹
  - åˆå§‹é·ç§»è…³æœ¬ (create tables)
- [ ] é€£æ¥æ± é…ç½® (SQLAlchemy)
- [ ] SSL é€£æ¥å¼·åˆ¶å•Ÿç”¨

#### æŠ€è¡“å¯¦ç¾
```bash
# å‰µå»º PostgreSQL Server
az postgres flexible-server create \
  --name ipa-postgres-server \
  --resource-group rg-ipa-platform \
  --location eastus \
  --admin-user ipa_admin \
  --admin-password <STRONG_PASSWORD> \
  --sku-name Standard_B1ms \
  --tier Burstable \
  --version 16 \
  --storage-size 32 \
  --backup-retention 7 \
  --public-access 0.0.0.0-255.255.255.255

# å‰µå»ºæ•¸æ“šåº«
az postgres flexible-server db create \
  --resource-group rg-ipa-platform \
  --server-name ipa-postgres-server \
  --database-name ipa_platform
```

```python
# backend/database.py
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker, declarative_base
import os

DATABASE_URL = os.getenv("DATABASE_URL")

engine = create_engine(
    DATABASE_URL,
    pool_size=10,
    max_overflow=20,
    pool_pre_ping=True,  # æª¢æŸ¥é€£æ¥å¥åº·
    echo=False
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
Base = declarative_base()

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
```

```python
# backend/models/workflow.py
from sqlalchemy import Column, String, Integer, DateTime, Text, JSON
from sqlalchemy.sql import func
from database import Base

class Workflow(Base):
    __tablename__ = "workflows"
    
    id = Column(String(36), primary_key=True)
    name = Column(String(255), nullable=False)
    description = Column(Text)
    config = Column(JSON, nullable=False)
    version = Column(Integer, default=1)
    created_at = Column(DateTime(timezone=True), server_default=func.now())
    updated_at = Column(DateTime(timezone=True), onupdate=func.now())
    created_by = Column(String(255))
    status = Column(String(50), default='draft')
```

#### å­ä»»å‹™
1. [ ] å‰µå»º Azure PostgreSQL Server
2. [ ] å‰µå»ºæ•¸æ“šåº«å’Œç”¨æˆ¶
3. [ ] é…ç½®é˜²ç«ç‰†è¦å‰‡ (å…è¨± Azure services)
4. [ ] å®‰è£ Alembic (`pip install alembic`)
5. [ ] åˆå§‹åŒ– Alembic (`alembic init migrations`)
6. [ ] é…ç½® SQLAlchemy é€£æ¥æ± 
7. [ ] å‰µå»ºåŸºç¤ model (Workflow, Execution, Agent)
8. [ ] ç”Ÿæˆåˆå§‹é·ç§»è…³æœ¬
9. [ ] æ¸¬è©¦æ•¸æ“šåº«é€£æ¥å’Œé·ç§»

---

### S0-5: Redis Cache Setup âœ… (ç„¡è®Šæ›´)
**Story Points**: 3  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: Backend  
**ä¾è³´**: ç„¡

#### æè¿°
è¨­ç½® Azure Cache for Redisï¼Œé…ç½®é€£æ¥æ± å’Œç·©å­˜ç­–ç•¥ã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Azure Cache for Redis å‰µå»ºå®Œæˆ
  - SKU: Basic C0 (250MB, $16/æœˆ)
- [ ] Redis å®¢æˆ¶ç«¯é…ç½® (redis-py)
- [ ] ç·©å­˜ç­–ç•¥å¯¦ç¾:
  - Workflow é…ç½®ç·©å­˜ (TTL: 5 åˆ†é˜)
  - Execution ç‹€æ…‹ç·©å­˜ (TTL: 1 åˆ†é˜)
  - Session å­˜å„² (TTL: 30 åˆ†é˜)

#### æŠ€è¡“å¯¦ç¾
```bash
# å‰µå»º Redis Cache
az redis create \
  --name ipa-redis-cache \
  --resource-group rg-ipa-platform \
  --location eastus \
  --sku Basic \
  --vm-size c0
```

```python
# backend/cache.py
import redis
import os
import json
from typing import Optional

REDIS_URL = os.getenv("REDIS_URL")
redis_client = redis.from_url(REDIS_URL, decode_responses=True)

class Cache:
    @staticmethod
    def get(key: str) -> Optional[dict]:
        value = redis_client.get(key)
        return json.loads(value) if value else None
    
    @staticmethod
    def set(key: str, value: dict, ttl: int = 300):
        redis_client.setex(key, ttl, json.dumps(value))
    
    @staticmethod
    def delete(key: str):
        redis_client.delete(key)
    
    @staticmethod
    def invalidate_pattern(pattern: str):
        for key in redis_client.scan_iter(pattern):
            redis_client.delete(key)

# ä½¿ç”¨ç¤ºä¾‹
def get_workflow(workflow_id: str):
    cache_key = f"workflow:{workflow_id}"
    cached = Cache.get(cache_key)
    if cached:
        return cached
    
    workflow = db.query(Workflow).filter_by(id=workflow_id).first()
    Cache.set(cache_key, workflow.to_dict(), ttl=300)
    return workflow
```

---

### S0-6: Azure Service Bus Setup ğŸ†• (æ›¿ä»£ RabbitMQ)
**Story Points**: 3  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: Backend  
**ä¾è³´**: ç„¡

#### æè¿°
è¨­ç½® Azure Service Busï¼Œå‰µå»º Queues å’Œ Topicsï¼Œé…ç½® Python SDKã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Azure Service Bus Namespace å‰µå»º
  - SKU: Basic ($10/æœˆ)
- [ ] Queues å‰µå»º:
  - `execution-queue`: åŸ·è¡Œä»»å‹™éšŠåˆ—
  - `notification-queue`: é€šçŸ¥éšŠåˆ—
  - `dlq-execution`: Dead Letter Queue for execution failures
- [ ] Python SDK é…ç½® (azure-servicebus)
- [ ] é‡è©¦æ©Ÿåˆ¶å¯¦ç¾ (æŒ‡æ•¸é€€é¿)
- [ ] Dead Letter Queue è™•ç†é‚è¼¯

#### æŠ€è¡“å¯¦ç¾
```bash
# å‰µå»º Service Bus Namespace
az servicebus namespace create \
  --name ipa-servicebus \
  --resource-group rg-ipa-platform \
  --location eastus \
  --sku Basic

# å‰µå»º Queues
az servicebus queue create \
  --name execution-queue \
  --namespace-name ipa-servicebus \
  --resource-group rg-ipa-platform \
  --max-delivery-count 5 \
  --lock-duration PT5M

az servicebus queue create \
  --name notification-queue \
  --namespace-name ipa-servicebus \
  --resource-group rg-ipa-platform
```

```python
# backend/message_queue.py
from azure.servicebus import ServiceBusClient, ServiceBusMessage
import os
import json

SERVICE_BUS_CONN_STR = os.getenv("AZURE_SERVICE_BUS_CONNECTION_STRING")
servicebus_client = ServiceBusClient.from_connection_string(SERVICE_BUS_CONN_STR)

class MessageQueue:
    @staticmethod
    def send_message(queue_name: str, message: dict):
        with servicebus_client:
            sender = servicebus_client.get_queue_sender(queue_name=queue_name)
            with sender:
                msg = ServiceBusMessage(json.dumps(message))
                sender.send_messages(msg)
    
    @staticmethod
    def receive_messages(queue_name: str, max_messages: int = 10):
        with servicebus_client:
            receiver = servicebus_client.get_queue_receiver(queue_name=queue_name)
            with receiver:
                messages = receiver.receive_messages(
                    max_message_count=max_messages,
                    max_wait_time=5
                )
                for msg in messages:
                    try:
                        body = json.loads(str(msg))
                        yield body
                        receiver.complete_message(msg)
                    except Exception as e:
                        print(f"Error processing message: {e}")
                        receiver.abandon_message(msg)

# ä½¿ç”¨ç¤ºä¾‹
def trigger_execution(workflow_id: str, params: dict):
    message = {
        "workflow_id": workflow_id,
        "params": params,
        "timestamp": datetime.utcnow().isoformat()
    }
    MessageQueue.send_message("execution-queue", message)
```

#### å­ä»»å‹™
1. [ ] å‰µå»º Service Bus Namespace
2. [ ] å‰µå»º Queues (execution, notification)
3. [ ] é…ç½® Dead Letter Queue
4. [ ] å®‰è£ Python SDK (`pip install azure-servicebus`)
5. [ ] å¯¦ç¾æ¶ˆæ¯ç™¼é€é‚è¼¯
6. [ ] å¯¦ç¾æ¶ˆæ¯æ¥æ”¶é‚è¼¯
7. [ ] é…ç½®é‡è©¦æ©Ÿåˆ¶
8. [ ] æ¸¬è©¦æ¶ˆæ¯æµ

---

### S0-7: Authentication Framework âœ… (ç„¡è®Šæ›´)
**Story Points**: 8  
**å„ªå…ˆç´š**: P0 - Critical  
**è² è²¬äºº**: Backend  
**ä¾è³´**: S0-4

#### æè¿°
å¯¦ç¾ OAuth 2.0 + JWT èº«ä»½é©—è­‰ï¼Œé›†æˆ Azure ADã€‚

#### é©—æ”¶æ¨™æº–
- [ ] OAuth 2.0 æˆæ¬Šæµç¨‹å¯¦ç¾
- [ ] JWT Token ç”Ÿæˆå’Œé©—è­‰
- [ ] Azure AD é›†æˆ (ä½¿ç”¨ MSAL)
- [ ] RBAC è§’è‰²å®šç¾© (Admin, User, Viewer)
- [ ] API ç«¯é»ä¿è­· (require_auth decorator)
- [ ] Token åˆ·æ–°æ©Ÿåˆ¶

#### æŠ€è¡“å¯¦ç¾
```python
# backend/auth.py
from fastapi import Depends, HTTPException, status
from fastapi.security import OAuth2PasswordBearer, OAuth2PasswordRequestForm
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
import os

SECRET_KEY = os.getenv("JWT_SECRET_KEY")
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 30

oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto")

def create_access_token(data: dict, expires_delta: timedelta = None):
    to_encode = data.copy()
    expire = datetime.utcnow() + (expires_delta or timedelta(minutes=15))
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user(token: str = Depends(oauth2_scheme)):
    credentials_exception = HTTPException(
        status_code=status.HTTP_401_UNAUTHORIZED,
        detail="Could not validate credentials",
        headers={"WWW-Authenticate": "Bearer"},
    )
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
    return username

def require_role(allowed_roles: list):
    def decorator(func):
        async def wrapper(*args, current_user = Depends(get_current_user), **kwargs):
            if current_user.role not in allowed_roles:
                raise HTTPException(status_code=403, detail="Insufficient permissions")
            return await func(*args, **kwargs)
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
@app.post("/workflows")
@require_role(["admin", "user"])
async def create_workflow(workflow: WorkflowCreate, current_user = Depends(get_current_user)):
    # ...
```

---

### S0-8: Monitoring Setup (Hybrid) ğŸ”„ (èª¿æ•´)
**Story Points**: 5  
**å„ªå…ˆç´š**: P1 - High  
**è² è²¬äºº**: DevOps  
**ä¾è³´**: S0-2

#### æè¿°
é…ç½®æ··åˆç›£æ§æ–¹æ¡ˆï¼šAzure Monitor (åŸºç¤) + Prometheus (è‡ªå®šç¾©æ¥­å‹™æŒ‡æ¨™)ã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Application Insights é…ç½®å®Œæˆ
  - è‡ªå‹•æ”¶é›† HTTP è«‹æ±‚/éŸ¿æ‡‰
  - ç•°å¸¸è¿½è¹¤
  - ä¾è³´è¿½è¹¤ (DB, Redis, Service Bus)
  - è‡ªå®šç¾©äº‹ä»¶è¨˜éŒ„
- [ ] Azure Monitor å‘Šè­¦è¦å‰‡
  - CPU > 80% for 5 mins
  - Memory > 85% for 5 mins
  - HTTP 5xx errors > 10 in 5 mins
- [ ] Prometheus + Grafana éƒ¨ç½² (å¯é¸ï¼Œç”¨æ–¼è‡ªå®šç¾©æŒ‡æ¨™)
  - éƒ¨ç½²åœ¨ Azure Container Instance
  - Grafana Dashboard é…ç½®
  - æ¥­å‹™æŒ‡æ¨™æ¡é›† (Workflow success rate, LLM API cost)

#### æŠ€è¡“å¯¦ç¾
```python
# backend/main.py - Application Insights é›†æˆ
from opencensus.ext.azure.log_exporter import AzureLogHandler
from opencensus.ext.azure.trace_exporter import AzureExporter
from opencensus.trace.samplers import ProbabilitySampler
from opencensus.trace import config_integration
import logging

# é…ç½® Application Insights
config_integration.trace_integrations(['requests', 'sqlalchemy', 'redis'])

logger = logging.getLogger(__name__)
logger.addHandler(AzureLogHandler(
    connection_string=os.getenv('APPLICATIONINSIGHTS_CONNECTION_STRING')
))

# è‡ªå®šç¾©äº‹ä»¶è¨˜éŒ„
from opencensus.ext.azure import metrics_exporter
from opencensus.stats import aggregation as aggregation_module
from opencensus.stats import measure as measure_module
from opencensus.stats import stats as stats_module
from opencensus.stats import view as view_module

# å®šç¾©è‡ªå®šç¾©æŒ‡æ¨™
workflow_execution_measure = measure_module.MeasureInt(
    "workflow_executions",
    "Number of workflow executions",
    "executions"
)

# è¨˜éŒ„è‡ªå®šç¾©æŒ‡æ¨™
def record_workflow_execution(workflow_id: str, success: bool):
    mmap = stats_module.stats.stats_recorder.new_measurement_map()
    mmap.measure_int_put(workflow_execution_measure, 1)
    mmap.record()
    
    logger.info(f"Workflow executed: {workflow_id}, success: {success}", extra={
        'custom_dimensions': {
            'workflow_id': workflow_id,
            'success': success
        }
    })
```

```bash
# å¯é¸: éƒ¨ç½² Prometheus + Grafana (è‡ªå®šç¾©æŒ‡æ¨™)
az container create \
  --name prometheus \
  --resource-group rg-ipa-platform \
  --image prom/prometheus:latest \
  --cpu 1 \
  --memory 1 \
  --ports 9090 \
  --environment-variables PROM_CONFIG_URL=https://...

az container create \
  --name grafana \
  --resource-group rg-ipa-platform \
  --image grafana/grafana:latest \
  --cpu 1 \
  --memory 1 \
  --ports 3000
```

---

### S0-9: Application Insights Logging ğŸ†• (æ›¿ä»£ ELK)
**Story Points**: 3 (åŸ 5 é»ï¼Œå…§å»ºé›†æˆæ›´ç°¡å–®)  
**å„ªå…ˆç´š**: P1 - High  
**è² è²¬äºº**: DevOps  
**ä¾è³´**: S0-2

#### æè¿°
é…ç½® Application Insights ä½œç‚ºé›†ä¸­å¼æ—¥èªŒç³»çµ±ï¼Œç„¡éœ€éƒ¨ç½²é¡å¤–åŸºç¤è¨­æ–½ã€‚

#### é©—æ”¶æ¨™æº–
- [ ] Application Insights å®Œæ•´é…ç½®
- [ ] çµæ§‹åŒ–æ—¥èªŒè¨˜éŒ„ (JSON format)
- [ ] æ—¥èªŒç´šåˆ¥é…ç½® (DEBUG/INFO/WARNING/ERROR)
- [ ] Correlation ID è¿½è¹¤ (è·¨æœå‹™è«‹æ±‚)
- [ ] Log Analytics æŸ¥è©¢ç¤ºä¾‹æ–‡æª”
- [ ] æ—¥èªŒä¿ç•™ç­–ç•¥ (90 å¤©ï¼Œå¯å»¶é•·åˆ° 730 å¤©)

#### æŠ€è¡“å¯¦ç¾
```python
# backend/logging_config.py
import logging
import json
from opencensus.ext.azure.log_exporter import AzureLogHandler
from opencensus.ext.azure.trace_exporter import AzureExporter
from opencensus.trace.tracer import Tracer
from opencensus.trace.samplers import ProbabilitySampler

# é…ç½®æ—¥èªŒæ ¼å¼
class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if hasattr(record, 'custom_dimensions'):
            log_data['custom_dimensions'] = record.custom_dimensions
        return json.dumps(log_data)

# é…ç½® Application Insights
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

handler = AzureLogHandler(
    connection_string=os.getenv('APPLICATIONINSIGHTS_CONNECTION_STRING')
)
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)

# é…ç½®åˆ†å¸ƒå¼è¿½è¹¤
tracer = Tracer(
    exporter=AzureExporter(
        connection_string=os.getenv('APPLICATIONINSIGHTS_CONNECTION_STRING')
    ),
    sampler=ProbabilitySampler(1.0)  # MVP éšæ®µ 100% æ¡æ¨£
)

# ä½¿ç”¨ç¤ºä¾‹
logger.info("Workflow execution started", extra={
    'custom_dimensions': {
        'workflow_id': workflow_id,
        'user_id': user_id,
        'execution_id': execution_id
    }
})
```

**Log Analytics æŸ¥è©¢ç¤ºä¾‹ (KQL)**:
```kusto
// æŸ¥è©¢æœ€è¿‘ 1 å°æ™‚çš„éŒ¯èª¤æ—¥èªŒ
traces
| where timestamp > ago(1h)
| where severityLevel >= 3  // ERROR and above
| project timestamp, message, customDimensions
| order by timestamp desc

// æŸ¥è©¢ç‰¹å®š Workflow çš„åŸ·è¡Œæ—¥èªŒ
traces
| where customDimensions.workflow_id == "workflow-123"
| order by timestamp asc

// åˆ†æ HTTP è«‹æ±‚æ€§èƒ½
requests
| where timestamp > ago(1d)
| summarize avg(duration), percentile(duration, 95), count() by name
| order by avg_duration desc
```

---

## ğŸ“… Sprint 0 æ™‚é–“ç·š

```
Week 1 (11/25-11/29):
  Day 1-2: S0-1 (Docker Compose), S0-2 (App Service), S0-4 (Database)
  Day 3-4: S0-5 (Redis), S0-6 (Service Bus), S0-7 (Auth)
  Day 5: S0-3 (CI/CD), S0-8 (Monitoring)

Week 2 (12/02-12/06):
  Day 1-2: S0-3 (CI/CD å®Œå–„), S0-9 (Logging)
  Day 3-4: é›†æˆæ¸¬è©¦ï¼Œä¿®å¾©å•é¡Œ
  Day 5: Sprint Review, Retrospective, Sprint 1 Planning
```

---

## ğŸ¯ Definition of Done (DoD)

### Sprint 0 æ•´é«” DoD
- [ ] æ‰€æœ‰ P0 ä»»å‹™å®Œæˆä¸¦é©—è­‰
- [ ] æ‰€æœ‰é–‹ç™¼äººå“¡å¯ä»¥åœ¨æœ¬åœ°é‹è¡Œå®Œæ•´æ‡‰ç”¨æ£§
- [ ] CI/CD æµæ°´ç·šå¯ä»¥è‡ªå‹•éƒ¨ç½²åˆ° Staging
- [ ] Staging ç’°å¢ƒå¥åº·æª¢æŸ¥é€šé
- [ ] åŸºç¤ç›£æ§å„€è¡¨æ¿å¯ç”¨ (App Insights)
- [ ] æ–‡æª”æ›´æ–° (README, Setup Guide)

### å€‹åˆ¥ä»»å‹™ DoD
- [ ] ä»£ç¢¼æäº¤åˆ° `main` åˆ†æ”¯
- [ ] å–®å…ƒæ¸¬è©¦é€šé (if applicable)
- [ ] ä»£ç¢¼å¯©æŸ¥é€šé (1 approver)
- [ ] æ–‡æª”æ›´æ–°
- [ ] Demo çµ¦ PO ç¢ºèª

---

## ğŸš¨ é¢¨éšªèˆ‡ç·©è§£ç­–ç•¥

| é¢¨éšª | åš´é‡æ€§ | ç·©è§£ç­–ç•¥ |
|------|--------|---------|
| **Azure è³‡æºé…é¡ä¸è¶³** | High | æå‰ç”³è«‹è³‡æºé…é¡ï¼Œæº–å‚™å‚™ç”¨è¨‚é–± |
| **Service Principal æ¬Šé™å•é¡Œ** | Medium | ä½¿ç”¨ Owner è§’è‰²æ¸¬è©¦ï¼Œå¾ŒæœŸé™ç´šç‚º Contributor |
| **Application Insights å­¸ç¿’æ›²ç·š** | Medium | æä¾› KQL æŸ¥è©¢ç¤ºä¾‹æ–‡æª”ï¼Œå®‰æ’åŸ¹è¨“ |
| **Service Bus èˆ‡ RabbitMQ å·®ç•°** | Low | ç·¨å¯«æŠ½è±¡å±¤ï¼Œä¾¿æ–¼å¾ŒæœŸåˆ‡æ› |

---

## ğŸ“Š æˆæœ¬ä¼°ç®— (MVP éšæ®µ)

| æœå‹™ | SKU | æœˆæˆæœ¬ |
|------|-----|--------|
| App Service Plan | Standard S1 | $75 |
| PostgreSQL | Burstable B1ms | $12 |
| Redis Cache | Basic C0 | $16 |
| Service Bus | Basic | $10 |
| Application Insights | Pay-as-you-go | $10-30 (estimated) |
| **ç¸½è¨ˆ** | | **$123-143** |

å°æ¯” Kubernetes æ–¹æ¡ˆ (~$300+/æœˆ)ï¼Œç¯€çœ **~55%** æˆæœ¬ã€‚

---

## ğŸ“ å‚™è¨»

1. **å¾ŒæœŸé·ç§»è·¯å¾‘**: ç•¶ MVP é©—è­‰æˆåŠŸï¼Œéœ€è¦æ›´é«˜å½ˆæ€§æ™‚ï¼Œå¯ä»¥é€æ­¥é·ç§»åˆ° Kubernetesï¼Œå·²æœ‰çš„ä»£ç¢¼ç„¡éœ€å¤§æ”¹ã€‚
2. **æŠ€è¡“å‚µå‹™**: Service Bus æŠ½è±¡å±¤éœ€è¦ä¿æŒèˆ‡ RabbitMQ å…¼å®¹çš„æ¥å£ï¼Œä¾¿æ–¼æœªä¾†åˆ‡æ›ã€‚
3. **ç›£æ§æ“´å±•**: MVP éšæ®µå…ˆç”¨ App Insightsï¼Œå¾ŒæœŸå¯ä»¥æ·»åŠ  Prometheus æ¡é›†è‡ªå®šç¾©æ¥­å‹™æŒ‡æ¨™ã€‚
4. **å­¸ç¿’è³‡æº**: 
   - [Azure App Service æ–‡æª”](https://docs.microsoft.com/azure/app-service/)
   - [Application Insights å¿«é€Ÿå…¥é–€](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview)
   - [Azure Service Bus Python SDK](https://docs.microsoft.com/azure/service-bus-messaging/service-bus-python-how-to-use-queues)

---

**æ–‡æª”ç‹€æ…‹**: âœ… MVP èª¿æ•´ç‰ˆå®Œæˆ (2025-11-20)  
**ä¸‹ä¸€æ­¥**: é–‹å§‹åŸ·è¡Œ Sprint 0 ä»»å‹™ (2025-11-25)
